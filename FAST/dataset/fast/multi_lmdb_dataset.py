#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Multi LMDB Dataset
ì—¬ëŸ¬ LMDB ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤
"""

import os
import random
import torch
from torch.utils.data import ConcatDataset, Dataset
from .fast_lmdb import FAST_LMDB


class MultiLMDBDataset(Dataset):
    """
    ì—¬ëŸ¬ LMDB ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤
    ê° ë°ì´í„°ì…‹ì— ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    def __init__(self, lmdb_configs, split='train', weights=None, **kwargs):
        """
        Args:
            lmdb_configs (list): LMDB ì„¤ì • ë¦¬ìŠ¤íŠ¸
                [
                    {'path': './data/dataset1.lmdb', 'weight': 1.0},
                    {'path': './data/dataset2.lmdb', 'weight': 0.5},
                    ...
                ]
            split (str): 'train' ë˜ëŠ” 'test'
            weights (list): ê° ë°ì´í„°ì…‹ì˜ ê°€ì¤‘ì¹˜ (deprecated, lmdb_configsì—ì„œ weight ì‚¬ìš©)
            **kwargs: FAST_LMDB ìƒì„±ìì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìë“¤
        """
        self.lmdb_configs = lmdb_configs
        self.split = split
        self.kwargs = kwargs
        
        # ê°œë³„ ë°ì´í„°ì…‹ë“¤ ìƒì„±
        self.datasets = []
        self.dataset_weights = []
        
        for config in lmdb_configs:
            lmdb_path = config['path']
            weight = config.get('weight', 1.0)
            
            print(f"ğŸ“‚ LMDB ë¡œë“œ ì¤‘: {lmdb_path} (ê°€ì¤‘ì¹˜: {weight})")
            
            dataset = FAST_LMDB(
                lmdb_path=lmdb_path,
                split=split,
                **kwargs
            )
            
            self.datasets.append(dataset)
            self.dataset_weights.append(weight)
        
        # ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìƒ˜í”Œ ì¸ë±ìŠ¤ ìƒì„±
        self._create_weighted_indices()
        
        print(f"ğŸ¯ ì´ ë°ì´í„°ì…‹ ìˆ˜: {len(self.datasets)}")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(self.weighted_indices)}")
    
    def _create_weighted_indices(self):
        """ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìƒ˜í”Œ ì¸ë±ìŠ¤ ìƒì„±"""
        self.weighted_indices = []
        
        for dataset_idx, (dataset, weight) in enumerate(zip(self.datasets, self.dataset_weights)):
            # ê° ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ìƒ˜í”Œ ìˆ˜
            dataset_size = len(dataset)
            
            # ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
            weighted_size = int(dataset_size * weight)
            
            # ì¸ë±ìŠ¤ ìƒì„± (ë°˜ë³µ ìƒ˜í”Œë§ í—ˆìš©)
            if weight >= 1.0:
                # ê°€ì¤‘ì¹˜ê°€ 1 ì´ìƒì´ë©´ ë°˜ë³µ ìƒ˜í”Œë§
                indices = list(range(dataset_size)) * int(weight)
                remaining = weighted_size - len(indices)
                if remaining > 0:
                    indices.extend(random.choices(range(dataset_size), k=remaining))
            else:
                # ê°€ì¤‘ì¹˜ê°€ 1 ë¯¸ë§Œì´ë©´ ë¶€ë¶„ ìƒ˜í”Œë§
                indices = random.sample(range(dataset_size), weighted_size)
            
            # (ë°ì´í„°ì…‹ ì¸ë±ìŠ¤, ìƒ˜í”Œ ì¸ë±ìŠ¤) íŠœí”Œë¡œ ì €ì¥
            for sample_idx in indices:
                self.weighted_indices.append((dataset_idx, sample_idx))
        
        # ì¸ë±ìŠ¤ ì„ê¸°
        random.shuffle(self.weighted_indices)
        
        print(f"ğŸ“ˆ ê°€ì¤‘ì¹˜ ì ìš© ê²°ê³¼:")
        for i, (dataset, weight) in enumerate(zip(self.datasets, self.dataset_weights)):
            actual_count = sum(1 for idx in self.weighted_indices if idx[0] == i)
            print(f"   ë°ì´í„°ì…‹ {i+1}: {len(dataset)} -> {actual_count} ìƒ˜í”Œ (ê°€ì¤‘ì¹˜: {weight})")
    
    def __len__(self):
        return len(self.weighted_indices)
    
    def __getitem__(self, index):
        """ìƒ˜í”Œ ë¡œë“œ"""
        dataset_idx, sample_idx = self.weighted_indices[index]
        return self.datasets[dataset_idx][sample_idx]
    
    def resample_indices(self):
        """ì—í¬í¬ë§ˆë‹¤ ì¸ë±ìŠ¤ ì¬ìƒ˜í”Œë§"""
        self._create_weighted_indices()


class ConcatLMDBDataset(ConcatDataset):
    """
    PyTorch ConcatDatasetì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ LMDB ê²°í•©
    """
    
    def __init__(self, lmdb_paths, split='train', **kwargs):
        """
        Args:
            lmdb_paths (list): LMDB íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            split (str): 'train' ë˜ëŠ” 'test'
            **kwargs: FAST_LMDB ìƒì„±ìì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìë“¤
        """
        datasets = []
        
        for lmdb_path in lmdb_paths:
            print(f"ğŸ“‚ LMDB ë¡œë“œ ì¤‘: {lmdb_path}")
            dataset = FAST_LMDB(
                lmdb_path=lmdb_path,
                split=split,
                **kwargs
            )
            datasets.append(dataset)
        
        super().__init__(datasets)
        
        print(f"ğŸ¯ ì´ ë°ì´í„°ì…‹ ìˆ˜: {len(datasets)}")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(self)}")
        
        # ê° ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥
        for i, dataset in enumerate(datasets):
            print(f"   ë°ì´í„°ì…‹ {i+1}: {len(dataset)} ìƒ˜í”Œ")


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_multi_lmdb_dataset(config_type='weighted', **kwargs):
    """
    ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ Multi LMDB ë°ì´í„°ì…‹ ìƒì„±
    
    Args:
        config_type (str): 'weighted' ë˜ëŠ” 'concat'
        **kwargs: ë°ì´í„°ì…‹ ìƒì„±ìì— ì „ë‹¬í•  ì¸ìë“¤
    """
    if config_type == 'weighted':
        return MultiLMDBDataset(**kwargs)
    elif config_type == 'concat':
        return ConcatLMDBDataset(**kwargs)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” config_type: {config_type}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    # ê°€ì¤‘ì¹˜ ì ìš© ì˜ˆì‹œ
    lmdb_configs = [
        {'path': './data/text_in_wild.lmdb', 'weight': 1.0},
        {'path': './data/ocr_public_train.lmdb', 'weight': 0.8},
        {'path': './data/handwriting_ts5_paper_form.lmdb', 'weight': 0.5},
    ]
    
    dataset = MultiLMDBDataset(
        lmdb_configs=lmdb_configs,
        split='train',
        is_transform=True,
        short_size=640
    )
    
    print(f"ê²°í•©ëœ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    sample = dataset[0]
    print(f"ìƒ˜í”Œ í‚¤: {list(sample.keys())}") 