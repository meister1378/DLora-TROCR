#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Multi LMDB ì„¤ì • íŒŒì¼
ì—¬ëŸ¬ í•œêµ­ì–´ OCR LMDB ë°ì´í„°ì…‹ì„ ë™ì‹œì— ì‚¬ìš©í•˜ëŠ” ì„¤ì •
"""

# ëª¨ë¸ ì„¤ì • (checkpoint_7ep.pth í˜¸í™˜)
model = dict(
    type='FAST',
    backbone=dict(
        type='fast_backbone',
        config='/home/mango/ocr_test/FAST/config/fast/nas-configs/fast_base.config'
    ),
    neck=dict(
        type='fast_neck',
        config='/home/mango/ocr_test/FAST/config/fast/nas-configs/fast_base.config'
    ),
    detection_head=dict(
        type='fast_head',
        config='/home/mango/ocr_test/FAST/config/fast/nas-configs/fast_base.config',
        pooling_size=15,
        dropout_ratio=0.1,
        loss_text=dict(
            type='DiceLoss',
            loss_weight=0.5
        ),
        loss_kernel=dict(
            type='DiceLoss',
            loss_weight=1.0
        ),
        loss_emb=dict(
            type='EmbLoss_v1',
            feature_dim=4,
            loss_weight=0.25
        )
    )
)

# ë°©ë²• 1: ê°€ì¤‘ì¹˜ ì ìš© Multi LMDB ì„¤ì •
data = dict(
    batch_size=6,
    train=dict(
        type='MultiLMDBDataset',
        lmdb_configs=[
            {'path': './data/text_in_wild.lmdb', 'weight': 1.0},           # 100% ì‚¬ìš©
            {'path': './data/ocr_public_train.lmdb', 'weight': 0.8},       # 80% ì‚¬ìš©
            {'path': './data/finance_logistics_train.lmdb', 'weight': 0.6}, # 60% ì‚¬ìš©
            {'path': './data/handwriting_ts5_paper_form.lmdb', 'weight': 0.4}, # 40% ì‚¬ìš©
            {'path': './data/public_admin_train1.lmdb', 'weight': 0.5},    # 50% ì‚¬ìš©
        ],
        split='train',
        is_transform=True,
        img_size=640,
        short_size=640,
        with_rec=False,
        read_type='cv2'
    ),
    test=dict(
        type='ConcatLMDBDataset',
        lmdb_paths=[
            './data/ocr_public_val.lmdb',
            './data/finance_logistics_val.lmdb',
            './data/public_admin_val.lmdb'
        ],
        split='test',
        short_size=640,
        with_rec=False,
        read_type='cv2'
    )
)

# ë°©ë²• 2: ë‹¨ìˆœ ê²°í•© ì„¤ì • (ëª¨ë“  ë°ì´í„° ë™ì¼ ë¹„ìœ¨)
data_simple_concat = dict(
    batch_size=6,
    train=dict(
        type='ConcatLMDBDataset',
        lmdb_paths=[
            './data/text_in_wild.lmdb',
            './data/ocr_public_train.lmdb',
            './data/finance_logistics_train.lmdb',
            './data/handwriting_ts5_paper_form.lmdb',
            './data/public_admin_train1.lmdb',
        ],
        split='train',
        is_transform=True,
        img_size=640,
        short_size=640,
        with_rec=False,
        read_type='cv2'
    ),
    test=dict(
        type='ConcatLMDBDataset',
        lmdb_paths=[
            './data/ocr_public_val.lmdb',
            './data/finance_logistics_val.lmdb',
            './data/public_admin_val.lmdb'
        ],
        split='test',
        short_size=640
    )
)

# ë°©ë²• 3: íŠ¹ì • ë°ì´í„°ì…‹ë§Œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
data_selective = dict(
    batch_size=8,
    train=dict(
        type='MultiLMDBDataset',
        lmdb_configs=[
            # í…ìŠ¤íŠ¸ ì¸ì‹ì— ì¢‹ì€ ë°ì´í„°ì…‹ë“¤ë§Œ ì„ íƒ
            {'path': './data/text_in_wild.lmdb', 'weight': 1.5},        # 150% ì‚¬ìš© (ì¤‘ìš”)
            {'path': './data/ocr_public_train.lmdb', 'weight': 1.0},    # 100% ì‚¬ìš©
            {'path': './data/handwriting_ts5_paper_form.lmdb', 'weight': 0.3}, # 30% ì‚¬ìš© (ë³´ì¡°)
        ],
        split='train',
        is_transform=True,
        img_size=640,
        short_size=640
    ),
    test=dict(
        type='FAST_LMDB',  # ë‹¨ì¼ LMDB ì‚¬ìš©
        lmdb_path='./data/ocr_public_val.lmdb',
        split='test',
        short_size=640
    )
)

# ìµœì í™” ì„¤ì •
optimizer = dict(
    type='Adam',
    lr=8e-4,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ì¡°ê¸ˆ ë‚®ì€ í•™ìŠµë¥ 
    weight_decay=5e-4
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
lr_config = dict(
    policy='poly',
    power=0.9,
    min_lr=1e-7,
    by_epoch=False
)

# í›ˆë ¨ ì„¤ì •
total_epochs = 800  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ë” ë§ì€ ì—í¬í¬
checkpoint_config = dict(interval=50)

# ë¡œê·¸ ì„¤ì •
log_config = dict(
    interval=10,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type='TensorboardLoggerHook')
    ]
)

# í‰ê°€ ì„¤ì •
evaluation = dict(
    interval=50,
    metric='hmean'
)

# ê¸°íƒ€ ì„¤ì •
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/multi_lmdb_korean_ocr'
load_from = None
resume_from = None
workflow = [('train', 1)]

# GPU ì„¤ì •
gpu_ids = range(1)

# ì»¤ìŠ¤í…€ í›… ì„¤ì • (ì—í¬í¬ë§ˆë‹¤ ë°ì´í„°ì…‹ ì¬ìƒ˜í”Œë§)
custom_hooks = [
    dict(
        type='ResampleHook',
        priority='NORMAL',
        interval=1  # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì¬ìƒ˜í”Œë§
    )
]

# ë°ì´í„°ì…‹ ì¡°í•© ì „ëµë³„ ì„¤ëª…
dataset_strategies = {
    'balanced': {
        'description': 'ëª¨ë“  ë°ì´í„°ì…‹ì„ ê· ë“±í•˜ê²Œ ì‚¬ìš©',
        'use_case': 'ë°ì´í„° ë‹¤ì–‘ì„± ìµœëŒ€í™”',
        'config': 'data_simple_concat'
    },
    'weighted': {
        'description': 'ë°ì´í„°ì…‹ë³„ ê°€ì¤‘ì¹˜ ì ìš©',
        'use_case': 'íŠ¹ì • ë°ì´í„°ì…‹ ê°•ì¡°, í’ˆì§ˆ ì¡°ì ˆ',
        'config': 'data'
    },
    'selective': {
        'description': 'í•µì‹¬ ë°ì´í„°ì…‹ë§Œ ì„ íƒì  ì‚¬ìš©',
        'use_case': 'íŠ¹ì • ë„ë©”ì¸ ì§‘ì¤‘, ë¹ ë¥¸ ìˆ˜ë ´',
        'config': 'data_selective'
    }
}

# ì‹¤í–‰ ì‹œ ì „ëµ ì„ íƒ ê°€ì´ë“œ
print("=" * 60)
print("ğŸ“Š Multi LMDB ì„¤ì • ì „ëµ:")
for strategy, info in dataset_strategies.items():
    print(f"ğŸ”¹ {strategy}: {info['description']}")
    print(f"   ì‚¬ìš© ì‚¬ë¡€: {info['use_case']}")
    print(f"   ì„¤ì •: {info['config']}")
    print()
print("=" * 60) 