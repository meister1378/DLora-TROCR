#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Multi LMDB í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
ì—¬ëŸ¬ LMDB ë°ì´í„°ì…‹ì„ ë™ì‹œì— ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
"""

import os
import sys
import argparse
import time
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

# FAST ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from dataset.fast.multi_lmdb_dataset import MultiLMDBDataset, ConcatLMDBDataset

# LMDB ì €ì¥ ê²½ë¡œ (gvfs ë˜ëŠ” ë§ˆìš´íŠ¸ëœ ê²½ë¡œ)
LMDB_BASE_PATH = "/mnt/nas/ocr_dataset"  # ë˜ëŠ” gvfs ê²½ë¡œ
GVFS_LMDB_PATH = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"

def get_lmdb_path():
    """ì‹¤ì œ LMDB ì €ì¥ ê²½ë¡œ í™•ì¸ (ë¡œì»¬ ê²½ë¡œ ìš°ì„  - gvfsì—ì„œëŠ” LMDBê°€ ì‘ë™í•˜ì§€ ì•ŠìŒ)"""
    # ë¨¼ì € ë¡œì»¬ ë§ˆìš´íŠ¸ ê²½ë¡œ í™•ì¸ (LMDB í˜¸í™˜ì„±)
    if os.path.exists(f"{LMDB_BASE_PATH}/text_in_wild_train.lmdb"):
        print(f"âœ… LMDB ë°œê²¬ (ë¡œì»¬): {LMDB_BASE_PATH}")
        print("ğŸ’¡ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš© (LMDBëŠ” gvfsì—ì„œ ì‘ë™í•˜ì§€ ì•ŠìŒ)")
        return LMDB_BASE_PATH
    
    # gvfs ê²½ë¡œ í™•ì¸ (í•˜ì§€ë§Œ ê²½ê³ )
    elif os.path.exists(f"{GVFS_LMDB_PATH}/text_in_wild_train.lmdb"):
        print(f"âš ï¸ LMDB ë°œê²¬ (gvfs): {GVFS_LMDB_PATH}")
        print("âŒ ê²½ê³ : gvfsì—ì„œëŠ” LMDBê°€ ì •ìƒ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ í•´ê²° ë°©ë²•: LMDBë¥¼ ë¡œì»¬ë¡œ ë³µì‚¬í•˜ê±°ë‚˜ /mnt/nas/ocr_dataset ì‚¬ìš©")
        return None
    
    else:
        print("âŒ LMDB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   í™•ì¸í•œ ê²½ë¡œ 1 (ë¡œì»¬): {LMDB_BASE_PATH}")
        print(f"   í™•ì¸í•œ ê²½ë¡œ 2 (gvfs): {GVFS_LMDB_PATH}")
        return None

def get_all_lmdb_configs(base_path):
    """ëª¨ë“  ìƒì„±ëœ LMDB íŒŒì¼ì„ ì°¾ì•„ì„œ ì„¤ì • ìƒì„±"""
    all_train_lmdbs = []
    all_valid_lmdbs = []
    
    # ì˜ˆìƒë˜ëŠ” ëª¨ë“  LMDB íŒŒì¼ë“¤
    expected_lmdbs = [
        ("text_in_wild_train.lmdb", "text_in_wild_valid.lmdb", "Text in Wild", 1.0),
        ("public_admin_train.lmdb", "public_admin_valid.lmdb", "ê³µê³µí–‰ì •ë¬¸ì„œ", 1.0),
        ("ocr_public_train.lmdb", "ocr_public_valid.lmdb", "OCR ê³µê³µë°ì´í„°", 1.0),  # 0.8 â†’ 1.0
        ("finance_logistics_train.lmdb", "finance_logistics_valid.lmdb", "ê¸ˆìœµë¬¼ë¥˜", 1.0),  # 0.8 â†’ 1.0
        ("handwriting_train.lmdb", "handwriting_valid.lmdb", "ì†ê¸€ì”¨", 0.3),  # 0.6 â†’ 0.3 (ë” ë‚®ê²Œ)
    ]
    
    print("ğŸ” LMDB íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
    
    for train_file, valid_file, name, weight in expected_lmdbs:
        train_path = f"{base_path}/{train_file}"
        valid_path = f"{base_path}/{valid_file}"
        
        if os.path.exists(train_path):
            all_train_lmdbs.append({
                'path': train_path, 
                'weight': weight, 
                'name': f"{name} Train"
            })
            print(f"âœ… {name} Train: {train_path}")
        else:
            print(f"âŒ {name} Train: {train_path} (ì—†ìŒ)")
            
        if os.path.exists(valid_path):
            all_valid_lmdbs.append({
                'path': valid_path,
                'name': f"{name} Valid"
            })
            print(f"âœ… {name} Valid: {valid_path}")
        else:
            print(f"âŒ {name} Valid: {valid_path} (ì—†ìŒ)")
    
    print(f"\nğŸ“Š ë°œê²¬ëœ LMDB:")
    print(f"   - Train: {len(all_train_lmdbs)}ê°œ")
    print(f"   - Valid: {len(all_valid_lmdbs)}ê°œ")
    
    return all_train_lmdbs, all_valid_lmdbs


def parse_args():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='Multi LMDB í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸')
    
    parser.add_argument('--config', type=str, required=True,
                        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--strategy', type=str, default='weighted',
                        choices=['weighted', 'concat', 'selective'],
                        help='ë°ì´í„°ì…‹ ê²°í•© ì „ëµ')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, default='./work_dirs/multi_lmdb',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--gpu_ids', type=str, default='0',
                        help='ì‚¬ìš©í•  GPU ID (ì‰¼í‘œë¡œ êµ¬ë¶„)')
    parser.add_argument('--batch_size', type=int, default=None,
                        help='ë°°ì¹˜ í¬ê¸° (ì„¤ì • íŒŒì¼ ê°’ ë®ì–´ì“°ê¸°)')
    parser.add_argument('--epochs', type=int, default=None,
                        help='í›ˆë ¨ ì—í¬í¬ ìˆ˜ (ì„¤ì • íŒŒì¼ ê°’ ë®ì–´ì“°ê¸°)')
    parser.add_argument('--lr', type=float, default=None,
                        help='í•™ìŠµë¥  (ì„¤ì • íŒŒì¼ ê°’ ë®ì–´ì“°ê¸°)')
    parser.add_argument('--test_only', action='store_true',
                        help='í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰')
    
    return parser.parse_args()


def create_dataset(strategy, **kwargs):
    """ì „ëµì— ë”°ë¥¸ ë°ì´í„°ì…‹ ìƒì„±"""
    
    # ì‹¤ì œ LMDB ê²½ë¡œ í™•ì¸
    lmdb_base_path = get_lmdb_path()
    if lmdb_base_path is None:
        raise ValueError("LMDB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    # ëª¨ë“  LMDB ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    train_configs, valid_configs = get_all_lmdb_configs(lmdb_base_path)
    
    if not train_configs:
        raise ValueError("Train LMDB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    if strategy == 'weighted':
        # ê°€ì¤‘ì¹˜ ì ìš© ë°ì´í„°ì…‹ (ëª¨ë“  train LMDB ì‚¬ìš©)
        print(f"ğŸ”§ Weighted ì „ëµ: {len(train_configs)}ê°œ Train LMDB ì‚¬ìš©")
        return MultiLMDBDataset(
            lmdb_configs=train_configs,
            **kwargs
        )
    
    elif strategy == 'concat':
        # ë‹¨ìˆœ ê²°í•© ë°ì´í„°ì…‹ (ëª¨ë“  train LMDB ê· ë“± ì‚¬ìš©)
        train_paths = [config['path'] for config in train_configs]
        print(f"ğŸ”§ Concat ì „ëµ: {len(train_paths)}ê°œ Train LMDB ê· ë“± ê²°í•©")
        
        return ConcatLMDBDataset(
            lmdb_paths=train_paths,
            **kwargs
        )
    
    elif strategy == 'selective':
        # ì„ íƒì  ë°ì´í„°ì…‹ (ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ)
        selective_configs = []
        for config in train_configs:
            if 'text_in_wild' in config['path'].lower():
                selective_configs.append({'path': config['path'], 'weight': 1.5, 'name': config['name']})
            elif 'ocr_public' in config['path'].lower():
                selective_configs.append({'path': config['path'], 'weight': 1.0, 'name': config['name']})
            elif 'handwriting' in config['path'].lower():
                selective_configs.append({'path': config['path'], 'weight': 0.3, 'name': config['name']})
        
        print(f"ğŸ”§ Selective ì „ëµ: {len(selective_configs)}ê°œ í•µì‹¬ LMDB ì„ íƒ")
        return MultiLMDBDataset(
            lmdb_configs=selective_configs,
            **kwargs
        )
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì „ëµ: {strategy}")


def create_validation_dataset(**kwargs):
    """ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± (ëª¨ë“  valid LMDB ê²°í•©)"""
    
    # ì‹¤ì œ LMDB ê²½ë¡œ í™•ì¸
    lmdb_base_path = get_lmdb_path()
    if lmdb_base_path is None:
        raise ValueError("LMDB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    # ëª¨ë“  valid LMDB ê°€ì ¸ì˜¤ê¸°
    _, valid_configs = get_all_lmdb_configs(lmdb_base_path)
    
    if not valid_configs:
        print("âš ï¸ Valid LMDBê°€ ì—†ìŠµë‹ˆë‹¤. Train LMDBì˜ ì¼ë¶€ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None
    
    valid_paths = [config['path'] for config in valid_configs]
    print(f"ğŸ”§ ê²€ì¦ ë°ì´í„°ì…‹: {len(valid_paths)}ê°œ Valid LMDB ê²°í•©")
    
    return ConcatLMDBDataset(
        lmdb_paths=valid_paths,
        **kwargs
    )


def test_dataset_loading(strategy='weighted'):
    """ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ§ª {strategy} ì „ëµìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸")
    
    try:
        # í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = create_dataset(
            strategy=strategy,
            split='train',
            is_transform=True,
            img_size=(640, 640),  # ëª…ì‹œì ìœ¼ë¡œ img_size ì„¤ì •
            short_size=640
        )
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}")
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        sample = train_dataset[0]
        print(f"   - ìƒ˜í”Œ í‚¤: {list(sample.keys())}")
        print(f"   - ì´ë¯¸ì§€ í¬ê¸°: {sample['imgs'].shape}")
        
        # DataLoader í…ŒìŠ¤íŠ¸
        dataloader = DataLoader(
            train_dataset,
            batch_size=4,
            shuffle=True,
            num_workers=4
        )
        
        for batch_idx, batch in enumerate(dataloader):
            print(f"   - ë°°ì¹˜ {batch_idx+1}: {batch['imgs'].shape}")
            if batch_idx >= 1:  # 2ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                break
        
        print(f"âœ… DataLoader í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_args()
    
    print("ğŸš€ Multi LMDB í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print(f"ğŸ“Š ì „ëµ: {args.strategy}")
    print(f"âš™ï¸ ì„¤ì • íŒŒì¼: {args.config}")
    print("=" * 50)
    
    # GPU ì„¤ì •
    gpu_ids = [int(x) for x in args.gpu_ids.split(',')]
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_ids
    
    if args.test_only:
        # í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
        test_dataset_loading(args.strategy)
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    if not os.path.exists(args.config):
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        return
    
    # ì‹¤ì œ í›ˆë ¨ ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„
    # (ê¸°ì¡´ FAST í›ˆë ¨ ì½”ë“œë¥¼ Multi LMDB ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •)
    
    print("ğŸ”„ í›ˆë ¨ ì‹œì‘...")
    
    try:
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = create_dataset(
            strategy=args.strategy,
            split='train',
            is_transform=True,
            img_size=(640, 640),  # ëª…ì‹œì ìœ¼ë¡œ img_size ì„¤ì •
            short_size=640
        )
        
        # ê²€ì¦ ë°ì´í„°ì…‹ (ë‹¨ìˆœ ê²°í•©)
        val_dataset = create_validation_dataset(
            split='test',
            img_size=(640, 640),  # ëª…ì‹œì ìœ¼ë¡œ img_size ì„¤ì •
            short_size=640
        )
        
        print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_dataset):,}ê°œ ì´ë¯¸ì§€")
        print(f"   ğŸ’¡ ì°¸ê³ : ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ì€ {len(train_dataset)*25:,}ê°œ ì •ë„ (ì´ë¯¸ì§€ë‹¹ í‰ê·  25ê°œ)")
        if val_dataset:
            print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ ì´ë¯¸ì§€")
            print(f"   ğŸ’¡ ì°¸ê³ : ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ì€ {len(val_dataset)*25:,}ê°œ ì •ë„")
        else:
            print("ğŸ“Š ê²€ì¦ ë°ì´í„°: ì—†ìŒ (Train ë°ì´í„°ë§Œ ì‚¬ìš©)")
        
        # DataLoader ìƒì„±
        batch_size = args.batch_size or 8  # ì„±ëŠ¥ê³¼ ì•ˆì •ì„± ê· í˜•
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=16,  # ë°ì´í„° ë¡œë”© ì„±ëŠ¥ í–¥ìƒ
            pin_memory=True,
            drop_last=True,
            persistent_workers=True,  # ì›Œì»¤ ì¬ì‚¬ìš©ìœ¼ë¡œ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
            prefetch_factor=2  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
        )
        
        val_loader = None
        if val_dataset:
            val_loader = DataLoader(
                val_dataset,
                batch_size=1,  # ğŸ”¥ ë°°ì¹˜ í¬ê¸° 1ë¡œ ê°•ì œ (í¬ê¸° ë¶ˆì¼ì¹˜ í•´ê²°)
                shuffle=False,
                num_workers=8,  # ì›Œì»¤ ìˆ˜ ê°ì†Œ
                pin_memory=True,
                drop_last=True,  # ğŸ”¥ ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
                persistent_workers=False  # ğŸ”¥ validationì€ ê°„í—ì ì´ë¯€ë¡œ False
            )
        
        print(f"ğŸ”„ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"ğŸ”„ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        if val_loader:
            print(f"ğŸ”„ ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
        else:
            print("ğŸ”„ ê²€ì¦ ë°°ì¹˜ ìˆ˜: ì—†ìŒ")
        
        # FAST ëª¨ë¸ í›ˆë ¨ ë¡œì§ êµ¬í˜„
        print("ğŸ”§ FAST ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # Config íŒŒì¼ ë¡œë“œ
        from mmcv import Config
        from models import build_model
        
        cfg = Config.fromfile(args.config)
        
        # ëª¨ë¸ ìƒì„±
        model = build_model(cfg.model)
        model = model.to(device='cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì‚¬ì „í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (torch.compile() ì „ì— ì‹¤í–‰!)
        checkpoint_path = args.checkpoint or "./checkpoint_7ep.pth"
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“¦ ì‚¬ì „í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # ğŸ” ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë””ë²„ê·¸
            print(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ ìµœìƒìœ„ í‚¤ë“¤: {list(checkpoint.keys())}")
            
            # EMA ë˜ëŠ” ì§ì ‘ state_dict í™•ì¸
            if 'ema' in checkpoint:
                state_dict = checkpoint['ema']
                print("   - EMA ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©")
                print(f"   - EMA í‚¤ ê°œìˆ˜: {len(state_dict)}")
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                print("   - ì¼ë°˜ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©")
                print(f"   - state_dict í‚¤ ê°œìˆ˜: {len(state_dict)}")
            else:
                state_dict = checkpoint
                print("   - ì²´í¬í¬ì¸íŠ¸ ìì²´ë¥¼ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ë¡œ ì‚¬ìš©")
                print(f"   - ì§ì ‘ í‚¤ ê°œìˆ˜: {len(state_dict)}")
            
            # ğŸ” ì›ë³¸ í‚¤ ë¶„ì„
            original_keys = list(state_dict.keys())
            module_keys = [k for k in original_keys if k.startswith('module.')]
            non_module_keys = [k for k in original_keys if not k.startswith('module.')]
            print(f"   - ì›ë³¸ í‚¤ ë¶„ì„: ì´ {len(original_keys)}ê°œ")
            print(f"     â€¢ module. prefix: {len(module_keys)}ê°œ")
            print(f"     â€¢ ì¼ë°˜ í‚¤: {len(non_module_keys)}ê°œ")
            
            # í‚¤ì—ì„œ 'module.' ì œê±°
            new_state_dict = {}
            for key, value in state_dict.items():
                new_key = key.replace("module.", "")
                new_state_dict[new_key] = value
            
            print(f"   - ì •ë¦¬ëœ í‚¤ ê°œìˆ˜: {len(new_state_dict)}")
            
            # ğŸ” í˜„ì¬ ëª¨ë¸ í‚¤ ë¶„ì„ (ì»´íŒŒì¼ ì „)
            current_model_keys = list(model.state_dict().keys())
            print(f"   - í˜„ì¬ ëª¨ë¸ í‚¤ ê°œìˆ˜: {len(current_model_keys)}")
            
            # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
            missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ (ëˆ„ë½: {len(missing_keys)}, ì˜ˆìƒì™¸: {len(unexpected_keys)})")
            
            # ğŸ” ëˆ„ë½ê³¼ ì˜ˆìƒì™¸ í‚¤ ìƒì„¸ ë¶„ì„ (ë¬¸ì œê°€ ìˆì„ ë•Œë§Œ)
            if missing_keys:
                print(f"âŒ ëˆ„ë½ëœ í‚¤ë“¤ (ì²˜ìŒ 5ê°œ):")
                for key in missing_keys[:5]:
                    print(f"     - {key}")
                if len(missing_keys) > 5:
                    print(f"     ... ì¶”ê°€ {len(missing_keys) - 5}ê°œ")
            
            if unexpected_keys:
                print(f"âš ï¸ ì˜ˆìƒì™¸ í‚¤ë“¤ (ì²˜ìŒ 5ê°œ):")
                for key in unexpected_keys[:5]:
                    print(f"     - {key}")
                if len(unexpected_keys) > 5:
                    print(f"     ... ì¶”ê°€ {len(unexpected_keys) - 5}ê°œ")
        else:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            print("   - ì²´í¬í¬ì¸íŠ¸ ì—†ì´ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ğŸš€ PyTorch ì»´íŒŒì¼ ìµœì í™” (ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í›„ ì‹¤í–‰!)
        try:
            print("ğŸš€ PyTorch ì»´íŒŒì¼ ìµœì í™” ì ìš© ì¤‘...")
            model = torch.compile(model, mode='default')  # 'max-autotune'ì€ SM ë¶€ì¡±ìœ¼ë¡œ ê²½ê³  ë°œìƒ
            print("âœ… ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ - ì†ë„ í–¥ìƒ ì˜ˆìƒ")
        except Exception as e:
            print(f"âš ï¸ ì»´íŒŒì¼ ìµœì í™” ì‹¤íŒ¨ (ì •ìƒ ë™ì‘): {e}")
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        lr = args.lr or 5e-5  # NaN ë°©ì§€ë¥¼ ìœ„í•´ ë” ë‚®ì€ í•™ìŠµë¥ 
        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)
        
        # í˜¼í•© ì •ë°€ë„ í›ˆë ¨ ì„¤ì • (NaN ë¬¸ì œë¡œ ì¼ì‹œ ë¹„í™œì„±í™”)
        scaler = None  # ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        total_epochs = args.epochs or 10  # HuggingFace ìŠ¤íƒ€ì¼ ê¸°ë³¸ê°’
        scheduler = torch.optim.lr_scheduler.PolynomialLR(
            optimizer, total_iters=total_epochs, power=0.9
        )
        
        # ë°°ì¹˜ í¬ê¸° (gradient accumulation ì œê±°ë¡œ ë‹¨ìˆœí™”)
        effective_batch_size = batch_size
        
        # ğŸ”§ Validation ì£¼ê¸°ë¥¼ ë°°ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        total_batches_per_epoch = len(train_loader)
        validation_interval = total_batches_per_epoch // 100  # ì „ì²´ ë°°ì¹˜ì˜ 1%ë§ˆë‹¤
        
        print(f"ğŸ”§ í›ˆë ¨ ì„¤ì •:")
        print(f"   - í•™ìŠµë¥ : {lr}")
        print(f"   - ì´ ì—í¬í¬: {total_epochs}")
        print(f"   - ì˜µí‹°ë§ˆì´ì €: Adam")
        print(f"   - ìŠ¤ì¼€ì¤„ëŸ¬: PolynomialLR")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   - Step ê°œë… ì œê±°: ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì¦‰ì‹œ ì—…ë°ì´íŠ¸")
        print(f"   - ì›Œì»¤ ìˆ˜: 16 (ë°ì´í„° ë¡œë”© ìµœì í™”)")
        print(f"   - í˜¼í•© ì •ë°€ë„: ë¹„í™œì„±í™” (ì•ˆì •ì„±)")
        print(f"   - Gradient Clipping: í™œì„±í™”")
        print(f"   - Validation: ì „ì²´ ë°°ì¹˜ì˜ 1%ë§ˆë‹¤ ì‹¤í–‰ ({validation_interval:,} ë°°ì¹˜ë§ˆë‹¤)")
        print(f"   - ì²´í¬í¬ì¸íŠ¸: ì „ì²´ ë°°ì¹˜ì˜ 1%ë§ˆë‹¤ ì €ì¥ ({validation_interval:,} ë°°ì¹˜ë§ˆë‹¤)")
        
        # í›ˆë ¨ ë£¨í”„
        print("ğŸš€ Multi LMDB í›ˆë ¨ ì‹œì‘!")
        print(f"ğŸ“Š Validation ì„¤ì •: ì´ {total_batches_per_epoch:,} ë°°ì¹˜ ì¤‘ {validation_interval:,} ë°°ì¹˜ë§ˆë‹¤ ì‹¤í–‰")
        model.train()
        
        # global_step ë³€ìˆ˜ ì œê±° (step ê°œë… ì™„ì „ ì œê±°)
        
        def run_validation(batch_num, epoch_num):
            """Validation ì‹¤í–‰ í•¨ìˆ˜"""
            if not val_loader:
                return None
                
            tqdm.write(f"\nğŸ” Validation ì‹œì‘ (ì—í¬í¬ {epoch_num+1}, ë°°ì¹˜ {batch_num:,})")
            
            try:
                model.eval()
                val_loss = 0.0
                val_start_time = time.time()
                
                with torch.no_grad():
                    val_pbar = tqdm(val_loader, desc="ğŸ” ê²€ì¦ ì¤‘", unit="batch", leave=False)
                    for val_batch_idx, batch in enumerate(val_pbar):
                        # ğŸ” ë°°ì¹˜ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                        if batch is None:
                            tqdm.write(f"âš ï¸ Validation ë°°ì¹˜ {val_batch_idx}: batch is None, ê±´ë„ˆë›°ê¸°")
                            continue
                            
                        if 'imgs' not in batch or batch['imgs'] is None:
                            tqdm.write(f"âš ï¸ Validation ë°°ì¹˜ {val_batch_idx}: imgsê°€ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
                            continue
                        
                        # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ ë° GPUë¡œ ì´ë™
                        imgs = batch['imgs']
                        gt_texts = batch.get('gt_texts', None)
                        gt_kernels = batch.get('gt_kernels', None)
                        training_masks = batch.get('training_masks', None)
                        gt_instances = batch.get('gt_instances', None)
                        
                        if torch.cuda.is_available():
                            imgs = imgs.cuda()
                            if gt_texts is not None:
                                gt_texts = gt_texts.cuda()
                            if gt_kernels is not None:
                                gt_kernels = gt_kernels.cuda()
                            if training_masks is not None:
                                training_masks = training_masks.cuda()
                            if gt_instances is not None:
                                gt_instances = gt_instances.cuda()
                        
                        try:
                            outputs = model(
                                imgs,
                                gt_texts=gt_texts,
                                gt_kernels=gt_kernels,
                                training_masks=training_masks,
                                gt_instances=gt_instances
                            )
                            
                            if outputs is None:
                                tqdm.write(f"âš ï¸ Validation ë°°ì¹˜ {val_batch_idx}: outputs is None, ê±´ë„ˆë›°ê¸°")
                                continue
                            
                            loss_text = outputs['loss_text'].mean()
                            loss_kernels = outputs['loss_kernels'].mean()
                            loss_emb = outputs['loss_emb'].mean()
                            
                            total_loss = loss_text + loss_kernels + loss_emb
                            val_loss += total_loss.item()
                            
                            # validation progress bar ì—…ë°ì´íŠ¸
                            val_pbar.set_postfix({'Val_Loss': f"{total_loss.item():.4f}"})
                        except Exception as e:
                            tqdm.write(f"âš ï¸ Validation ë°°ì¹˜ {val_batch_idx} ì˜¤ë¥˜: {e}")
                            continue
                    
                    val_pbar.close()
                
                avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
                val_time = time.time() - val_start_time
                
                tqdm.write(f"ğŸ“Š Validation ì™„ë£Œ (ì—í¬í¬ {epoch_num+1}, ë°°ì¹˜ {batch_num:,}) - {val_time:.1f}ì´ˆ")
                tqdm.write(f"   - í‰ê·  ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
                
                return avg_val_loss
                
            except Exception as e:
                tqdm.write(f"âŒ Validation ì „ì²´ ì˜¤ë¥˜: {e}")
                return None
            finally:
                # ğŸ”¥ ë°˜ë“œì‹œ model.train()ìœ¼ë¡œ ë³µì›
                model.train()
                tqdm.write(f"âœ… ëª¨ë¸ ìƒíƒœë¥¼ train()ìœ¼ë¡œ ë³µì›")
        
        # ì—í¬í¬ ì§„í–‰ë¥  í‘œì‹œ
        epoch_pbar = tqdm(range(total_epochs), desc="ğŸ¯ ì—í¬í¬", unit="epoch")
        
        for epoch in epoch_pbar:
            epoch_pbar.set_description(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{total_epochs}")
            
            # í›ˆë ¨
            train_loss = 0.0
            model.train()
            # accumulation_loss ì œê±° (step ê°œë… ì™„ì „ ì œê±°)
            
            # ë°°ì¹˜ ì§„í–‰ë¥  í‘œì‹œ
            batch_pbar = tqdm(train_loader, desc=f"ğŸ“š í›ˆë ¨ ì¤‘", unit="batch", leave=False)
            
            for batch_idx, batch in enumerate(batch_pbar):
                # ë§¤ ë°°ì¹˜ë§ˆë‹¤ zero_grad (gradient accumulation ì œê±°)
                optimizer.zero_grad()
                
                # ğŸ” ë°°ì¹˜ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                if batch is None:
                    tqdm.write(f"âš ï¸ ë°°ì¹˜ {batch_idx}: batch is None, ê±´ë„ˆë›°ê¸°")
                    continue
                    
                if 'imgs' not in batch or batch['imgs'] is None:
                    tqdm.write(f"âš ï¸ ë°°ì¹˜ {batch_idx}: imgsê°€ ì—†ìŒ, ê±´ë„ˆë›°ê¸°")
                    continue
                
                # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ ë° GPUë¡œ ì´ë™
                imgs = batch['imgs']
                gt_texts = batch.get('gt_texts', None)
                gt_kernels = batch.get('gt_kernels', None) 
                training_masks = batch.get('training_masks', None)
                gt_instances = batch.get('gt_instances', None)
                
                if torch.cuda.is_available():
                    imgs = imgs.cuda()
                    if gt_texts is not None:
                        gt_texts = gt_texts.cuda()
                    if gt_kernels is not None:
                        gt_kernels = gt_kernels.cuda()
                    if training_masks is not None:
                        training_masks = training_masks.cuda()
                    if gt_instances is not None:
                        gt_instances = gt_instances.cuda()
                
                # Forward pass (FAST ëª¨ë¸ í˜•ì‹ì— ë§ê²Œ)
                try:
                    # í˜¼í•© ì •ë°€ë„ í›ˆë ¨
                    if scaler:
                        with torch.cuda.amp.autocast():
                            outputs = model(
                                imgs,
                                gt_texts=gt_texts,
                                gt_kernels=gt_kernels,
                                training_masks=training_masks,
                                gt_instances=gt_instances
                            )
                            
                            # ì†ì‹¤ ê³„ì‚° (FAST ëª¨ë¸ì˜ ë‹¤ì¤‘ ì†ì‹¤)
                            loss_text = outputs['loss_text'].mean()
                            loss_kernels = outputs['loss_kernels'].mean()
                            loss_emb = outputs['loss_emb'].mean()
                            
                            total_loss = loss_text + loss_kernels + loss_emb
                        
                        # Backward pass (ìŠ¤ì¼€ì¼ë§ ì ìš©)
                        scaler.scale(total_loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        outputs = model(
                            imgs,
                            gt_texts=gt_texts,
                            gt_kernels=gt_kernels,
                            training_masks=training_masks,
                            gt_instances=gt_instances
                        )
                        
                        # ì†ì‹¤ ê³„ì‚° (FAST ëª¨ë¸ì˜ ë‹¤ì¤‘ ì†ì‹¤)
                        loss_text = outputs['loss_text'].mean()
                        loss_kernels = outputs['loss_kernels'].mean()
                        loss_emb = outputs['loss_emb'].mean()
                        
                        total_loss = loss_text + loss_kernels + loss_emb
                        
                        # NaN ì²´í¬
                        if torch.isnan(total_loss):
                            print(f"âš ï¸ NaN ì†ì‹¤ ê°ì§€ë¨ - ë°°ì¹˜ {batch_idx} ê±´ë„ˆë›°ê¸°")
                            continue
                        
                        # Backward pass (gradient accumulation ì œê±°)
                        total_loss.backward()
                        
                        # accumulation_loss ì œê±°ë¨ (step ê°œë… ì™„ì „ ì œê±°)
                        
                        # ğŸ”¥ ë°°ì¹˜ ê¸°ì¤€ validation (gradient accumulationê³¼ ë…ë¦½ì )
                        if (batch_idx + 1) % validation_interval == 0:
                            val_loss = run_validation(batch_idx + 1, epoch)
                            if val_loss is not None:
                                tqdm.write(f"ğŸ¯ ë°°ì¹˜ {batch_idx + 1:,}: ê²€ì¦ ì†ì‹¤ = {val_loss:.4f}")
                            
                            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                            checkpoint_file = f"{args.output_dir}/checkpoint_latest.pth"
                            os.makedirs(args.output_dir, exist_ok=True)
                            torch.save({
                                'epoch': epoch + 1,
                                'batch_idx': batch_idx + 1,
                                'state_dict': model.state_dict(),
                                'optimizer': optimizer.state_dict(),
                                'scheduler': scheduler.state_dict(),
                                'train_loss': total_loss.item(),
                                'val_loss': val_loss if val_loss is not None else 0.0,
                            }, checkpoint_file)
                            tqdm.write(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_file} (ë°°ì¹˜ {batch_idx + 1:,})")
                        
                        # ë§¤ ë°°ì¹˜ë§ˆë‹¤ optimizer step (step ê°œë… ì œê±°)
                        # Gradient clipping (NaN ë°©ì§€)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        
                        optimizer.step()
                        
                        # ì†ì‹¤ ëˆ„ì  (ë§¤ ë°°ì¹˜)
                        train_loss += total_loss.item()
                        
                        # tqdm ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ê¸°ì¤€)
                        current_avg_loss = train_loss / (batch_idx + 1)
                        batch_pbar.set_postfix({
                            'Batch': batch_idx + 1,
                            'Loss': f"{total_loss.item():.4f}",
                            'Avg': f"{current_avg_loss:.4f}",
                            'Text': f"{loss_text.item():.3f}",
                            'Kernel': f"{loss_kernels.item():.3f}",
                            'Emb': f"{loss_emb.item():.3f}"
                        })
                            
                        # ì¤‘ìš”í•œ ë§ˆì¼ìŠ¤í†¤ë§Œ print ì¶œë ¥ (validation ì£¼ê¸°ì˜ ì ˆë°˜ë§ˆë‹¤)
                        if (batch_idx + 1) % max(1, validation_interval // 2) == 0:
                            tqdm.write(f"âœ… ë°°ì¹˜ {batch_idx + 1:,} - Loss: {current_avg_loss:.4f}")
                
                except Exception as e:
                    tqdm.write(f"âŒ í›ˆë ¨ ì˜¤ë¥˜ (ë°°ì¹˜ {batch_idx}): {e}")
                    continue
            
            # ë°°ì¹˜ progress bar ë‹«ê¸°
            batch_pbar.close()
            
            # ì—í¬í¬ í‰ê·  ì†ì‹¤ (ë°°ì¹˜ ê¸°ì¤€)
            avg_train_loss = train_loss / max(1, len(train_loader))
            
            # ì—í¬í¬ progress bar ì—…ë°ì´íŠ¸
            epoch_pbar.set_postfix({
                'Train_Loss': f"{avg_train_loss:.4f}",
                'Batches': len(train_loader),
                'Batch_Size': effective_batch_size
            })
            
            tqdm.write(f"ğŸ“Š ì—í¬í¬ {epoch+1} ì™„ë£Œ - í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
            tqdm.write(f"   - ë°°ì¹˜ ìˆ˜: {len(train_loader):,}, ë°°ì¹˜ í¬ê¸°: {effective_batch_size}")
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_checkpoint = f"{args.output_dir}/checkpoint_final.pth"
        os.makedirs(args.output_dir, exist_ok=True)
        torch.save({
            'epoch': total_epochs,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'scheduler': scheduler.state_dict(),
        }, final_checkpoint)
        
        # ì—í¬í¬ progress bar ë‹«ê¸°
        epoch_pbar.close()
        
        tqdm.write("âœ… Multi LMDB í›ˆë ¨ ì™„ë£Œ!")
        tqdm.write(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_checkpoint}")
        
    except Exception as e:
        tqdm.write(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # ì‹¤í–‰ ì˜ˆì‹œ:
    # 
    # 1. ëª¨ë“  LMDB í…ŒìŠ¤íŠ¸
    # python train_multi_lmdb.py --config config/fast/korean_ocr/multi_lmdb_config.py --test_only
    #
    # 2. Concat ì „ëµìœ¼ë¡œ í•™ìŠµ (ëª¨ë“  LMDB ê· ë“± ê²°í•©)
    # python train_multi_lmdb.py --config config/fast/korean_ocr/multi_lmdb_config.py --strategy concat --epochs 100
    #
    # 3. Weighted ì „ëµìœ¼ë¡œ í•™ìŠµ (ê°€ì¤‘ì¹˜ ì ìš©)
    # python train_multi_lmdb.py --config config/fast/korean_ocr/multi_lmdb_config.py --strategy weighted --epochs 100
    #
    # 4. GPU ì—¬ëŸ¬ ê°œ ì‚¬ìš©
    # python train_multi_lmdb.py --config config/fast/korean_ocr/multi_lmdb_config.py --strategy concat --gpu_ids 0,1,2,3
    
    print("ğŸš€ Multi LMDB í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 50)
    print("ğŸ“š ëª¨ë“  í•œêµ­ì–´ OCR LMDBë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤")
    print("")
    print("ğŸ”§ ì§€ì›í•˜ëŠ” ì „ëµ:")
    print("   â€¢ concat: ëª¨ë“  LMDB ê· ë“± ê²°í•©")
    print("   â€¢ weighted: ë°ì´í„°ì…‹ë³„ ê°€ì¤‘ì¹˜ ì ìš©") 
    print("   â€¢ selective: ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ ì„ íƒ")
    print("")
    print("ğŸ“ ìë™ ê²€ìƒ‰ ê²½ë¡œ:")
    print(f"   â€¢ {LMDB_BASE_PATH}")
    print(f"   â€¢ {GVFS_LMDB_PATH}")
    print("=" * 50)
    print("")
    
    main() 