#!/usr/bin/env python3
"""
LMDB ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì´ ì œëŒ€ë¡œ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
"""

import os
import sys
import cv2
import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import lmdb
import pickle

# FAST ê´€ë ¨ imports
sys.path.append('.')
from dataset.fast.fast_lmdb import FAST_LMDB

def visualize_annotations(img, sample, output_path):
    """ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™”"""
    
    # ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜
    if len(img.shape) == 3 and img.shape[2] == 3:
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        img_rgb = img
    
    # matplotlibë¡œ ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
    
    # ì›ë³¸ ì´ë¯¸ì§€
    ax1.imshow(img_rgb)
    ax1.set_title('Original Image', fontsize=16)
    ax1.axis('off')
    
    # ì–´ë…¸í…Œì´ì…˜ì´ í¬í•¨ëœ ì´ë¯¸ì§€
    ax2.imshow(img_rgb)
    ax2.set_title('Image with Annotations', fontsize=16)
    ax2.axis('off')
    
    # ì–´ë…¸í…Œì´ì…˜ ê·¸ë¦¬ê¸°
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
    color_idx = 0
    
    # GT í…ìŠ¤íŠ¸ ì˜ì—­ ê·¸ë¦¬ê¸°
    if 'gt_texts' in sample and len(sample['gt_texts']) > 0:
        gt_texts = sample['gt_texts']
        print(f"     ğŸ“ ì‹œê°í™”í•  GT í…ìŠ¤íŠ¸: {len(gt_texts)}ê°œ")
        
        # ì‹¤ì œ bbox ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        if 'gt_instances' in sample and len(sample['gt_instances']) > 0:
            gt_instances = sample['gt_instances']
            print(f"     ğŸ·ï¸ GT ì¸ìŠ¤í„´ìŠ¤ ì •ë³´: {len(gt_instances)}ê°œ")
            
            for i, (text, instance) in enumerate(zip(gt_texts, gt_instances)):
                if isinstance(instance, torch.Tensor):
                    # Tensorë¥¼ numpyë¡œ ë³€í™˜
                    bbox = instance.cpu().numpy()
                    if len(bbox) >= 4:  # ìµœì†Œ 4ê°œ ì¢Œí‘œ
                        # bbox ì¢Œí‘œ ì¶”ì¶œ (x1, y1, x2, y2 í˜•íƒœë¡œ ê°€ì •)
                        x1, y1, x2, y2 = bbox[:4]
                        
                        # ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
                        rect = patches.Rectangle(
                            (x1, y1), x2-x1, y2-y1,
                            linewidth=2, edgecolor=colors[color_idx % len(colors)],
                            facecolor='none'
                        )
                        ax2.add_patch(rect)
                        
                        # í…ìŠ¤íŠ¸ í‘œì‹œ
                        ax2.text(x1, y1-5, f'{i+1}: {text[:10]}...', 
                                fontsize=8, color=colors[color_idx % len(colors)],
                                bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
                        
                        color_idx += 1
                        print(f"       ğŸ“ í…ìŠ¤íŠ¸ {i+1}: {text[:20]}... (ì¢Œí‘œ: {x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})")
                else:
                    # bbox ì •ë³´ê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜
                    x, y = 50 + (i % 5) * 100, 50 + (i // 5) * 50
                    
                    rect = patches.Rectangle(
                        (x, y), 80, 30,
                        linewidth=2, edgecolor=colors[color_idx % len(colors)],
                        facecolor='none'
                    )
                    ax2.add_patch(rect)
                    
                    ax2.text(x, y-5, f'{i+1}: {text[:10]}...', 
                            fontsize=8, color=colors[color_idx % len(colors)],
                            bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
                    
                    color_idx += 1
        else:
            # bbox ì •ë³´ê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜
            for i, text in enumerate(gt_texts):
                x, y = 50 + (i % 5) * 100, 50 + (i // 5) * 50
                
                rect = patches.Rectangle(
                    (x, y), 80, 30,
                    linewidth=2, edgecolor=colors[color_idx % len(colors)],
                    facecolor='none'
                )
                ax2.add_patch(rect)
                
                ax2.text(x, y-5, f'{i+1}: {text[:10]}...', 
                        fontsize=8, color=colors[color_idx % len(colors)],
                        bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
                
                color_idx += 1
    
    # GT ì»¤ë„ ì˜ì—­ ê·¸ë¦¬ê¸° (ìˆëŠ” ê²½ìš°)
    if 'gt_kernels' in sample and len(sample['gt_kernels']) > 0:
        gt_kernels = sample['gt_kernels']
        print(f"     ğŸ¯ ì‹œê°í™”í•  GT ì»¤ë„: {len(gt_kernels)}ê°œ")
        
        for i, kernel in enumerate(gt_kernels):
            # ì»¤ë„ ì˜ì—­ì„ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œ
            x, y = 50 + (i % 5) * 100, 200 + (i // 5) * 50
            
            rect = patches.Rectangle(
                (x, y), 80, 30,
                linewidth=2, edgecolor='yellow',
                facecolor='none', linestyle='--'
            )
            ax2.add_patch(rect)
            
            ax2.text(x, y-5, f'K{i+1}', 
                    fontsize=8, color='yellow',
                    bbox=dict(boxstyle="round,pad=0.2", facecolor='black', alpha=0.8))
    
    plt.tight_layout()
    
    # ê²°ê³¼ ì €ì¥
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"     ğŸ’¾ ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™” ì €ì¥: {output_path}")
    
    plt.show()

def simple_visualize_annotations(img, sample, output_path):
    """ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™”"""
    
    # ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜
    if len(img.shape) == 3 and img.shape[2] == 3:
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        img_rgb = img
    
    # matplotlibë¡œ ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
    
    # ì›ë³¸ ì´ë¯¸ì§€
    ax1.imshow(img_rgb)
    ax1.set_title('Original Image', fontsize=16)
    ax1.axis('off')
    
    # ì–´ë…¸í…Œì´ì…˜ì´ í¬í•¨ëœ ì´ë¯¸ì§€
    ax2.imshow(img_rgb)
    ax2.set_title('Image with Real Annotations', fontsize=16)
    ax2.axis('off')
    
    # ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ ê·¸ë¦¬ê¸°
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
    color_idx = 0
    
    # GT í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ bbox ë§¤ì¹­
    if 'gt_texts' in sample and 'gt_instances' in sample:
        gt_texts = sample['gt_texts']
        gt_instances = sample['gt_instances']
        
        print(f"     ğŸ“Š ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ ì •ë³´:")
        print(f"       - í…ìŠ¤íŠ¸ ìˆ˜: {len(gt_texts)}")
        print(f"       - ì¸ìŠ¤í„´ìŠ¤ ìˆ˜: {len(gt_instances)}")
        
        # í…ìŠ¤íŠ¸ì™€ bbox ë§¤ì¹­
        for i, (text, instance) in enumerate(zip(gt_texts, gt_instances)):
            if isinstance(instance, torch.Tensor):
                # Tensorë¥¼ numpyë¡œ ë³€í™˜
                bbox = instance.cpu().numpy()
                
                if len(bbox) >= 4:  # ìµœì†Œ 4ê°œ ì¢Œí‘œ
                    # bbox ì¢Œí‘œ ì¶”ì¶œ (ì‹¤ì œ ì¢Œí‘œ)
                    x1, y1, x2, y2 = bbox[:4]
                    
                    # ì‹¤ì œ ìœ„ì¹˜ì— ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
                    rect = patches.Rectangle(
                        (x1, y1), x2-x1, y2-y1,
                        linewidth=2, edgecolor=colors[color_idx % len(colors)],
                        facecolor='none'
                    )
                    ax2.add_patch(rect)
                    
                    # í…ìŠ¤íŠ¸ í‘œì‹œ
                    ax2.text(x1, y1-5, f'{i+1}: {text[:10]}...', 
                            fontsize=8, color=colors[color_idx % len(colors)],
                            bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
                    
                    color_idx += 1
                    print(f"       ğŸ“ í…ìŠ¤íŠ¸ {i+1}: '{text[:20]}...' (ì¢Œí‘œ: {x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})")
                else:
                    print(f"       âš ï¸ í…ìŠ¤íŠ¸ {i+1}: bbox ì¢Œí‘œ ë¶€ì¡± ({len(bbox)}ê°œ)")
            else:
                print(f"       âš ï¸ í…ìŠ¤íŠ¸ {i+1}: Tensorê°€ ì•„ë‹˜ ({type(instance)})")
    
    plt.tight_layout()
    
    # ê²°ê³¼ ì €ì¥
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"     ğŸ’¾ ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™” ì €ì¥: {output_path}")
    
    plt.close()  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë‹«ê¸°

def get_raw_image_and_gt(lmdb_path, index):
    """ì›ë³¸ ì´ë¯¸ì§€ì™€ GT ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ ì—†ì´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        env = lmdb.open(lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)
        
        with env.begin(write=False) as txn:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img_key = f'image-{index:09d}'.encode()
            img_data = txn.get(img_key)
            if img_data is None:
                return None, None
            
            # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
            img_np = np.frombuffer(img_data, dtype=np.uint8)
            img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
            if img is None:
                return None, None
            
            # BGR -> RGB ë³€í™˜
            img = img[:, :, [2, 1, 0]]
            
            # GT ë°ì´í„° ë¡œë“œ
            gt_key = f'gt-{index:09d}'.encode()
            gt_data = txn.get(gt_key)
            if gt_data is None:
                return img, None
            
            # pickleë¡œ ì§ë ¬í™”ëœ GT ë°ì´í„° ë³µì›
            gt_info = pickle.loads(gt_data)
            
        env.close()
        return img, gt_info
        
    except Exception as e:
        print(f"     âš ï¸ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None, None

def verify_lmdb_data():
    """LMDB ë°ì´í„° ê²€ì¦"""
    
    print("ğŸ” LMDB ë°ì´í„° ê²€ì¦ ì‹œì‘")
    
    # LMDB ê²½ë¡œë“¤
    lmdb_paths = [
        '/mnt/nas/ocr_dataset/text_in_wild_train.lmdb',
        '/mnt/nas/ocr_dataset/public_admin_train.lmdb',
        '/mnt/nas/ocr_dataset/ocr_public_train.lmdb',
        '/mnt/nas/ocr_dataset/finance_logistics_train.lmdb',
        '/mnt/nas/ocr_dataset/handwriting_train.lmdb'
    ]
    
    for i, lmdb_path in enumerate(lmdb_paths):
        if os.path.exists(lmdb_path):
            print(f"\nğŸ“‚ ë°ì´í„°ì…‹ {i+1}: {os.path.basename(lmdb_path)}")
            
            try:
                # ì›ë³¸ ì„¤ì •ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±
                dataset = FAST_LMDB(
                    lmdb_path=lmdb_path,
                    split='train',
                    is_transform=True,
                    img_size=736,  # ì›ë³¸ ì„¤ì • ì‚¬ìš©
                    short_size=736,  # ì›ë³¸ ì„¤ì • ì‚¬ìš©
                    pooling_size=9,
                    read_type='cv2'
                )
                
                print(f"   ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
                
                # ì›ë³¸ ì´ë¯¸ì§€ì™€ GT ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                for sample_idx in range(min(5, len(dataset))):
                    print(f"   ğŸ“‹ ìƒ˜í”Œ {sample_idx+1}:")
                    
                    # ì›ë³¸ ì´ë¯¸ì§€ì™€ GT ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    raw_img, raw_gt = get_raw_image_and_gt(lmdb_path, sample_idx)
                    
                    if raw_img is not None:
                        print(f"     ğŸ–¼ï¸ ì›ë³¸ ì´ë¯¸ì§€: {raw_img.shape}, dtype: {raw_img.dtype}")
                        print(f"     ğŸ–¼ï¸ ë²”ìœ„: {raw_img.min()} ~ {raw_img.max()}")
                        
                        # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                        output_path = f"verify_sample_{i+1}_{sample_idx+1}_raw.jpg"
                        cv2.imwrite(output_path, cv2.cvtColor(raw_img, cv2.COLOR_RGB2BGR))
                        print(f"     ğŸ’¾ ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥: {output_path}")
                        
                        # GT ì •ë³´ ì¶œë ¥
                        if raw_gt is not None:
                            print(f"     ğŸ“ ì›ë³¸ GT ì •ë³´:")
                            if 'bboxes' in raw_gt:
                                bboxes = raw_gt['bboxes']
                                print(f"       - bboxes ê°œìˆ˜: {len(bboxes)}")
                                if len(bboxes) > 0:
                                    print(f"       - ì²« ë²ˆì§¸ bbox: {bboxes[0]}")
                            
                            if 'words' in raw_gt:
                                words = raw_gt['words']
                                print(f"       - words ê°œìˆ˜: {len(words)}")
                                if len(words) > 0:
                                    print(f"       - ì²« ë²ˆì§¸ word: '{words[0]}'")
                            
                            # ì›ë³¸ bbox ê·¸ë¦¬ê¸°
                            if 'bboxes' in raw_gt and 'words' in raw_gt:
                                bboxes = raw_gt['bboxes']
                                words = raw_gt['words']
                                
                                if len(bboxes) > 0:
                                    img_with_bbox = raw_img.copy()
                                    
                                    for j, (bbox, word) in enumerate(zip(bboxes, words)):
                                        if word != '###':  # ë¬´ì‹œí•  í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
                                            # bbox ì¢Œí‘œë¥¼ ì‹¤ì œ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                                            h, w = raw_img.shape[:2]
                                            bbox_pixels = np.array(bbox) * [w, h, w, h, w, h, w, h]
                                            bbox_pixels = bbox_pixels.reshape(-1, 2).astype(np.int32)
                                            
                                            # bbox ê·¸ë¦¬ê¸°
                                            cv2.polylines(img_with_bbox, [bbox_pixels], True, (0, 255, 0), 2)
                                            
                                            # í…ìŠ¤íŠ¸ í‘œì‹œ
                                            center = np.mean(bbox_pixels, axis=0).astype(int)
                                            cv2.putText(img_with_bbox, f"{j+1}:{word[:10]}", 
                                                      (center[0], center[1]), 
                                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
                                    
                                    # bboxê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€ ì €ì¥
                                    bbox_output_path = f"verify_sample_{i+1}_{sample_idx+1}_raw_with_bbox.jpg"
                                    cv2.imwrite(bbox_output_path, cv2.cvtColor(img_with_bbox, cv2.COLOR_RGB2BGR))
                                    print(f"     ğŸ’¾ bbox ì´ë¯¸ì§€ ì €ì¥: {bbox_output_path}")
                                else:
                                    print(f"     âš ï¸ bbox ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                        else:
                            print(f"     âš ï¸ GT ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                    else:
                        print(f"     âŒ ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                        
                        print()  # ë¹ˆ ì¤„ ì¶”ê°€
                    
                    print()
                
                # ë°ì´í„°ì…‹ í†µê³„
                print(f"   ğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
                print(f"     - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
                
                # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì¶”ê°€ ê²€ì¦
                import random
                random_indices = random.sample(range(len(dataset)), min(10, len(dataset)))
                
                valid_samples = 0
                total_annotations = 0
                
                for idx in random_indices:
                    try:
                        sample = dataset[idx]
                        if 'imgs' in sample and 'gt_texts' in sample:
                            valid_samples += 1
                            total_annotations += len(sample['gt_texts'])
                    except Exception as e:
                        print(f"     âš ï¸ ìƒ˜í”Œ {idx} ì˜¤ë¥˜: {e}")
                
                print(f"     - ìœ íš¨í•œ ìƒ˜í”Œ: {valid_samples}/{len(random_indices)}")
                print(f"     - í‰ê·  ì–´ë…¸í…Œì´ì…˜ ìˆ˜: {total_annotations/max(1, valid_samples):.1f}")
                
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"   âŒ íŒŒì¼ ì—†ìŒ: {lmdb_path}")

def test_with_original_config():
    """ì›ë³¸ ì„¤ì •ìœ¼ë¡œ detection í…ŒìŠ¤íŠ¸"""
    
    print("\nğŸ”§ ì›ë³¸ ì„¤ì •ìœ¼ë¡œ detection í…ŒìŠ¤íŠ¸")
    
    try:
        from mmcv import Config
        from models import build_model
        
        # ì›ë³¸ ì„¤ì • íŒŒì¼ ì‚¬ìš©
        config_path = "config/fast/ic15/fast_sample_finetune.py"
        checkpoint_path = "outputs/validation_test/checkpoint_latest.pth"
        
        if not os.path.exists(config_path):
            print(f"âŒ ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}")
            return
        
        if not os.path.exists(checkpoint_path):
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {checkpoint_path}")
            return
        
        # ì„¤ì • ë¡œë“œ
        cfg = Config.fromfile(config_path)
        print(f"ğŸ“Š ì›ë³¸ ì„¤ì •:")
        print(f"   - ì´ë¯¸ì§€ í¬ê¸°: {cfg.data.train.img_size}")
        print(f"   - test_cfg: {cfg.test_cfg}")
        
        # ëª¨ë¸ ë¡œë“œ
        model = build_model(cfg.model)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(checkpoint_path, map_location=device)
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # í‚¤ ì •ë¦¬
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("module.", "")
            new_state_dict[new_key] = value
        
        model.load_state_dict(new_state_dict, strict=False)
        model.eval()
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ detection
        test_image_path = "../WF_2000_5320060_0005_0005.jpg"
        
        if os.path.exists(test_image_path):
            from dataset.utils import get_img, scale_aligned_short
            import torchvision.transforms as transforms
            from PIL import Image
            
            img = get_img(test_image_path, read_type='cv2')
            original_img = img.copy()
            
            # ì›ë³¸ ì„¤ì •ì— ë§ì¶° í¬ê¸° ì¡°ì •
            img = scale_aligned_short(img, short_size=736)  # ì›ë³¸ ì„¤ì •
            
            # PIL ë³€í™˜ ë° ì •ê·œí™”
            img_pil = Image.fromarray(img)
            img_pil = img_pil.convert('RGB')
            
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            img_tensor = transform(img_pil).unsqueeze(0).to(device)
            
            # ë©”íƒ€ë°ì´í„°
            img_meta = {
                'filename': [os.path.basename(test_image_path)],
                'org_img_size': [original_img.shape[:2]],
                'img_size': [img.shape[:2]]
            }
            
            # ì¶”ë¡ 
            with torch.no_grad():
                data = {
                    'imgs': img_tensor,
                    'img_metas': img_meta,
                    'cfg': cfg
                }
                outputs = model(**data)
            
            # ê²°ê³¼ ë¶„ì„
            if 'results' in outputs and len(outputs['results']) > 0:
                results = outputs['results'][0]
                bboxes = results.get('bboxes', [])
                scores = results.get('scores', [])
                
                print(f"ğŸ¯ ì›ë³¸ ì„¤ì • ê²€ì¶œ ê²°ê³¼: {len(bboxes)}ê°œ")
                
                # ì›ë³¸ ì„ê³„ê°’ ì ìš©
                high_score_count = sum(1 for score in scores if score > 0.88)  # ì›ë³¸ ì„ê³„ê°’
                print(f"ğŸ“ˆ ë†’ì€ ì‹ ë¢°ë„ (0.88+): {high_score_count}ê°œ")
                
                if len(scores) > 0:
                    max_score = max(scores)
                    avg_score = sum(scores) / len(scores)
                    print(f"ğŸ“Š ìµœê³  ìŠ¤ì½”ì–´: {max_score:.3f}")
                    print(f"ğŸ“Š í‰ê·  ìŠ¤ì½”ì–´: {avg_score:.3f}")
                
                print(f"âœ… ì›ë³¸ ì„¤ì • detection ì™„ë£Œ")
            else:
                print(f"âŒ ì›ë³¸ ì„¤ì • ê²€ì¶œ ê²°ê³¼ ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ ì›ë³¸ ì„¤ì • í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ LMDB ë°ì´í„° ê²€ì¦ ì‹œì‘")
    
    # LMDB ë°ì´í„° ê²€ì¦
    verify_lmdb_data()
    
    # ì›ë³¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_with_original_config()
    
    print("\nğŸ‰ LMDB ë°ì´í„° ê²€ì¦ ì™„ë£Œ!")

if __name__ == '__main__':
    main() 