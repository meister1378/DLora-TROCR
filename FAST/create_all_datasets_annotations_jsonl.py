#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ëª¨ë“  í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ì„ train/validë¡œ ë¶„ë¦¬í•˜ì—¬ LMDB ìƒì„±í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸
ì „ì²´ ë°ì´í„°ì…‹ ë³€í™˜ (ì œí•œ ì—†ìŒ)
ë°ì´í„°ì…‹ë³„ ì „ìš© í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©ìœ¼ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
"""

import os
import sys
import json
import pickle
import time
import numpy as np
import cv2
# import torch
from tqdm import tqdm
import lmdb
import random
import gc
import subprocess
from pathlib import Path
import orjson
import ijson  # ìŠ¤íŠ¸ë¦¬ë° JSON íŒŒì‹±
import orjson
# import bigjson  # ì œê±°ë¨ - orjson ì‚¬ìš©
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp
from tqdm.contrib.concurrent import process_map, thread_map
import uuid
import psutil
import time
from typing import Optional

# FAST ëª¨ë“ˆ import
sys.path.append('.')
sys.path.append('FAST')  # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ë“¤ì„ ìœ„í•œ ê²½ë¡œ
from dataset.fast.fast_lmdb import FAST_LMDB

# ğŸš€ ìµœì í™”ëœ lookup ë”•ì…”ë„ˆë¦¬ë“¤ (pickle ë°©ì‹)
optimized_lookups = {}

# bbox ë””ë²„ê·¸ ì¶œë ¥ í”Œë˜ê·¸ (ì „ì—­ ë³€ìˆ˜)
bbox_debug_flags = {
    'text_in_wild': False,
    'public_admin': False,
    'ocr_public': False,
    'finance_logistics': False,
    'handwriting': False
}

def _order_points_clockwise(points: np.ndarray) -> np.ndarray:
    """ì‚¬ê°í˜• 4ì ì„ TL, TR, BR, BL ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ë ¬í•œë‹¤."""
    if points.shape != (4, 2):
        points = points.reshape(-1, 2)[:4]
    s = points.sum(axis=1)
    diff = np.diff(points, axis=1).reshape(-1)
    tl = points[np.argmin(s)]
    br = points[np.argmax(s)]
    tr = points[np.argmin(diff)]
    bl = points[np.argmax(diff)]
    return np.array([tl, tr, br, bl], dtype=np.float32)

def normalize_ic15_clockwise_flat8(bbox_flat8):
    """[x1,y1,x2,y2,x3,y3,x4,y4]ì„ IC15 í‘œì¤€ ìˆœì„œ(TL,TR,BR,BL)ë¡œ ì •ê·œí™”í•œë‹¤."""
    try:
        if not isinstance(bbox_flat8, (list, tuple)) or len(bbox_flat8) != 8:
            return bbox_flat8
        pts = np.array(bbox_flat8, dtype=np.float32).reshape(-1, 2)
        ordered = _order_points_clockwise(pts)
        return ordered.reshape(-1).astype(float).tolist()
    except Exception:
        return bbox_flat8

def load_optimized_lookup(dataset_name):
    """ìµœì í™”ëœ lookup ë”•ì…”ë„ˆë¦¬ë¥¼ pickleì—ì„œ ë¡œë“œ (5-10ë°° ë¹ ë¦„)"""
    try:
        if dataset_name in optimized_lookups:
            return optimized_lookups[dataset_name]
        
        # 1. ì••ì¶•ëœ pickle íŒŒì¼ ì‹œë„ (ìš°ì„ ìˆœìœ„)
        pkl_gz_file = f"FAST/lookup_{dataset_name}.pkl.gz"
        if os.path.exists(pkl_gz_file):
            print(f"  ğŸš€ ì••ì¶•ëœ pickle ë”•ì…”ë„ˆë¦¬ ë¡œë“œ: {pkl_gz_file}")
            import gzip
            with gzip.open(pkl_gz_file, 'rb') as f:
                lookup_dict = pickle.load(f)
            optimized_lookups[dataset_name] = lookup_dict
            return lookup_dict
        
        # 2. ì¼ë°˜ pickle íŒŒì¼ ì‹œë„
        pkl_file = f"FAST/lookup_{dataset_name}.pkl"
        if os.path.exists(pkl_file):
            print(f"  ğŸš€ pickle ë”•ì…”ë„ˆë¦¬ ë¡œë“œ: {pkl_file}")
            with open(pkl_file, 'rb') as f:
                lookup_dict = pickle.load(f)
            optimized_lookups[dataset_name] = lookup_dict
            return lookup_dict
        
        # 3. ê¸°ì¡´ Python ëª¨ë“ˆ ë°©ì‹ (fallback)
        module_name = f"optimized_lookup_{dataset_name}"
        if os.path.exists(f"FAST/{module_name}.py"):
            print(f"  ğŸŒ fallback Python í•¨ìˆ˜ ë¡œë“œ: {module_name}")
            module = __import__(module_name)
            lookup_func = getattr(module, f"lookup_{dataset_name}")
            optimized_lookups[dataset_name] = lookup_func
            return lookup_func
        
        print(f"  âš ï¸ ìµœì í™”ëœ lookup íŒŒì¼ ì—†ìŒ: {dataset_name} (fallback ì‚¬ìš©)")
        return None
            
    except Exception as e:
        print(f"  âš ï¸ ìµœì í™”ëœ lookup ë¡œë“œ ì‹¤íŒ¨: {e} (fallback ì‚¬ìš©)")
        return None

def scan_directory_recursive(directory, target_filename, extensions=('.jpg', '.png', '.jpeg')):
    """os.scandirì„ ì‚¬ìš©í•œ ì¬ê·€ì  íŒŒì¼ ê²€ìƒ‰ (os.walkë³´ë‹¤ ë¹ ë¦„)"""
    if not os.path.exists(directory):
        return None
    
    try:
        with os.scandir(directory) as entries:
            for entry in entries:
                if entry.is_file() and entry.name == target_filename:
                    return entry.path
                elif entry.is_dir() and not entry.name.startswith('.'):
                    # ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ë””ë ‰í† ë¦¬ ê²€ìƒ‰
                    result = scan_directory_recursive(entry.path, target_filename, extensions)
                    if result:
                        return result
    except (OSError, PermissionError):
        pass
    
    return None

def optimized_find_image_path(filename, base_path, dataset_name, fallback_cache=None):
    """ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸° (pickle ë”•ì…”ë„ˆë¦¬ ìš°ì„ , fallback ì§€ì›)"""
    # 1. ìµœì í™”ëœ lookup ë”•ì…”ë„ˆë¦¬/í•¨ìˆ˜ ì‹œë„
    lookup_obj = load_optimized_lookup(dataset_name)
    if lookup_obj:
        try:
            # pickle ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ìƒˆë¡œìš´ ë°©ì‹)
            if isinstance(lookup_obj, dict):
                # ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ (O(1), ì´ˆê³ ì†)
                if filename in lookup_obj:
                    result = lookup_obj[filename]
                    if result and os.path.exists(result):
                        return result
                
                # í™•ì¥ì ì¶”ê°€í•´ì„œ ì‹œë„
                for ext in ['.png', '.jpg', '.jpeg']:
                    filename_with_ext = f"{filename}{ext}"
                    if filename_with_ext in lookup_obj:
                        result = lookup_obj[filename_with_ext]
                        if result and os.path.exists(result):
                            return result
                    
                    # í™•ì¥ì ì œê±°í•´ì„œ ì‹œë„
                    filename_no_ext = filename.replace(ext, '')
                    if filename_no_ext in lookup_obj:
                        result = lookup_obj[filename_no_ext]
                        if result and os.path.exists(result):
                            return result
            
            # ê¸°ì¡´ í•¨ìˆ˜ì¸ ê²½ìš° (fallback)
            elif callable(lookup_obj):
                result = lookup_obj(filename, base_path)
                if result and os.path.exists(result):
                    return result
                    
        except Exception as e:
            print(f"  âš ï¸ ìµœì í™”ëœ lookup ì‹¤íŒ¨: {e}")
    
    # 2. Fallback ìºì‹œ ì‚¬ìš©
    if fallback_cache and filename in fallback_cache:
        return fallback_cache[filename]
    
    # 3. ë§ˆì§€ë§‰ fallback: os.scandir ì¬ê·€ ê²€ìƒ‰ (os.walkë³´ë‹¤ ë¹ ë¦„)
    print(f"  ğŸš€ Fallback os.scandir ì‚¬ìš©: {filename}")
    return scan_directory_recursive(base_path, filename)
    
    return None

# FTP ë§ˆìš´íŠ¸ëœ ë°ì´í„°ì…‹ ê¸°ë³¸ ê²½ë¡œ
FTP_BASE_PATH = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"
# ë¡œì»¬ LMDB ìƒì„± ê²½ë¡œ
LOCAL_OUTPUT_PATH = "/mnt/nas/ocr_dataset"
# í•©ì³ì§„ JSON íŒŒì¼ ê²½ë¡œ
MERGED_JSON_PATH = "/home/mango/ocr_test/FAST/json_merged"

# ì¸ì‹(Recognition) í¬ë¡­ì„ LMDBì— ë™ë°˜ ì €ì¥í•˜ê¸° ìœ„í•œ ìœ í‹¸

def _iter_recog_crops_bytes(img_bytes: bytes, gt_info: dict):
    """ì¸ì‹ìš© í¬ë¡­ì„ ë©”ëª¨ë¦¬ì—ì„œ ìƒì„±í•˜ì—¬ (jpg_bytes, label) ë¡œ yield"""
    try:
        img_np = np.frombuffer(img_bytes, dtype=np.uint8)
        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
        if img is None:
            return
        h, w = img.shape[:2]

        bboxes = gt_info.get('bboxes', []) or []
        words = gt_info.get('words', []) or []
        count = min(len(bboxes), len(words))
        for j in range(count):
            coords = bboxes[j]
            text = words[j]
            if text is None:
                continue
            label = str(text).strip()
            if label == "" or label == "###":
                continue
            try:
                xs = coords[0::2]
                ys = coords[1::2]
                x1 = max(0, min(int(min(xs)), w - 1))
                y1 = max(0, min(int(min(ys)), h - 1))
                x2 = max(0, min(int(max(xs)), w))
                y2 = max(0, min(int(max(ys)), h))
                if x2 <= x1 or y2 <= y1:
                    continue
                crop = img[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                ok, enc = cv2.imencode('.jpg', crop)
                if not ok:
                    continue
                yield enc.tobytes(), label
            except Exception:
                continue
    except Exception:
        return

def scan_images_with_scandir(image_dir, extensions=('.jpg', '.jpeg', '.png', '.bmp')):
    """scandirì„ ì‚¬ìš©í•œ ë¹ ë¥¸ ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰"""
    image_files = {}
    
    try:
        with os.scandir(image_dir) as entries:
            for entry in entries:
                if entry.is_file() and entry.name.lower().endswith(extensions):
                    image_files[entry.name] = entry.path
    except Exception as e:
        print(f"âš ï¸ scandir ì‹¤íŒ¨: {e}")
    
    return image_files

def scan_images_recursive_with_scandir(base_dir, extensions=('.jpg', '.jpeg', '.png', '.bmp')):
    """os.scandirì„ ì‚¬ìš©í•œ ì¬ê·€ì  ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰ (os.walk ëŒ€ì²´)"""
    image_files = {}
    
    def _scan_recursive(directory):
        try:
            with os.scandir(directory) as entries:
                for entry in entries:
                    if entry.is_file() and entry.name.lower().endswith(extensions):
                        image_files[entry.name] = entry.path
                    elif entry.is_dir() and not entry.name.startswith('.'):
                        _scan_recursive(entry.path)
        except (OSError, PermissionError) as e:
            print(f"âš ï¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨ {directory}: {e}")
    
    if os.path.exists(base_dir):
        _scan_recursive(base_dir)
    
    return image_files

def build_image_cache_parallel(base_path, dataset_type):
    """ë³‘ë ¬ë¡œ ì´ë¯¸ì§€ ê²½ë¡œ ìºì‹œ êµ¬ì¶•"""
    print(f"ğŸ”„ ë³‘ë ¬ ì´ë¯¸ì§€ ê²½ë¡œ ìºì‹œ êµ¬ì¶• ì¤‘... ({dataset_type})")
    cache = {}
    
    def scan_directory(directory):
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº” í•¨ìˆ˜"""
        local_cache = {}
        if os.path.exists(directory):
            try:
                with os.scandir(directory) as entries:
                    for entry in entries:
                        if entry.is_file() and entry.name.lower().endswith(('.jpg', '.png', '.jpeg')):
                            local_cache[entry.name] = entry.path
            except Exception as e:
                print(f"âš ï¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨: {directory} - {e}")
        return local_cache
    
    # ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ ëª©ë¡
    scan_dirs = []
    
    if dataset_type == "ocr_public":
        for split in ['Training', 'Validation']:
            scan_dirs.append(f"{base_path}/{split}/01.ì›ì²œë°ì´í„°")
    
    elif dataset_type == "finance_logistics":
        for split in ['Training', 'Validation']:
            scan_dirs.append(f"{base_path}/{split}/01.ì›ì²œë°ì´í„°")
    
    elif dataset_type == "handwriting":
        for split in ['1.Training', '2.Validation']:
            scan_dirs.append(f"{base_path}/{split}/ì›ì²œë°ì´í„°")
    
    elif dataset_type == "public_admin":
        for train_num in [1, 2, 3]:
            scan_dirs.append(f"{base_path}/Training/[ì›ì²œ]train{train_num}/02.ì›ì²œë°ì´í„°(jpg)")
        scan_dirs.append(f"{base_path}/Validation/[ì›ì²œ]validation/02.ì›ì²œë°ì´í„°(Jpg)")
    
    # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
    max_workers = min(mp.cpu_count(), 16)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_dir = {executor.submit(scan_directory, dir_path): dir_path for dir_path in scan_dirs}
        
        for future in tqdm(as_completed(future_to_dir), total=len(scan_dirs), desc="ë””ë ‰í† ë¦¬ ìŠ¤ìº”"):
            local_cache = future.result()
            cache.update(local_cache)
    
    print(f"  âœ… ìºì‹œ ì™„ë£Œ: {len(cache)}ê°œ íŒŒì¼")
    return cache

def cleanup_memory():
    """ê°•ë ¥í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    # 1. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    collected = gc.collect()
    
    # 2. CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except:
        pass
    
    # 3. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    
    print(f"  ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬: GC {collected}ê°œ í•´ì œ, í˜„ì¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")

def is_ftp_mounted():
    """gvfs FTPê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    gvfs_path = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\ocr_dataset"
    return os.path.exists(gvfs_path)

def load_json_with_orjson(json_path):
    """JSON íŒŒì¼ì„ orjsonìœ¼ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ê³ ì†)"""
    print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(json_path)
    file_size_gb = file_size / (1024**3)
    print(f"  ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_gb:.2f} GB")
    
    try:
        # orjsonìœ¼ë¡œ ë¡œë“œ (ê³ ì†)
        print("  ğŸš€ orjsonìœ¼ë¡œ ë¡œë“œ ì¤‘...")
        with open(json_path, 'rb') as f:
            data = orjson.loads(f.read())
        print("  âœ… orjson ë¡œë“œ ì„±ê³µ")
        return data, None  # (data, file_handle)
        
    except MemoryError:
        print("  âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± - ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„...")
        cleanup_memory()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„
        with open(json_path, 'rb') as f:
            data = orjson.loads(f.read())
        print("  âœ… orjson ë¡œë“œ ì„±ê³µ (ì¬ì‹œë„)")
        return data, None
        
    except Exception as e:
        print(f"  âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def safe_close_file(file_handle):
    """íŒŒì¼ í•¸ë“¤ì„ ì•ˆì „í•˜ê²Œ ë‹«ëŠ” í•¨ìˆ˜"""
    if file_handle:
        try:
            file_handle.close()
        except:
            pass

# ============================================================================
# Text in the Wild ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_text_in_wild_train_valid(max_samples=500):
    """Text in the wild train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª Text in the wild train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/13.í•œêµ­ì–´ê¸€ìì²´/04. Text in the wild_230209_add"
    json_path = f"{MERGED_JSON_PATH}/textinthewild_data_info.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_train.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_valid.lmdb"
    
    if os.path.exists(json_path):
        create_lmdb_text_in_wild_split(base_path, json_path, train_output_path, valid_output_path, 
                                     train_ratio=0.9, max_samples=max_samples, random_seed=42)
        cleanup_memory()
    else:
        print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")

def create_lmdb_text_in_wild_split(base_path, json_path, train_output_path, valid_output_path, train_ratio=0.9, max_samples=None, random_seed=42):
    """Text in the wild LMDB ìƒì„± (í•©ì³ì§„ JSONì—ì„œ train/valid ë¶„í• )"""
    print(f"ğŸ§ª Text in the wild LMDB ìƒì„± ì¤‘... (train/valid {train_ratio}:{1-train_ratio} ë¶„í• )")
    
    random.seed(random_seed)
    os.makedirs(os.path.dirname(train_output_path), exist_ok=True)
    os.makedirs(os.path.dirname(valid_output_path), exist_ok=True)
    
    # Text in the WildëŠ” ì‘ì€ íŒŒì¼ì´ë¯€ë¡œ orjsonìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
    print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    
    # orjsonì„ ì‚¬ìš©í•œ ì „ì²´ JSON ë¡œë“œ (ë¹ ë¥¸ ì²˜ë¦¬)
    with open(json_path, 'rb') as f:
        data = orjson.loads(f.read())
    
    # imagesì™€ annotations ì²˜ë¦¬ (ë¹ ë¥¸ Python ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
    images_info = {img['id']: img for img in data.get('images', [])}
    image_annotations = {}
    for ann in data.get('annotations', []):
        img_id = ann['image_id']
        if img_id not in image_annotations:
            image_annotations[img_id] = []
        image_annotations[img_id].append(ann)
    
    # JSON ë°ì´í„° ì¦‰ì‹œ í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    del data
    gc.collect()
    print(f"  ğŸ—‘ï¸ JSON ì›ë³¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ì–´ì„œ train/valid ë¶„í• 
    img_ids = list(images_info.keys())
    
    if max_samples and len(img_ids) > max_samples:
        img_ids = img_ids[:max_samples]
        print(f"ğŸ“Š {max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ")
    elif max_samples is None:
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ì²˜ë¦¬: {len(img_ids)}ê°œ ì´ë¯¸ì§€")
    
    random.shuffle(img_ids)
    train_size = int(len(img_ids) * train_ratio)
    train_img_ids = img_ids[:train_size]
    valid_img_ids = img_ids[train_size:]
    
    print(f"ğŸ“Š ì´ {len(img_ids)}ê°œ ì´ë¯¸ì§€ -> Train: {len(train_img_ids)}ê°œ, Valid: {len(valid_img_ids)}ê°œ")
    
    # Training LMDB ìƒì„±
    create_lmdb_text_in_wild_from_ids(base_path, images_info, image_annotations, train_img_ids, train_output_path, "Training")
    
    # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
    del train_img_ids
    gc.collect()
    print(f"ğŸ—‘ï¸ Training ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # Validation LMDB ìƒì„±
    create_lmdb_text_in_wild_from_ids(base_path, images_info, image_annotations, valid_img_ids, valid_output_path, "Validation")
    
    # ëª¨ë“  ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ
    del valid_img_ids
    del images_info
    del image_annotations
    gc.collect()
    print(f"ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

# ============================================================================
# ê³µí†µ ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================

def process_single_text_wild_image(args):
    """Text in Wild ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    img_id, img_info, annotations, base_path, lookup_dict = args
    
    try:
        # íŒŒì¼ëª…ì— í™•ì¥ì ì¶”ê°€ (.jpg)
        img_file_name = img_info['file_name']
        if not img_file_name.endswith('.jpg'):
            img_file_name = f"{img_file_name}.jpg"
        
        # ğŸš€ ìµœì í™”ëœ ê²½ë¡œ ì°¾ê¸° (ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ì ‘ê·¼)
        img_path = None
        if lookup_dict and isinstance(lookup_dict, dict):
            if img_file_name in lookup_dict:
                img_path = lookup_dict[img_file_name]
            else:
                # í™•ì¥ì ë³€í˜• ì‹œë„
                for ext in ['.png', '.jpeg']:
                    alt_name = img_file_name.replace('.jpg', ext)
                    if alt_name in lookup_dict:
                        img_path = lookup_dict[alt_name]
                        break
        
        # fallback: íƒ€ì…ë³„ ê²½ë¡œ ë¡œì§
        if not img_path:
            img_type = img_info.get('type', 'book')
            if img_type == "book":
                image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
            elif img_type == "sign":
                image_dir = f"{base_path}/01_textinthewild_signboard_images_new/01_textinthewild_signboard_images_new/Signboard"
            elif img_type == "traffic sign":
                image_dir = f"{base_path}/01_textinthewild_traffic_sign_images_new/01_textinthewild_traffic_sign_images_new/Traffic_Sign"
            elif img_type == "product":
                image_dir = f"{base_path}/01_textinthewild_goods_images_new/01_textinthewild_goods_images_new/Goods"
            else:
                image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
            
            img_path = os.path.join(image_dir, img_file_name)
        
        if not img_path or not os.path.exists(img_path):
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
        bboxes = []
        words = []
        
        for ann in annotations:
            # bbox: [x, y, width, height] -> [x1, y1, x2, y1, x2, y2, x1, y2]
            x, y, w, h = ann['bbox']
            x1, y1, x2, y2 = x, y, x + w, y + h
            
            # ì›ë³¸ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ë¦¬í•‘ ì—†ìŒ)
            pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
            
            # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
            if not bbox_debug_flags['text_in_wild']:
                print(f"ğŸ“‹ Text in Wild bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                bbox_debug_flags['text_in_wild'] = True
            
            bboxes.append(pixel_coords)
            words.append(ann['text'])
        
        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_info['file_name']
        }
        
        return (img_id, img_data, gt_info)
        
    except Exception as e:
        return None

def process_single_public_admin_image(args):
    """ê³µê³µí–‰ì •ë¬¸ì„œ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    img_info, annotations, base_path, lookup_dict, dataset_lookup_name, image_path_cache = args
    
    try:
        # íŒŒì¼ëª… ì¶”ì¶œ
        img_file_name = img_info.get('image.file.name', '')
        image_category = img_info.get('image.category', '')
        image_make_code = img_info.get('image.make.code', '')
        image_make_year = img_info.get('image.make.year', '')
        
        if not img_file_name:
            return None
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸°
        img_path = optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
        if not img_path or not os.path.exists(img_path):
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
        bboxes = []
        words = []
        img_id = img_info.get('id')
        
        for ann in annotations:
            # annotation.bbox: [x, y, width, height] -> [x1, y1, x2, y1, x2, y2, x1, y2]
            x, y, w, h = ann['annotation.bbox']
            x1, y1, x2, y2 = x, y, x + w, y + h
            
            # ì›ë³¸ í”½ì…€ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (8ê°œ ì¢Œí‘œ í˜•íƒœë¡œ í†µì¼)
            pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
            
            # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
            if not bbox_debug_flags['public_admin']:
                print(f"ğŸ“‹ Public Admin bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                bbox_debug_flags['public_admin'] = True
            
            bboxes.append(pixel_coords)
            words.append(ann['annotation.text'])
        
        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_file_name
        }
        
        return (img_id, img_data, gt_info)
        
    except Exception as e:
        return None

def process_single_ocr_public_image(args):
    """OCR ê³µê³µ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    img_info, annotations, base_path, dataset_lookup_name, image_path_cache = args
    
    try:
        img_file_name = img_info.get('file_name', '')
        
        # í™•ì¥ì í™•ì¸
        if not img_file_name.endswith(('.jpg', '.png', '.jpeg')):
            img_file_name = f"{img_file_name}.jpg"
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸°
        img_path = optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
        if not img_path or not os.path.exists(img_path):
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
        bboxes = []
        words = []
        img_id = img_info.get('id')
        
        for ann in annotations:
            bbox_coords = ann['bbox']
            try:
                # ì›ë³¸ bboxê°€ [x1, x2, x3, x4, y1, y2, y3, y4] í˜•íƒœì¸ì§€ í™•ì¸
                if len(bbox_coords) == 8:
                    # x, y ì¢Œí‘œ ë¶„ë¦¬
                    x_coords = bbox_coords[:4]  # [x1, x2, x3, x4]
                    y_coords = bbox_coords[4:]  # [y1, y2, y3, y4]
                    
                    # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¬êµ¬ì„±: [x1, y1, x2, y2, x3, y3, x4, y4]
                    pixel_coords = []
                    for i in range(4):
                        pixel_coords.extend([x_coords[i], y_coords[i]])
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['ocr_public']:
                        print(f"ğŸ“‹ OCR Public bbox í˜•íƒœ: ì›ë³¸ [x1={x_coords[0]}, x2={x_coords[1]}, x3={x_coords[2]}, x4={x_coords[3]}, y1={y_coords[0]}, y2={y_coords[1]}, y3={y_coords[2]}, y4={y_coords[3]}] -> ìˆ˜ì • [x1={pixel_coords[0]}, y1={pixel_coords[1]}, x2={pixel_coords[2]}, y2={pixel_coords[3]}, x3={pixel_coords[4]}, y3={pixel_coords[5]}, x4={pixel_coords[6]}, y4={pixel_coords[7]}]")
                        bbox_debug_flags['ocr_public'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann['text'])
                else:
                    # ê¸°ì¡´ ë¡œì§ (8ê°œê°€ ì•„ë‹Œ ê²½ìš°)
                    x_coords = [bbox_coords[0], bbox_coords[2], bbox_coords[4], bbox_coords[6]]
                    y_coords = [bbox_coords[1], bbox_coords[3], bbox_coords[5], bbox_coords[7]]
                    
                    # ì›ë³¸ í”½ì…€ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    pixel_coords = [
                        x_coords[0], y_coords[0],
                        x_coords[1], y_coords[1],
                        x_coords[2], y_coords[2],
                        x_coords[3], y_coords[3]
                    ]
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    bboxes.append(pixel_coords)
                    words.append(ann['text'])
            except (IndexError, TypeError):
                try:
                    # 4ê°œ ì¢Œí‘œì¸ì§€ í™•ì¸ (x, y, w, h)
                    x, y, w, h = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                    x1, y1, x2, y2 = x, y, x + w, y + h
                    
                    # ì›ë³¸ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ë¦¬í•‘ ì—†ìŒ)
                    pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['ocr_public']:
                        print(f"ğŸ“‹ OCR Public bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                        bbox_debug_flags['ocr_public'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann['text'])
                except (IndexError, TypeError):
                    continue
        
        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_file_name
        }
        
        return (img_id, img_data, gt_info)
        
    except Exception as e:
        return None

def process_single_finance_logistics_image(args):
    """ê¸ˆìœµë¬¼ë¥˜ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    sub_dataset, img_info_data, annotations_for_dataset = args
    
    if not annotations_for_dataset:
        return None
        
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_info_data['file_path'], 'rb') as f:
            img_data = f.read()
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ)
        bboxes = []
        words = []
        img_w = img_info_data['width']
        img_h = img_info_data['height']
        
        for ann in annotations_for_dataset:
            bbox_coords = ann.get('bbox', [])
            
            try:
                # ğŸš€ bigjson Arrayë¥¼ ì•ˆì „í•˜ê²Œ Python listë¡œ ë³€í™˜
                if hasattr(bbox_coords, '__getitem__') and not isinstance(bbox_coords, list):
                    # bigjson Arrayì¸ ê²½ìš° Python listë¡œ ë³€í™˜
                    bbox_list = []
                    try:
                        for i in range(8):  # ìµœëŒ€ 8ê°œê¹Œì§€ ì‹œë„
                            bbox_list.append(bbox_coords[i])
                    except (IndexError, TypeError):
                        # 8ê°œë³´ë‹¤ ì ìœ¼ë©´ 4ê°œ ì‹œë„
                        try:
                            bbox_list = []
                            for i in range(4):
                                bbox_list.append(bbox_coords[i])
                        except (IndexError, TypeError):
                            continue
                    bbox_coords = bbox_list
                
                # 8ê°œ ì¢Œí‘œì¸ì§€ í™•ì¸ (4ê°œ ì ì˜ x,y)
                if len(bbox_coords) >= 8:
                    # merged JSONì—ì„œ [x1,x2,x3,x4,y1,y2,y3,y4] í˜•íƒœë¥¼ [x1,y1,x2,y2,x3,y3,x4,y4]ë¡œ ë³€í™˜
                    x_coords = bbox_coords[:4]  # [x1, x2, x3, x4]
                    y_coords = bbox_coords[4:]  # [y1, y2, y3, y4]
                    
                    # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¬êµ¬ì„±: [x1, y1, x2, y2, x3, y3, x4, y4]
                    pixel_coords = []
                    for i in range(4):
                        pixel_coords.extend([x_coords[i], y_coords[i]])
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['finance_logistics']:
                        print(f"ğŸ“‹ Finance Logistics bbox í˜•íƒœ: ì›ë³¸ [x1={x_coords[0]}, x2={x_coords[1]}, x3={x_coords[2]}, x4={x_coords[3]}, y1={y_coords[0]}, y2={y_coords[1]}, y3={y_coords[2]}, y4={y_coords[3]}] -> ìˆ˜ì • [x1={pixel_coords[0]}, y1={pixel_coords[1]}, x2={pixel_coords[2]}, y2={pixel_coords[3]}, x3={pixel_coords[4]}, y3={pixel_coords[5]}, x4={pixel_coords[6]}, y4={pixel_coords[7]}]")
                        bbox_debug_flags['finance_logistics'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann.get('text', ''))
                elif len(bbox_coords) >= 4:
                    # 4ê°œ ì¢Œí‘œì¸ì§€ í™•ì¸ (x, y, w, h)
                    x, y, w, h = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                    x1, y1, x2, y2 = x, y, x + w, y + h
                    
                    # ì›ë³¸ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ë¦¬í•‘ ì—†ìŒ)
                    pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['finance_logistics']:
                        print(f"ğŸ“‹ Finance Logistics bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                        bbox_debug_flags['finance_logistics'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann.get('text', ''))
            except (IndexError, TypeError, ValueError):
                # bboxê°€ ì˜ëª»ëœ í˜•ì‹ì´ë©´ ê±´ë„ˆë›°ê¸°
                continue
        
        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_info_data['filename']
        }
        
        return (sub_dataset, img_data, gt_info)
        
    except Exception as e:
        return None

def process_single_handwriting_image(args):
    """ì†ê¸€ì”¨ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    args í˜•íƒœ:
      - (img_file_name, img_info_data) (ì´ì „ í˜¸í™˜)
      - (img_file_name, img_info_data, annotations_for_image) (ì‹ ê·œ)
    """
    if len(args) == 3:
        img_file_name, img_info_data, annotations_for_image = args
    else:
        img_file_name, img_info_data = args
        annotations_for_image = []

    try:
        img_path = img_info_data['file_path']
        if not os.path.exists(img_path):
            return None

        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()

        bboxes = []
        words = []

        # 1) ìš°ì„  merged JSONì˜ annotations ì‚¬ìš© (ìˆì„ ê²½ìš°)
        if annotations_for_image:
            for ann in annotations_for_image:
                bbox_coords = ann.get('bbox', [])
                try:
                    if isinstance(bbox_coords, list) and len(bbox_coords) >= 8:
                        # [x1,x2,x3,x4,y1,y2,y3,y4] -> interleave -> normalize
                        x_coords = bbox_coords[:4]
                        y_coords = bbox_coords[4:]
                        pixel_coords = []
                        for i in range(4):
                            pixel_coords.extend([x_coords[i], y_coords[i]])
                        pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                        if not bbox_debug_flags['handwriting']:
                            print(f"ğŸ“‹ Handwriting bbox(merged) í˜•íƒœ: x={x_coords}, y={y_coords} -> {pixel_coords}")
                            bbox_debug_flags['handwriting'] = True
                        bboxes.append(pixel_coords)
                        words.append(ann.get('text', ''))
                    elif isinstance(bbox_coords, list) and len(bbox_coords) >= 4:
                        # [x,y,w,h]
                        x, y, w, h = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                        x1, y1, x2, y2 = x, y, x + w, y + h
                        pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                        pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                        bboxes.append(pixel_coords)
                        words.append(ann.get('text', ''))
                except Exception:
                    continue

        # 2) fallback: original_json_pathì—ì„œ ì§ì ‘ ì½ê¸°
        if not bboxes:
            original_json_path = img_info_data.get("original_json_path", "")
            # ê²½ë¡œê°€ ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ base_pathì™€ í•©ì¹˜ëŠ” ì²˜ë¦¬ëŠ” ìƒìœ„ì—ì„œ ë³´ì¥
            if original_json_path and os.path.exists(original_json_path):
                try:
                    json_data, json_file_handle = load_json_with_orjson(original_json_path)
                    try:
                        if 'bbox' in json_data:
                            for bbox_info in json_data['bbox']:
                                x_coords = bbox_info.get('x')
                                y_coords = bbox_info.get('y')
                                if x_coords is None or y_coords is None:
                                    continue
                                pixel_coords = []
                                for i in range(4):
                                    pixel_coords.extend([x_coords[i], y_coords[i]])
                                pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                                if not bbox_debug_flags['handwriting']:
                                    print(f"ğŸ“‹ Handwriting bbox(orig) í˜•íƒœ: x={x_coords}, y={y_coords} -> {pixel_coords}")
                                    bbox_debug_flags['handwriting'] = True
                                bboxes.append(pixel_coords)
                                words.append(bbox_info.get('data', ''))
                    finally:
                        safe_close_file(json_file_handle)
                except Exception:
                    pass

        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_info_data['filename']
        }
        return (img_file_name, img_data, gt_info)
    except Exception:
        return None

def create_parallel_lmdb_from_args(process_args, output_path, split_name, process_func, max_workers=None):
    """ê³µí†µ ë³‘ë ¬ LMDB ìƒì„± í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½í˜•)"""
    print(f"ğŸš€ {split_name} ë³‘ë ¬ LMDB ìƒì„± ì¤‘... ({len(process_args)}ê°œ ìƒ˜í”Œ)")
    
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìµœì  ì›Œì»¤ ìˆ˜
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 16)  # ì›Œì»¤ ìˆ˜ë¥¼ 16ê°œë¡œ ì¦ê°€
    print(f"  ğŸ”§ ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ")
    
    # LMDB í™˜ê²½ ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    env = lmdb.open(output_path, 
                    map_size=1099511627776,  # 1TB
                    writemap=True,  # ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”
                    meminit=False,  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë¹„í™œì„±í™”
                    map_async=True)  # ë¹„ë™ê¸° ë§µí•‘
    
    print(f"  ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ + ì¦‰ì‹œ ì €ì¥ ì‹œì‘...")
    
    idx = 0
    recog_saved_total = 0
    start_time = time.time()
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    chunk_size = 10000  # 10000ê°œì”© ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # process_argsë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ìˆœíšŒ
        for chunk_start in tqdm(range(0, len(process_args), chunk_size), desc=f"{split_name} ì²­í¬ ì²˜ë¦¬"):
            chunk_end = min(chunk_start + chunk_size, len(process_args))
            chunk_args = process_args[chunk_start:chunk_end]
            
            # í˜„ì¬ ì²­í¬ì˜ futureë§Œ ìƒì„±
            futures = {executor.submit(process_func, arg) for arg in chunk_args}
            
            # ë” ì‘ì€ íŠ¸ëœì­ì…˜ ë‹¨ìœ„ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
            txn_batch_size = 500  # 500ê°œì”© íŠ¸ëœì­ì…˜ ë¶„í•  (ë” ì‘ê²Œ)
            batch_count = 0
            txn = None
            
            # í˜„ì¬ ì²­í¬ì˜ ì‘ì—…ë§Œ ì²˜ë¦¬
            for future in as_completed(futures):
                result = future.result()
                
                if result is not None:
                    img_id, img_data, gt_info = result
                    
                    # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘ (ë°°ì¹˜ ë‹¨ìœ„)
                    if batch_count % txn_batch_size == 0:
                        if txn is not None:
                            txn.commit()  # ì´ì „ íŠ¸ëœì­ì…˜ ì»¤ë°‹
                        txn = env.begin(write=True)  # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘
                    
                    # Detection ì›ë³¸ ë¯¸ì €ì¥, word-levelë§Œ ì €ì¥
                    try:
                        for crop_jpg, label in _iter_recog_crops_bytes(img_data, gt_info):
                            img_key = f'image-{idx:09d}'.encode()
                            lab_key = f'label-{idx:09d}'.encode()
                            txn.put(img_key, crop_jpg)
                            txn.put(lab_key, label.encode('utf-8', errors='ignore'))
                            idx += 1
                            batch_count += 1
                        
                    except Exception:
                        pass
                    
                    # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
                    del result
                    del img_data
                    del gt_info
            
            # ë§ˆì§€ë§‰ íŠ¸ëœì­ì…˜ ì»¤ë°‹
            if txn is not None:
                txn.commit()
            del chunk_args, futures
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            print(f"  ğŸ—‘ï¸ ì²­í¬ {chunk_start//chunk_size + 1} ì™„ë£Œ: {idx}ê°œ ì²˜ë¦¬, GC {collected}ê°œ í•´ì œ")
        
        # ë§ˆì§€ë§‰ì— ìƒ˜í”Œ ìˆ˜ ì €ì¥ (word-level ìƒ˜í”Œ ìˆ˜)
        txn = env.begin(write=True)
        txn.put('num-samples'.encode(), str(idx).encode())
        txn.commit()
        try:
            env.sync()
        except Exception:
            pass
    
    env.close()
    
    # ìµœì¢… ë©”ëª¨ë¦¬ í•´ì œ
    del process_args
    gc.collect()
    
    total_time = time.time() - start_time
    speed = idx / total_time if total_time > 0 else 0
    print(f"âœ… {split_name} ë³‘ë ¬ LMDB ìƒì„± ì™„ë£Œ: {idx}ê°œ í¬ë¡­ ìƒ˜í”Œ")
    print(f"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {speed:.1f} samples/sec")
    print(f"ğŸ—‘ï¸ {split_name} ëª¨ë“  ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")


# ============================================================================
# JSONL ì €ì¥ ìœ í‹¸ë¦¬í‹° (ERNIE SFT í¬ë§·)
# ============================================================================

def _build_ocr_jsonl_record(image_path: str, words: list[str]) -> dict:
    """ERNIE SFT VL í¬ë§·ì˜ í•œ ë ˆì½”ë“œë¥¼ ìƒì„±í•œë‹¤.

    - image_info: ë¡œì»¬ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - text_info: [mask("OCR:"), no_mask(ì •ë‹µ í…ìŠ¤íŠ¸)]
    """
    # ë‹¨ìˆœ ê²°í•©(ê³µë°± êµ¬ë¶„). í•„ìš”ì‹œ ì¤„ë°”ê¿ˆ ê·œì¹™ìœ¼ë¡œ ë°”ê¿”ë„ ë¨
    target_text = " ".join([w for w in (words or []) if isinstance(w, str) and w.strip()])
    return {
        "image_info": [
            {"matched_text_index": 0, "image_url": image_path},
        ],
        "text_info": [
            {"text": "OCR:", "tag": "mask"},
            {"text": target_text, "tag": "no_mask"},
        ],
    }


def _save_crops_and_make_records(image_path: str, bboxes: list[list[float]], words: list[str], crop_dir: str, prefix: str, mirror_root: Optional[str] = None) -> list[dict]:
    """ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ bboxesë¡œ í¬ë¡­ì„ ì €ì¥í•˜ê³  JSONL ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    
    mirror_rootê°€ ì£¼ì–´ì§€ë©´, image_pathì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ mirror_rootì— ëŒ€í•œ ìƒëŒ€ ê²½ë¡œë¥¼ ê³„ì‚°í•˜ì—¬
    crop_dir ì•„ë˜ì— ë™ì¼í•œ í´ë” íŠ¸ë¦¬ë¡œ ë¶„ì‚° ì €ì¥í•œë‹¤. (ê¸°ì¡´ ê²½ë¡œì²˜ëŸ¼ ì—¬ëŸ¬ í´ë”)
    mirror_rootê°€ ì—†ìœ¼ë©´ image_pathì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ ë§ˆì§€ë§‰ 2ë‹¨ê³„ë¥¼ ì‚¬ìš©í•´ ë¶„ì‚° ì €ì¥í•œë‹¤.
    """
    try:
        # ë¶„ì‚° ì €ì¥ ëŒ€ìƒ ë””ë ‰í† ë¦¬ ê²°ì •
        dst_dir = crop_dir
        try:
            parent_dir = os.path.dirname(image_path)
            if mirror_root and os.path.exists(mirror_root):
                # base_path ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ íŠ¸ë¦¬ë¥¼ ìœ ì§€
                rel_dir = os.path.relpath(parent_dir, mirror_root)
                # ë„ˆë¬´ ìƒìœ„(../)ë¡œ ì˜¬ë¼ê°€ëŠ” ê²½ìš° ë°©ì§€
                if not rel_dir.startswith(".."):
                    dst_dir = os.path.join(crop_dir, rel_dir)
                else:
                    # mirror_rootì™€ ë¬´ê´€í•˜ë©´ ë§ˆì§€ë§‰ 2ë‹¨ê³„ë§Œ ìœ ì§€
                    parts = Path(parent_dir).parts
                    tail_parts = parts[-2:] if len(parts) >= 2 else parts
                    dst_dir = os.path.join(crop_dir, *tail_parts)
            else:
                # ê¸°ë³¸: ë§ˆì§€ë§‰ 2ë‹¨ê³„ í´ë” êµ¬ì„±
                parts = Path(parent_dir).parts
                tail_parts = parts[-2:] if len(parts) >= 2 else parts
                dst_dir = os.path.join(crop_dir, *tail_parts)
        except Exception:
            dst_dir = crop_dir

        os.makedirs(dst_dir, exist_ok=True)
        img = cv2.imread(image_path, cv2.IMREAD_COLOR)
        if img is None:
            return []
        h, w = img.shape[:2]

        records: list[dict] = []
        count = min(len(bboxes or []), len(words or []))
        stem = Path(image_path).stem
        for i in range(count):
            coords = bboxes[i]
            label = str(words[i] if words[i] is not None else "").strip()
            if not label or label == "###":
                continue
            try:
                xs = [coords[0], coords[2], coords[4], coords[6]]
                ys = [coords[1], coords[3], coords[5], coords[7]]
                x1 = max(0, min(int(min(xs)), w - 1))
                y1 = max(0, min(int(min(ys)), h - 1))
                x2 = max(0, min(int(max(xs)), w))
                y2 = max(0, min(int(max(ys)), h))
                if x2 <= x1 or y2 <= y1:
                    continue
                crop = img[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                # ê³ ìœ  íŒŒì¼ëª… ìƒì„±
                uniq = uuid.uuid4().hex[:8]
                out_path = os.path.join(dst_dir, f"{prefix}_{stem}_{i:06d}_{uniq}.jpg")
                ok = cv2.imwrite(out_path, crop)
                if not ok:
                    continue
                records.append(_build_ocr_jsonl_record(out_path, [label]))
            except Exception:
                continue
        return records
    except Exception:
        return []


def create_parallel_jsonl_from_args(process_args, output_path, split_name, to_json_func, max_workers=None, max_total_samples: Optional[int] = None):
    """ê³µí†µ ë³‘ë ¬ JSONL ìƒì„± í•¨ìˆ˜.

    process_args: ì‘ì—… ì¸ì ë¦¬ìŠ¤íŠ¸
    to_json_func: ê° ì¸ìì—ì„œ (image_path, words) ë˜ëŠ” dict(JSON ì§ë ¬í™” ê°€ëŠ¥) ë°˜í™˜
    """
    print(f"ğŸš€ {split_name} ë³‘ë ¬ JSONL ìƒì„± ì¤‘... ({len(process_args)}ê°œ ìƒ˜í”Œ)")

    # ì›Œì»¤/ì²­í¬ í™˜ê²½ ì„¤ì •
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 16)
    chunk_size_env = os.getenv("FAST_JSONL_CHUNK_SIZE")
    try:
        chunk_size = int(chunk_size_env) if chunk_size_env else 10000
    except ValueError:
        chunk_size = 10000
    process_chunksize_env = os.getenv("FAST_JSONL_PROCESS_CHUNKSIZE")
    try:
        process_chunksize = int(process_chunksize_env) if process_chunksize_env else 32
    except ValueError:
        process_chunksize = 32
    print(f"  ğŸ”§ ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ, ì²­í¬ í¬ê¸°: {chunk_size}, í”„ë¡œì„¸ìŠ¤ ì²­í¬: {process_chunksize}")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # ë°˜í™˜ ê°ì²´ë¥¼ ì¼ê´€ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”
    def _normalize_to_records(item) -> list[dict]:
        records: list[dict] = []
        if item is None:
            return records
        # dict ë‹¨ê±´
        if isinstance(item, dict):
            return [item]
        # ë¦¬ìŠ¤íŠ¸/íŠœí”Œ ë¬¶ìŒ
        if isinstance(item, (list, tuple)) and item:
            # ìš”ì†Œê°€ dict/tuple/list
            first = item[0]
            if isinstance(first, dict):
                # ì´ë¯¸ dict ë¦¬ìŠ¤íŠ¸
                for elem in item:
                    if isinstance(elem, dict):
                        records.append(elem)
                return records
            # (path, words)ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            for elem in item:
                try:
                    pth, wrds = elem
                    records.append(_build_ocr_jsonl_record(pth, wrds))
                except Exception:
                    continue
            return records
        # ë‹¨ì¼ (path, words)
        try:
            pth, wrds = item
            return [_build_ocr_jsonl_record(pth, wrds)]
        except Exception:
            return []

    total_written = 0
    start_time = time.time()

    with open(output_path, "w", encoding="utf-8") as fout:
        # ì²­í¬ ìŠ¤íŠ¸ë¦¬ë°: ê° ì²­í¬ì—ì„œ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì†Œë¹„í•˜ë©° ê¸°ë¡ (ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
        for chunk_start in tqdm(range(0, len(process_args), chunk_size), desc=f"{split_name} ì²­í¬ ì²˜ë¦¬"):
            chunk_end = min(chunk_start + chunk_size, len(process_args))
            chunk_args = process_args[chunk_start:chunk_end]

            used_thread_fallback = False
            try:
                # í”„ë¡œì„¸ìŠ¤ í’€ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ imap_unorderedë¡œ ìŠ¤íŠ¸ë¦¬ë° ì†Œë¹„
                ctx = mp.get_context("spawn")
                with ctx.Pool(processes=max_workers) as pool:
                    iterator = pool.imap_unordered(to_json_func, chunk_args, chunksize=process_chunksize)
                    for item in tqdm(iterator, total=len(chunk_args), desc=f"{split_name} ë³€í™˜"):
                        if max_total_samples is not None and total_written >= max_total_samples:
                            break
                        for record in _normalize_to_records(item):
                            if max_total_samples is not None and total_written >= max_total_samples:
                                break
                            line = orjson.dumps(record).decode("utf-8")
                            fout.write(line + "\n")
                            total_written += 1
                    pool.close()
                    pool.join()
            except Exception:
                # í´ë°±: ìŠ¤ë ˆë“œ í’€ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì†Œë¹„
                used_thread_fallback = True
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(to_json_func, arg) for arg in chunk_args]
                    for fut in tqdm(as_completed(futures), total=len(futures), desc=f"{split_name} ë³€í™˜(thread)"):
                        if max_total_samples is not None and total_written >= max_total_samples:
                            break
                        item = fut.result()
                        for record in _normalize_to_records(item):
                            if max_total_samples is not None and total_written >= max_total_samples:
                                break
                            line = orjson.dumps(record).decode("utf-8")
                            fout.write(line + "\n")
                            total_written += 1

            # íŒŒì¼ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
            fout.flush()
            try:
                os.fsync(fout.fileno())
            except Exception:
                pass
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° glibc trim (ë¦¬ëˆ…ìŠ¤)
            collected = gc.collect()
            try:
                if os.name == "posix" and os.getenv("FAST_JSONL_TRIM", "1") == "1":
                    import ctypes, ctypes.util  # ì§€ì—° import
                    libc = ctypes.CDLL(ctypes.util.find_library("c"))
                    libc.malloc_trim(0)
            except Exception:
                pass
            # RSS ì¶œë ¥(ë””ë²„ê·¸)
            try:
                process = psutil.Process()
                rss_mb = process.memory_info().rss / 1024 / 1024
                print(f"  ğŸ—‘ï¸ ì²­í¬ {(chunk_start // chunk_size) + 1} ì™„ë£Œ: ëˆ„ì  {total_written}ê°œ, GC {collected}ê°œ í•´ì œ, RSS {rss_mb:.1f}MB, {'thread_fallback' if used_thread_fallback else 'proc'}")
            except Exception:
                print(f"  ğŸ—‘ï¸ ì²­í¬ {(chunk_start // chunk_size) + 1} ì™„ë£Œ: ëˆ„ì  {total_written}ê°œ, GC {collected}ê°œ í•´ì œ")

            if max_total_samples is not None and total_written >= max_total_samples:
                break

    total_time = time.time() - start_time
    speed = total_written / total_time if total_time > 0 else 0
    print(f"âœ… {split_name} JSONL ìƒì„± ì™„ë£Œ: {total_written}ê°œ ìƒ˜í”Œ")
    print(f"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {speed:.1f} samples/sec")


# ----------------------------------------------------------------------------
# per-dataset JSONL line ìƒì„±ê¸° (ì´ë¯¸ì§€ ê²½ë¡œ + ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸)
# ----------------------------------------------------------------------------

def text_in_wild_to_jsonl(args):
    """(img_id, img_info, annotations, base_path, lookup_dict, crop_dir) â†’ [records]"""
    img_id, img_info, annotations, base_path, lookup_dict, crop_dir = args
    try:
        img_file_name = img_info.get("file_name")
        if not img_file_name.endswith(".jpg"):
            img_file_name = f"{img_file_name}.jpg"

        # ê²½ë¡œ ì°¾ê¸° (lookup â†’ fallback ê²½ë¡œ ê·œì¹™)
        img_path = None
        if lookup_dict and isinstance(lookup_dict, dict):
            img_path = lookup_dict.get(img_file_name)
            if not img_path:
                for ext in [".png", ".jpeg"]:
                    alt = img_file_name.replace(".jpg", ext)
                    if alt in lookup_dict:
                        img_path = lookup_dict[alt]
                        break
        if not img_path:
            img_type = img_info.get("type", "book")
            if img_type == "book":
                image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
            elif img_type == "sign":
                image_dir = f"{base_path}/01_textinthewild_signboard_images_new/01_textinthewild_signboard_images_new/Signboard"
            elif img_type == "traffic sign":
                image_dir = f"{base_path}/01_textinthewild_traffic_sign_images_new/01_textinthewild_traffic_sign_images_new/Traffic_Sign"
            elif img_type == "product":
                image_dir = f"{base_path}/01_textinthewild_goods_images_new/01_textinthewild_goods_images_new/Goods"
            else:
                image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
            img_path = os.path.join(image_dir, img_file_name)
        if not img_path or not os.path.exists(img_path):
            return None

        # bboxes (x,y,w,h) â†’ 8ì¢Œí‘œ, words
        bboxes = []
        words = []
        for ann in annotations:
            x, y, w_box, h_box = ann['bbox']
            x1, y1, x2, y2 = x, y, x + w_box, y + h_box
            pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
            bboxes.append(pixel_coords)
            words.append(ann.get('text', ''))

        return _save_crops_and_make_records(img_path, bboxes, words, crop_dir, prefix="tiw", mirror_root=base_path)
    except Exception:
        return None


def public_admin_to_jsonl(args):
    """(img_info, annotations, base_path, lookup_dict, dataset_lookup_name, image_path_cache, crop_dir)
       â†’ [records]
    """
    img_info, annotations, base_path, lookup_dict, dataset_lookup_name, image_path_cache, crop_dir = args
    try:
        img_file_name = img_info.get("image.file.name", "")
        if not img_file_name:
            return None
        img_path = optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
        if not img_path or not os.path.exists(img_path):
            return None
        bboxes = []
        words = []
        for ann in annotations:
            x, y, w_box, h_box = ann['annotation.bbox']
            x1, y1, x2, y2 = x, y, x + w_box, y + h_box
            pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
            bboxes.append(pixel_coords)
            words.append(ann.get('annotation.text', ''))
        return _save_crops_and_make_records(img_path, bboxes, words, crop_dir, prefix="pa", mirror_root=base_path)
    except Exception:
        return None


def ocr_public_to_jsonl(args):
    """(img_info, annotations, base_path, dataset_lookup_name, image_path_cache, crop_dir) â†’ [records]"""
    img_info, annotations, base_path, dataset_lookup_name, image_path_cache, crop_dir = args
    try:
        img_file_name = img_info.get("file_name", "")
        if not img_file_name.endswith((".jpg", ".png", ".jpeg")):
            img_file_name = f"{img_file_name}.jpg"
        img_path = optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
        if not img_path or not os.path.exists(img_path):
            return None
        bboxes = []
        words = []
        for ann in annotations:
            bbox_coords = ann.get('bbox', [])
            try:
                if len(bbox_coords) == 8:
                    x_coords = bbox_coords[:4]
                    y_coords = bbox_coords[4:]
                    pixel_coords = []
                    for i in range(4):
                        pixel_coords.extend([x_coords[i], y_coords[i]])
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                else:
                    x_coords = [bbox_coords[0], bbox_coords[2], bbox_coords[4], bbox_coords[6]]
                    y_coords = [bbox_coords[1], bbox_coords[3], bbox_coords[5], bbox_coords[7]]
                    pixel_coords = [
                        x_coords[0], y_coords[0],
                        x_coords[1], y_coords[1],
                        x_coords[2], y_coords[2],
                        x_coords[3], y_coords[3],
                    ]
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                bboxes.append(pixel_coords)
                words.append(ann.get('text', ''))
            except (IndexError, TypeError):
                try:
                    x, y, w_box, h_box = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                    x1, y1, x2, y2 = x, y, x + w_box, y + h_box
                    pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    bboxes.append(pixel_coords)
                    words.append(ann.get('text', ''))
                except Exception:
                    continue
        return _save_crops_and_make_records(img_path, bboxes, words, crop_dir, prefix="ocrp", mirror_root=base_path)
    except Exception:
        return None


def finance_logistics_to_jsonl(args):
    """(sub_dataset, img_info_data, annotations_for_dataset, crop_dir) â†’ [records]"""
    sub_dataset, img_info_data, annotations_for_dataset, crop_dir = args
    try:
        img_path = img_info_data.get("file_path")
        if not img_path or not os.path.exists(img_path):
            return None
        bboxes = []
        words = []
        for ann in (annotations_for_dataset or []):
            bbox_coords = ann.get('bbox', [])
            try:
                if isinstance(bbox_coords, list) and len(bbox_coords) >= 8:
                    x_coords = bbox_coords[:4]
                    y_coords = bbox_coords[4:]
                    pixel_coords = []
                    for i in range(4):
                        pixel_coords.extend([x_coords[i], y_coords[i]])
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                elif isinstance(bbox_coords, list) and len(bbox_coords) >= 4:
                    x, y, w_box, h_box = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                    x1, y1, x2, y2 = x, y, x + w_box, y + h_box
                    pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                else:
                    continue
                bboxes.append(pixel_coords)
                words.append(ann.get('text', ''))
            except Exception:
                continue
        return _save_crops_and_make_records(img_path, bboxes, words, crop_dir, prefix="fin")
    except Exception:
        return None


def handwriting_to_jsonl(args):
    """(img_file_name, img_info_data, annotations_for_image?, crop_dir) â†’ [records]"""
    if len(args) == 3:
        img_file_name, img_info_data, annotations_for_image = args
        crop_dir = None
    else:
        img_file_name, img_info_data, annotations_for_image, crop_dir = args
    try:
        img_path = img_info_data.get("file_path")
        if not img_path or not os.path.exists(img_path):
            return None
        bboxes = []
        words = []
        if annotations_for_image:
            for ann in annotations_for_image:
                bbox_coords = ann.get('bbox', [])
                try:
                    if isinstance(bbox_coords, list) and len(bbox_coords) >= 8:
                        x_coords = bbox_coords[:4]
                        y_coords = bbox_coords[4:]
                        pixel_coords = []
                        for i in range(4):
                            pixel_coords.extend([x_coords[i], y_coords[i]])
                        pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    elif isinstance(bbox_coords, list) and len(bbox_coords) >= 4:
                        x, y, w_box, h_box = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                        x1, y1, x2, y2 = x, y, x + w_box, y + h_box
                        pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                        pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    else:
                        continue
                    bboxes.append(pixel_coords)
                    words.append(ann.get('text', ''))
                except Exception:
                    continue
        return _save_crops_and_make_records(img_path, bboxes, words, crop_dir, prefix="hw")
    except Exception:
        return None


# ----------------------------------------------------------------------------
# per-dataset JSONL ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í•¨ìˆ˜
# ----------------------------------------------------------------------------

def create_jsonl_text_in_wild_split(base_path, json_path, train_output_path, valid_output_path, train_ratio=0.9, max_samples=None, random_seed=42):
    print(f"ğŸ§ª Text in the wild JSONL ìƒì„± ì¤‘... (train/valid {train_ratio}:{1-train_ratio} ë¶„í• )")

    random.seed(random_seed)
    os.makedirs(os.path.dirname(train_output_path), exist_ok=True)
    os.makedirs(os.path.dirname(valid_output_path), exist_ok=True)

    with open(json_path, 'rb') as f:
        data = orjson.loads(f.read())

    images_info = {img['id']: img for img in data.get('images', [])}
    image_annotations = {}
    for ann in data.get('annotations', []):
        img_id = ann.get('image_id')
        image_annotations.setdefault(img_id, []).append(ann)

    del data
    gc.collect()

    img_ids = list(images_info.keys())
    if max_samples and len(img_ids) > max_samples:
        img_ids = img_ids[:max_samples]
    random.shuffle(img_ids)
    train_size = int(len(img_ids) * train_ratio)
    train_img_ids = img_ids[:train_size]
    valid_img_ids = img_ids[train_size:]

    dataset_lookup_name = "text_in_wild"
    lookup_dict = load_optimized_lookup(dataset_lookup_name)
    crop_dir_train = os.path.join(LOCAL_OUTPUT_PATH, "crops", "text_in_wild", "train")
    crop_dir_valid = os.path.join(LOCAL_OUTPUT_PATH, "crops", "text_in_wild", "valid")

    # train
    train_args = []
    for img_id in train_img_ids:
        img_info = images_info.get(img_id)
        anns = image_annotations.get(img_id, [])
        train_args.append((img_id, img_info, anns, base_path, lookup_dict, crop_dir_train))
    create_parallel_jsonl_from_args(train_args, train_output_path, "TextInWild-Train", text_in_wild_to_jsonl, max_total_samples=max_samples)

    # valid
    valid_args = []
    for img_id in valid_img_ids:
        img_info = images_info.get(img_id)
        anns = image_annotations.get(img_id, [])
        valid_args.append((img_id, img_info, anns, base_path, lookup_dict, crop_dir_valid))
    create_parallel_jsonl_from_args(valid_args, valid_output_path, "TextInWild-Valid", text_in_wild_to_jsonl, max_total_samples=max_samples)


def create_jsonl_public_admin_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    print(f"ğŸ§ª {dataset_name} JSONL ìƒì„± ì¤‘...")
    data, file_handle = load_json_with_orjson(json_path)
    try:
        images = data.get('images', [])
        total_images = len(images) if isinstance(images, list) else 0
        if max_samples and total_images > max_samples:
            indices = list(range(total_images))
            random.seed(42)
            random.shuffle(indices)
            indices = indices[:max_samples]
        else:
            indices = list(range(total_images))

        image_annotations = {}
        annotations = data.get('annotations', [])
        i = 0
        while True:
            try:
                ann = annotations[i]
                img_id = ann.get('image_id', ann.get('id'))
                image_annotations.setdefault(img_id, []).append(ann)
                i += 1
            except IndexError:
                break

        del data
        del annotations
        gc.collect()

        if 'train_partly' in dataset_name.lower() or ('train' in dataset_name.lower() and 'partly' in dataset_name.lower()):
            dataset_lookup_name = "public_admin_train_partly"
        elif 'train' in dataset_name.lower() and 'partly' not in dataset_name.lower():
            dataset_lookup_name = "public_admin_train"
        else:
            dataset_lookup_name = "public_admin_valid"

        lookup_func = load_optimized_lookup(dataset_lookup_name)
        image_path_cache = {}
        if not lookup_func:
            for train_num in [1, 2, 3]:
                image_dir = f"{base_path}/Training/[ì›ì²œ]train{train_num}/02.ì›ì²œë°ì´í„°(jpg)"
                if os.path.exists(image_dir):
                    scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=(".jpg",))
                    image_path_cache.update(scanned_files)
            image_dir = f"{base_path}/Validation/[ì›ì²œ]validation/02.ì›ì²œë°ì´í„°(Jpg)"
            if os.path.exists(image_dir):
                scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=(".jpg",))
                image_path_cache.update(scanned_files)

        process_args = []
        crop_dir = os.path.join(LOCAL_OUTPUT_PATH, "crops", "public_admin", "train" if 'train' in dataset_name else ("train_partly" if 'partly' in dataset_name.lower() else "valid"))
        for i in indices:
            img_info = images[i]
            img_id = img_info.get('id', i)
            anns = image_annotations.get(img_id, [])
            process_args.append((img_info, anns, base_path, lookup_func, dataset_lookup_name, image_path_cache, crop_dir))

        del images
        del image_annotations
        gc.collect()

        create_parallel_jsonl_from_args(process_args, output_path, dataset_name, public_admin_to_jsonl, max_total_samples=max_samples)
    finally:
        safe_close_file(file_handle)


def create_jsonl_ocr_public_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    print(f"ğŸ§ª {dataset_name} JSONL ìƒì„± ì¤‘...")
    data, file_handle = load_json_with_orjson(json_path)
    try:
        images = data.get('images', [])
        if hasattr(images, '__getitem__') and not isinstance(images, list):
            images_list = []
            chunk_size = 10000
            i = 0
            while True:
                try:
                    chunk = []
                    for j in range(chunk_size):
                        try:
                            chunk.append(images[i + j])
                        except IndexError:
                            break
                    images_list.extend(chunk)
                    i += len(chunk)
                    if len(chunk) < chunk_size:
                        break
                    if i % 20000 == 0:
                        gc.collect()
                except IndexError:
                    break
            images = images_list

        if max_samples and len(images) > max_samples:
            random.seed(42)
            random.shuffle(images)
            images = images[:max_samples]

        image_annotations = {}
        annotations = data.get('annotations', [])
        if hasattr(annotations, '__getitem__') and not isinstance(annotations, list):
            chunk_size = 10000
            annotations_list = []
            i = 0
            while True:
                try:
                    chunk = []
                    for j in range(chunk_size):
                        try:
                            chunk.append(annotations[i + j])
                        except IndexError:
                            break
                    annotations_list.extend(chunk)
                    i += len(chunk)
                    if len(chunk) < chunk_size:
                        break
                    if i % 50000 == 0:
                        gc.collect()
                except IndexError:
                    break
            annotations = annotations_list

        for ann in tqdm(annotations, desc="ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”"):
            img_id = ann.get('image_id', ann.get('id'))
            image_annotations.setdefault(img_id, []).append(ann)

        del data
        del annotations

        dataset_lookup_name = "ocr_public_train" if 'train' in dataset_name.lower() else "ocr_public_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        image_path_cache = {}
        if not lookup_func:
            if 'train' in dataset_name.lower():
                image_dir = f"{base_path}/Training/01.ì›ì²œë°ì´í„°"
            else:
                image_dir = f"{base_path}/Validation/01.ì›ì²œë°ì´í„°"
            if os.path.exists(image_dir):
                scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=(".jpg", ".png", ".jpeg"))
                image_path_cache.update(scanned_files)

        process_args = []
        crop_dir = os.path.join(LOCAL_OUTPUT_PATH, "crops", "ocr_public", "train" if 'train' in dataset_name.lower() else "valid")
        for img_info in images:
            img_id = img_info.get('id')
            anns = image_annotations.get(img_id, [])
            process_args.append((img_info, anns, base_path, dataset_lookup_name, image_path_cache, crop_dir))

        del images
        del image_annotations

        create_parallel_jsonl_from_args(process_args, output_path, dataset_name, ocr_public_to_jsonl, max_total_samples=max_samples)
    finally:
        safe_close_file(file_handle)


def create_jsonl_finance_logistics_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    print(f"ğŸ§ª {dataset_name} JSONL ìƒì„± ì¤‘...")
    data, file_handle = load_json_with_orjson(json_path)
    try:
        images = data.get('images', [])
        annotations = data.get('annotations', [])

        dataset_lookup_name = "finance_logistics_train" if 'train' in dataset_name.lower() else "finance_logistics_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        fallback_cache = {}
        if not lookup_func:
            if 'train' in dataset_name.lower():
                scan_dirs = [f"{base_path}/Training/01.ì›ì²œë°ì´í„°"]
            else:
                scan_dirs = [f"{base_path}/Validation/01.ì›ì²œë°ì´í„°"]
            for scan_dir in scan_dirs:
                if os.path.exists(scan_dir):
                    scanned_files = scan_images_recursive_with_scandir(scan_dir, extensions=(".png",))
                    fallback_cache.update(scanned_files)

        image_info = {}
        target_count = max_samples if max_samples else None
        i = 0
        matched_count = 0
        while True:
            try:
                img = images[i]
                sub_dataset = img.get('sub_dataset', '')
                filename = f"{sub_dataset}.png"
                img_path = optimized_find_image_path(filename, base_path, dataset_lookup_name, fallback_cache)
                if img_path:
                    image_info[sub_dataset] = {
                        'file_path': img_path,
                        'width': img.get('width', 1000),
                        'height': img.get('height', 1000),
                        'filename': filename,
                    }
                    matched_count += 1
                i += 1
                if target_count and matched_count >= target_count:
                    break
            except IndexError:
                break

        all_annotations = {}
        for ann in annotations:
            sub_dataset = ann.get('sub_dataset', '')
            if sub_dataset in image_info:
                all_annotations.setdefault(sub_dataset, []).append({
                    'bbox': ann.get('bbox', []),
                    'text': ann.get('text', ''),
                    'sub_dataset': sub_dataset,
                })

        process_args = []
        crop_dir = os.path.join(LOCAL_OUTPUT_PATH, "crops", "finance_logistics", "train" if 'train' in dataset_name.lower() else "valid")
        for sub_dataset, info in image_info.items():
            anns = all_annotations.get(sub_dataset, [])
            process_args.append((sub_dataset, info, anns, crop_dir))

        del images
        del annotations
        del image_info
        del all_annotations

        create_parallel_jsonl_from_args(process_args, output_path, dataset_name, finance_logistics_to_jsonl, max_total_samples=max_samples)
    finally:
        safe_close_file(file_handle)


def create_jsonl_handwriting_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    print(f"ğŸ§ª {dataset_name} JSONL ìƒì„± ì¤‘...")
    with open(json_path, 'rb') as f:
        data = orjson.loads(f.read())
    try:
        images = data.get('images', [])
        if max_samples and len(images) > max_samples:
            random.seed(42)
            random.shuffle(images)
            images = images[:max_samples]

        dataset_lookup_name = "handwriting_train" if 'train' in dataset_name.lower() else "handwriting_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        fallback_cache = {}
        if not lookup_func:
            if 'train' in dataset_name.lower():
                scan_dirs = [f"{base_path}/1.Training/ì›ì²œë°ì´í„°"]
            else:
                scan_dirs = [f"{base_path}/2.Validation/ì›ì²œë°ì´í„°"]
            for scan_dir in scan_dirs:
                if os.path.exists(scan_dir):
                    scanned_files = scan_images_recursive_with_scandir(scan_dir, extensions=(".png",))
                    fallback_cache.update(scanned_files)

        filename_to_info = {}
        for img in images:
            img_file_name = img.get('file_name', '')
            if img_file_name and not img_file_name.endswith('.png'):
                filename = f"{img_file_name}.png"
            else:
                filename = img_file_name
            img_path = optimized_find_image_path(filename, base_path, dataset_lookup_name, fallback_cache)
            if img_path:
                filename_to_info[img_file_name] = {
                    'file_path': img_path,
                    'width': img.get('width', 1000),
                    'height': img.get('height', 1000),
                    'filename': filename,
                    'original_json_path': img.get('original_json_path', ''),
                }

        image_id_to_filename = {img.get('id'): img.get('file_name', '') for img in images}
        annotations = data.get('annotations', [])
        image_annotations = {}
        for ann in annotations:
            img_id = ann.get('image_id')
            key_fname = image_id_to_filename.get(img_id)
            if not key_fname:
                continue
            image_annotations.setdefault(key_fname, []).append(ann)

        process_args = []
        crop_dir = os.path.join(LOCAL_OUTPUT_PATH, "crops", "handwriting", "train" if 'train' in dataset_name.lower() else "valid")
        for img_file_name, info in filename_to_info.items():
            anns = image_annotations.get(img_file_name, [])
            process_args.append((img_file_name, info, anns, crop_dir))

        del images
        del annotations
        del filename_to_info
        del image_annotations

        create_parallel_jsonl_from_args(process_args, output_path, dataset_name, handwriting_to_jsonl, max_total_samples=max_samples)
    except Exception as e:
        print(f"âŒ ì†ê¸€ì”¨ JSONL ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def create_lmdb_text_in_wild_from_ids(base_path, images_info, image_annotations, img_ids, output_path, split_name):
    """Text in the wild ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° LMDB ìƒì„± (thread_map ë³‘ë ¬ì²˜ë¦¬ ë²„ì „)"""
    print(f"ğŸš€ {split_name} ë³‘ë ¬ LMDB ìƒì„± ì¤‘... ({len(img_ids)}ê°œ ìƒ˜í”Œ)")
    
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìµœì  ì›Œì»¤ ìˆ˜
    max_workers = min(mp.cpu_count(), 16)  # ì›Œì»¤ ìˆ˜ë¥¼ 16ê°œë¡œ ì¦ê°€
    print(f"  ğŸ”§ ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ")
    
    # ğŸš€ lookup ë”•ì…”ë„ˆë¦¬ ì‚¬ì „ ë¡œë“œ
    dataset_lookup_name = "text_in_wild"
    lookup_dict = load_optimized_lookup(dataset_lookup_name)
    
    # ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„
    process_args = []
    for img_id in img_ids:
        if img_id not in images_info:
            continue
        img_info = images_info[img_id]
        annotations = image_annotations.get(img_id, [])
        process_args.append((img_id, img_info, annotations, base_path, lookup_dict))
    
    print(f"  ğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°: {len(process_args)}ê°œ")
    
    # JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ (ê°€ì¥ í° ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶€ë¶„)
    del images_info
    del image_annotations
    del img_ids
    gc.collect()
    print(f"  ğŸ—‘ï¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ + ì¦‰ì‹œ LMDB ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
    start_time = time.time()
    
    # LMDB í™˜ê²½ ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    env = lmdb.open(output_path, 
                    map_size=1099511627776,  # 1TB
                    writemap=True,  # ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”
                    meminit=False,  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë¹„í™œì„±í™”
                    map_async=True)  # ë¹„ë™ê¸° ë§µí•‘
    
    print(f"  ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ + ì¦‰ì‹œ ì €ì¥ ì‹œì‘...")
    
    idx = 0
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    chunk_size = 10000  # 10000ê°œì”© ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # process_argsë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ìˆœíšŒ
        for chunk_start in tqdm(range(0, len(process_args), chunk_size), desc=f"{split_name} ì²­í¬ ì²˜ë¦¬"):
            chunk_end = min(chunk_start + chunk_size, len(process_args))
            chunk_args = process_args[chunk_start:chunk_end]
            
            # í˜„ì¬ ì²­í¬ì˜ futureë§Œ ìƒì„±
            futures = {executor.submit(process_single_text_wild_image, arg) for arg in chunk_args}
            
            # ë” ì‘ì€ íŠ¸ëœì­ì…˜ ë‹¨ìœ„ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
            txn_batch_size = 500  # 500ê°œì”© íŠ¸ëœì­ì…˜ ë¶„í•  (ë” ì‘ê²Œ)
            batch_count = 0
            txn = None
            
            # í˜„ì¬ ì²­í¬ì˜ ì‘ì—…ë§Œ ì²˜ë¦¬
            for future in as_completed(futures):
                result = future.result()
                
                if result is not None:
                    img_id, img_data, gt_info = result
                    
                    # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘ (ë°°ì¹˜ ë‹¨ìœ„)
                    if batch_count % txn_batch_size == 0:
                        if txn is not None:
                            txn.commit()  # ì´ì „ íŠ¸ëœì­ì…˜ ì»¤ë°‹
                        txn = env.begin(write=True)  # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘
                    
                    # ì¸ì‹ìš©: ë‹¨ì–´ ë‹¨ìœ„ í¬ë¡­ì„ LMDBì— ì €ì¥ (ì›ë³¸/GT ì €ì¥ ì•ˆ í•¨)
                    for w_idx, (crop_jpg, label) in enumerate(_iter_recog_crops_bytes(img_data, gt_info)):
                        label_bytes = label.encode('utf-8', errors='ignore')
                        if not label_bytes:
                            continue
                        img_key = f'image-{idx:09d}'.encode()
                        lab_key = f'label-{idx:09d}'.encode()
                        txn.put(img_key, crop_jpg)
                        txn.put(lab_key, label_bytes)
                        idx += 1
                        batch_count += 1

                    # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
                    del result
                    del img_data
                    del gt_info
            
            # ë§ˆì§€ë§‰ íŠ¸ëœì­ì…˜ ì»¤ë°‹
            if txn is not None:
                txn.commit()
            del chunk_args, futures
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            print(f"  ğŸ—‘ï¸ ì²­í¬ {chunk_start//chunk_size + 1} ì™„ë£Œ: {idx}ê°œ ì²˜ë¦¬, GC {collected}ê°œ í•´ì œ")
        
        # ë§ˆì§€ë§‰ ì»¤ë°‹ (ìƒˆ íŠ¸ëœì­ì…˜ìœ¼ë¡œ)
        txn = env.begin(write=True)
        txn.put('num-samples'.encode(), str(idx).encode())
        txn.commit()
    
    env.close()
    
    # ìµœì¢… ë©”ëª¨ë¦¬ í•´ì œ
    del process_args
    del lookup_dict
    gc.collect()
    
    total_time = time.time() - start_time
    speed = idx / total_time if total_time > 0 else 0
    print(f"âœ… {split_name} ë³‘ë ¬ LMDB ìƒì„± ì™„ë£Œ: {idx}ê°œ ìƒ˜í”Œ")
    print(f"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {speed:.1f} samples/sec")
    print(f"ğŸ—‘ï¸ {split_name} ëª¨ë“  ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

# ============================================================================
# ê³µê³µí–‰ì •ë¬¸ì„œ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_public_admin_train_valid(max_samples=500):
    """ê³µê³µí–‰ì •ë¬¸ì„œ OCR train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª ê³µê³µí–‰ì •ë¬¸ì„œ OCR train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR"
    train_json_path = f"{MERGED_JSON_PATH}/public_admin_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/public_admin_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_valid.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_public_admin_from_json(base_path, train_json_path, train_output_path, "ê³µê³µí–‰ì •ë¬¸ì„œ Train", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_public_admin_from_json(base_path, valid_json_path, valid_output_path, "ê³µê³µí–‰ì •ë¬¸ì„œ Valid", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_public_admin_train_partly(max_samples=500):
    """ê³µê³µí–‰ì •ë¬¸ì„œ OCR train_partly LMDB ìƒì„± (í•™ìŠµ ë°ì´í„°ì…‹)"""
    print("=" * 60)
    print("ğŸ§ª ê³µê³µí–‰ì •ë¬¸ì„œ OCR train_partly LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR"
    train_json_path = f"{MERGED_JSON_PATH}/public_admin_train_partly_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train_partly.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_public_admin_from_json(base_path, train_json_path, train_output_path, "ê³µê³µí–‰ì •ë¬¸ì„œ Train Partly", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")

def create_lmdb_public_admin_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    """ê³µê³µí–‰ì •ë¬¸ì„œ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„±"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    
    # JSON íŒŒì¼ ë¡œë“œ (orjson ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # imagesì™€ annotations ì²˜ë¦¬ (orjsonìœ¼ë¡œ ë¡œë“œëœ Python ë¦¬ìŠ¤íŠ¸)
        images = data.get('images', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: orjson Python ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œì„ ìœ„í•´ ì¸ë±ìŠ¤ ê¸°ë°˜ ì²˜ë¦¬
        total_images = 0
        for _ in images:
            total_images += 1
        
        if max_samples and total_images > max_samples:
            print(f"ğŸ“Š {max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì´ {total_images}ê°œ ì¤‘)")
            # ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„± í›„ ì„ê¸°
            indices = list(range(total_images))
            random.seed(42)
            random.shuffle(indices)
            indices = indices[:max_samples]
        else:
            indices = list(range(total_images))
        
        # ì´ë¯¸ì§€ë³„ë¡œ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”
        image_annotations = {}
        annotations = data.get('annotations', [])
        
        # len() í˜¸ì¶œ ì—†ì´ ì•ˆì „í•œ ë°˜ë³µ ì²˜ë¦¬
        i = 0
        while True:
            try:
                ann = annotations[i]
                
                if i % 10000 == 0:  # 1ë§Œê°œë§ˆë‹¤ ì§„í–‰ìƒí™©
                    print(f"    ğŸ“Š ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬: {i+1}ê°œ")
                
                img_id = ann.get('image_id', ann.get('id'))
                if img_id not in image_annotations:
                    image_annotations[img_id] = []
                image_annotations[img_id].append(ann)
                
                i += 1
            except IndexError:
                break
        
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì™„ë£Œ: {len(image_annotations)}ê°œ ì´ë¯¸ì§€")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ JSON ë°ì´í„° í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        del data
        del annotations
        gc.collect()
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        print("  ğŸ”„ ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì¤‘...")
        # dataset_nameì— ë”°ë¼ ì •í™•í•œ lookup ì´ë¦„ ê²°ì •
        if 'train_partly' in dataset_name.lower() or ('train' in dataset_name.lower() and 'partly' in dataset_name.lower()):
            dataset_lookup_name = "public_admin_train_partly"
        elif 'train' in dataset_name.lower() and 'partly' not in dataset_name.lower():
            dataset_lookup_name = "public_admin_train"
        else:
            dataset_lookup_name = "public_admin_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìºì‹œ (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        image_path_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìºì‹œ ìƒì„± ì¤‘...")
            # Training í´ë”ë“¤ ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            for train_num in [1, 2, 3]:
                image_dir = f"{base_path}/Training/[ì›ì²œ]train{train_num}/02.ì›ì²œë°ì´í„°(jpg)"
                if os.path.exists(image_dir):
                    scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=('.jpg',))
                    image_path_cache.update(scanned_files)
            
            # Validation í´ë” ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            image_dir = f"{base_path}/Validation/[ì›ì²œ]validation/02.ì›ì²œë°ì´í„°(Jpg)"
            if os.path.exists(image_dir):
                scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=('.jpg',))
                image_path_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(image_path_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„
        process_args = []
        for i, img_idx in enumerate(indices):
            img_info = images[img_idx]  # orjson Python ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì ‘ê·¼
            img_id = img_info.get('id', i)
            annotations = image_annotations.get(img_id, [])
            process_args.append((img_info, annotations, base_path, lookup_func, dataset_lookup_name, image_path_cache))
        
        print(f"  ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(process_args)}ê°œ")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
        del images
        del image_annotations
        del indices
        gc.collect()
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ LMDB ìƒì„±
        create_parallel_lmdb_from_args(process_args, output_path, dataset_name, process_single_public_admin_image)
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

# ============================================================================
# OCR ê³µê³µ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_ocr_public_train_valid(max_samples=500):
    """023.OCR ë°ì´í„°(ê³µê³µ) train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª 023.OCR ë°ì´í„°(ê³µê³µ) train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°"
    train_json_path = f"{MERGED_JSON_PATH}/ocr_public_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/ocr_public_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_train.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_valid.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_ocr_public_from_json(base_path, train_json_path, train_output_path, "OCR ê³µê³µ Train", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_ocr_public_from_json(base_path, valid_json_path, valid_output_path, "OCR ê³µê³µ Valid", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_lmdb_ocr_public_from_json(base_path, json_path, output_path, dataset_name, max_samples=None, use_groups=False):
    """OCR ê³µê³µ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„±"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    
    if use_groups:
        # ê·¸ë£¹ë³„ ì²˜ë¦¬
        def process_group(group_data, original_path):
            # ê·¸ë£¹ë³„ ì²˜ë¦¬ ë¡œì§
            print(f"  ğŸ“ ê·¸ë£¹ ë°ì´í„° ì²˜ë¦¬: {len(group_data['images'])}ê°œ ì´ë¯¸ì§€")
            return len(group_data['images'])
        
        total_processed = process_json_by_groups(json_path, process_group, max_samples)
        print(f"âœ… ê·¸ë£¹ë³„ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_processed}ê°œ ì²˜ë¦¬ë¨")
        return
    
    # ê¸°ì¡´ ë°©ì‹ (ì „ì²´ JSON ë¡œë“œ)
    # JSON íŒŒì¼ ë¡œë“œ (orjson ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # imagesì™€ annotations ì²˜ë¦¬ (ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        images = data.get('images', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        
        # bigjson Arrayë¥¼ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ)
        if hasattr(images, '__getitem__') and not isinstance(images, list):
            print("  ğŸ”„ bigjson Arrayë¥¼ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘... (ë©”ëª¨ë¦¬ ì ˆì•½)")
            images_list = []
            chunk_size = 10000  # ì²­í¬ë¡œ ë³€í™˜
            i = 0
            while True:
                try:
                    chunk = []
                    for j in range(chunk_size):
                        try:
                            chunk.append(images[i + j])
                        except IndexError:
                            break
                    images_list.extend(chunk)
                    i += len(chunk)
                    if len(chunk) < chunk_size:
                        break
                    print(f"    ğŸ“Š ë³€í™˜ ì§„í–‰: {i}ê°œ")
                    # ì²­í¬ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                    if i % 20000 == 0:
                        gc.collect()
                except IndexError:
                    break
            images = images_list
            print(f"  âœ… ë³€í™˜ ì™„ë£Œ: {len(images)}ê°œ ì´ë¯¸ì§€")
        
        if max_samples and len(images) > max_samples:
            print(f"ğŸ“Š {max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì´ {len(images)}ê°œ ì¤‘)")
            random.seed(42)
            random.shuffle(images)
            images = images[:max_samples]
        
        # ì´ë¯¸ì§€ë³„ë¡œ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” (ë³‘ë ¬ ì²˜ë¦¬)
        image_annotations = {}
        annotations = data.get('annotations', [])
        
        # bigjson Arrayë¥¼ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½ ë°©ì‹)
        if hasattr(annotations, '__getitem__') and not isinstance(annotations, list):
            print("  ğŸ”„ ì–´ë…¸í…Œì´ì…˜ bigjson Arrayë¥¼ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘... (ë©”ëª¨ë¦¬ ì ˆì•½)")
            # ì‘ì€ ì²­í¬ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            chunk_size = 10000  # ì²­í¬
            annotations_list = []
            i = 0
            while True:
                try:
                    chunk = []
                    for j in range(chunk_size):
                        try:
                            chunk.append(annotations[i + j])
                        except IndexError:
                            break
                    annotations_list.extend(chunk)
                    i += len(chunk)
                    if len(chunk) < chunk_size:
                        break
                    print(f"    ğŸ“Š ì–´ë…¸í…Œì´ì…˜ ë³€í™˜ ì§„í–‰: {i}ê°œ")
                    # ì²­í¬ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                    if i % 50000 == 0:
                        gc.collect()
                except IndexError:
                    break
            annotations = annotations_list
            print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ë³€í™˜ ì™„ë£Œ: {len(annotations)}ê°œ")
        
        # ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” (bigjsonì€ ìŠ¤ë ˆë“œ ì•ˆì „í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆœì°¨ ì²˜ë¦¬)
        print("  ğŸ”„ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì¤‘...")
        for ann in tqdm(annotations, desc="ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”"):
            img_id = ann.get('image_id', ann.get('id'))
            if img_id not in image_annotations:
                image_annotations[img_id] = []
            image_annotations[img_id].append(ann)
        
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì™„ë£Œ: {len(image_annotations)}ê°œ ì´ë¯¸ì§€")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ JSON ë°ì´í„° í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        del data
        del annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        print("  ğŸ”„ ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì¤‘...")
        dataset_lookup_name = "ocr_public_train" if 'train' in dataset_name.lower() else "ocr_public_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìºì‹œ (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        image_path_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ ê²½ë¡œ ìºì‹œ êµ¬ì¶• ì¤‘...")
            # Training/Validation êµ¬ë¶„
            if 'train' in dataset_name.lower():
                image_dir = f"{base_path}/Training/01.ì›ì²œë°ì´í„°"
            else:
                image_dir = f"{base_path}/Validation/01.ì›ì²œë°ì´í„°"
            
            # ì‹¤ì œ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            if os.path.exists(image_dir):
                scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=('.jpg', '.png', '.jpeg'))
                image_path_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(image_path_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„
        process_args = []
        for img_info in images:
            img_id = img_info.get('id')
            annotations = image_annotations.get(img_id, [])
            process_args.append((img_info, annotations, base_path, dataset_lookup_name, image_path_cache))
        
        print(f"  ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(process_args)}ê°œ")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
        del images
        del image_annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ LMDB ìƒì„±
        create_parallel_lmdb_from_args(process_args, output_path, dataset_name, process_single_ocr_public_image)
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

# ============================================================================
# ê¸ˆìœµë¬¼ë¥˜ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_finance_logistics_train_valid(max_samples=None):
    """025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜) train/valid LMDB ìƒì„± (ì „ì²´ ë°ì´í„°)"""
    print("=" * 60)
    print("ğŸ§ª 025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜) train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°"
    train_json_path = f"{MERGED_JSON_PATH}/finance_logistics_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/finance_logistics_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_train.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_valid.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_finance_logistics_from_json(base_path, train_json_path, train_output_path, "ê¸ˆìœµë¬¼ë¥˜ Train", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_finance_logistics_from_json(base_path, valid_json_path, valid_output_path, "ê¸ˆìœµë¬¼ë¥˜ Valid", max_samples)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_lmdb_finance_logistics_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    """ê¸ˆìœµë¬¼ë¥˜ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„± (ì´ˆê³ ì† ë²„ì „: orjson ì§ì ‘ ì‚¬ìš©)"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    
    # JSON íŒŒì¼ ë¡œë“œ (orjson ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # ğŸš€ ìµœì í™” 1: bigjson Array ì§ì ‘ ì‚¬ìš© (ë³€í™˜ ì—†ìŒ)
        images = data.get('images', [])
        annotations = data.get('annotations', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ - bigjson Array ì§ì ‘ ì‚¬ìš©")
        
        # ğŸš€ ìµœì í™” 2: ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        print("  ğŸ”„ ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì¤‘...")
        dataset_lookup_name = "finance_logistics_train" if 'train' in dataset_name.lower() else "finance_logistics_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìŠ¤ìº” (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        fallback_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            # Training/Validation êµ¬ë¶„
            if 'train' in dataset_name.lower():
                scan_dirs = [f"{base_path}/Training/01.ì›ì²œë°ì´í„°"]
            else:
                scan_dirs = [f"{base_path}/Validation/01.ì›ì²œë°ì´í„°"]
            
            for scan_dir in scan_dirs:
                if os.path.exists(scan_dir):
                    scanned_files = scan_images_recursive_with_scandir(scan_dir, extensions=('.png',))
                    fallback_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(fallback_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ìµœì í™” 3: bigjson ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (500ê°œë§Œ ì„ íƒ, ë¹ ë¥´ê²Œ)
        print("  ğŸ”„ ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì¤‘...")
        image_info = {}  # sub_dataset â†’ image_info
        
        # ğŸš€ ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ (max_samplesê°€ ìˆìœ¼ë©´ ì œí•œ)
        target_count = max_samples if max_samples else None  # ì „ì²´ ë°ì´í„° ì²˜ë¦¬
        if target_count:
            print(f"  ğŸ“Š ëª©í‘œ ì´ë¯¸ì§€ ìˆ˜: {target_count}ê°œ (ì œí•œ)")
        else:
            print(f"  ğŸ“Š ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)")
        
        i = 0
        matched_count = 0
        while True:
            try:
                img = images[i]
                sub_dataset = img.get('sub_dataset', '')
                filename = f"{sub_dataset}.png"
                
                # ğŸš€ ìµœì í™”ëœ ê²½ë¡œ ì°¾ê¸°
                img_path = optimized_find_image_path(filename, base_path, dataset_lookup_name, fallback_cache)
                if img_path:
                    image_info[sub_dataset] = {
                        'file_path': img_path,
                        'width': img.get('width', 1000),
                        'height': img.get('height', 1000),
                        'filename': filename
                    }
                    matched_count += 1
                
                i += 1
                if i % 10000 == 0:
                    if target_count:
                        print(f"    ğŸ“Š ë§¤í•‘ ì§„í–‰: {i}ê°œ ì²˜ë¦¬, {matched_count}ê°œ ë§¤ì¹­ (ëª©í‘œ: {target_count}ê°œ)")
                    else:
                        print(f"    ğŸ“Š ë§¤í•‘ ì§„í–‰: {i}ê°œ ì²˜ë¦¬, {matched_count}ê°œ ë§¤ì¹­ (ì „ì²´ ì²˜ë¦¬)")
                
                # ëª©í‘œ ë‹¬ì„±ì‹œ ì¡°ê¸° ì¢…ë£Œ ğŸ¯ (target_countê°€ ì„¤ì •ëœ ê²½ìš°ë§Œ)
                if target_count and matched_count >= target_count:
                    print(f"    ğŸ¯ ëª©í‘œ ë‹¬ì„±: {matched_count}ê°œ ì´ë¯¸ì§€ ì„ íƒ ì™„ë£Œ!")
                    break
                    
            except IndexError:
                break
        
        print(f"  âœ… ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì™„ë£Œ: {len(image_info)}ê°œ")
        
        # ğŸš€ ìµœì í™” 4: ë‹¨ìˆœ ìˆœì°¨ ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ (sub_dataset ê¸°ë°˜)
        print("  ğŸ”„ ìˆœì°¨ ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬...")
        
        all_annotations = {}
        total_found = 0
        
        print(f"  ğŸš€ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘ (Iterator ë°©ì‹)")
        
        # ğŸš€ bigjson Arrayë¥¼ Iteratorë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        i = 0
        for ann in annotations:
            try:
                # ğŸš€ None ì²´í¬ë¡œ ë ê°ì§€
                if ann is None:
                    print(f"    ğŸ ì–´ë…¸í…Œì´ì…˜ ë ê°ì§€ (None) - ì´ {total_found:,}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
                    break
                
                # annì´ ë¹ˆ ê°’ì´ê±°ë‚˜ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš° ì²´í¬
                if not ann or not hasattr(ann, 'get'):
                    i += 1
                    continue
                
                sub_dataset = ann.get('sub_dataset', '')
                
                if sub_dataset in image_info:
                    if sub_dataset not in all_annotations:
                        all_annotations[sub_dataset] = []
                    
                    # ğŸš€ bigjson Array bboxë¥¼ ì•ˆì „í•˜ê²Œ Python listë¡œ ë³€í™˜
                    bbox_data = ann.get('bbox', [])
                    safe_bbox = []
                    
                    if bbox_data:
                        try:
                            # bigjson Arrayì¸ ê²½ìš° ì•ˆì „í•˜ê²Œ ë³€í™˜
                            if hasattr(bbox_data, '__getitem__') and not isinstance(bbox_data, list):
                                # ìµœëŒ€ 8ê°œê¹Œì§€ ì‹œë„
                                for j in range(8):
                                    try:
                                        safe_bbox.append(bbox_data[j])
                                    except (IndexError, TypeError):
                                        break
                            else:
                                safe_bbox = bbox_data
                        except Exception:
                            safe_bbox = []
                    
                    all_annotations[sub_dataset].append({
                        'bbox': safe_bbox,
                        'text': ann.get('text', ''),
                        'sub_dataset': sub_dataset
                    })
                    total_found += 1
                
                i += 1
                if i % 100000 == 0:
                    print(f"    ğŸ“Š ì²˜ë¦¬ ì§„í–‰: {i:,}ê°œ, ë°œê²¬: {total_found:,}ê°œ")
                    
            except Exception as e:
                if i % 100000 == 0:
                    print(f"    âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                i += 1
                continue
        
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ ì™„ë£Œ: {len(all_annotations)}ê°œ ì´ë¯¸ì§€, {total_found:,}ê°œ ì–´ë…¸í…Œì´ì…˜")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ JSON ë°ì´í„° í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        del data
        del annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ìµœì í™” 5: ë³‘í–‰ì²˜ë¦¬ LMDB ìƒì„± (ê°„ë‹¨í•œ ThreadPoolExecutor)
        print("  ğŸ”„ ë³‘í–‰ì²˜ë¦¬ LMDB ìƒì„± ì¤‘...")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ (all_annotationsë¥¼ í¬í•¨)
        process_args = []
        for sub_dataset, img_info_data in image_info.items():
            annotations_for_dataset = all_annotations.get(sub_dataset, [])
            process_args.append((sub_dataset, img_info_data, annotations_for_dataset))
        
        print(f"  ğŸš€ ë³‘í–‰ì²˜ë¦¬ ì‹œì‘: {len(process_args)}ê°œ ì´ë¯¸ì§€, 16ê°œ ì›Œì»¤")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
        del image_info
        del all_annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        create_parallel_lmdb_from_args(process_args, output_path, dataset_name, process_single_finance_logistics_image, max_workers=16)
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

# ============================================================================
# ì†ê¸€ì”¨ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_handwriting_train_valid(max_samples=500):
    """053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª 053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°"
    train_json_path = f"{MERGED_JSON_PATH}/handwriting_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/handwriting_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_train.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_valid.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_handwriting_from_json(base_path, train_json_path, train_output_path, "ì†ê¸€ì”¨ Train", None)
        test_fast_model_input(train_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_handwriting_from_json(base_path, valid_json_path, valid_output_path, "ì†ê¸€ì”¨ Valid", None)
        test_fast_model_input(valid_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_lmdb_handwriting_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    """ì†ê¸€ì”¨ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„± (orjson ìµœì í™” ë²„ì „)"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    print(f"ğŸ“‹ bbox í˜•íƒœ: [x1, y1, x2, y1, x2, y2, x3, y3] -> [x1, y1, x2, y1, x2, y2, x3, y3] (8ê°œ ì¢Œí‘œ)")
    
    # ğŸ“„ ì†ê¸€ì”¨ëŠ” orjsonìœ¼ë¡œ ë¹ ë¥´ê²Œ ë¡œë“œ
    print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    with open(json_path, 'rb') as f:
        data = orjson.loads(f.read())
    print("âœ… orjson ë¡œë“œ ì„±ê³µ")
    
    try:
        # ğŸš€ ìµœì í™” 1: orjsonìœ¼ë¡œ ë¡œë“œëœ Python ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì‚¬ìš©
        images = data.get('images', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(images)}ê°œ ì´ë¯¸ì§€")
        
        # ğŸš€ ìµœì í™” 2: scandirë¡œ ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” (í•œ ë²ˆë§Œ)
        print("  ğŸ”„ scandirë¡œ ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        filename_to_path = {}
        
        # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        dataset_lookup_name = "handwriting_train" if 'train' in dataset_name.lower() else "handwriting_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìŠ¤ìº” (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        fallback_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            # Training/Validation êµ¬ë¶„í•´ì„œ ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            if 'train' in dataset_name.lower():
                scan_dirs = [f"{base_path}/1.Training/ì›ì²œë°ì´í„°"]
            else:
                scan_dirs = [f"{base_path}/2.Validation/ì›ì²œë°ì´í„°"]
            
            for scan_dir in scan_dirs:
                if os.path.exists(scan_dir):
                    scanned_files = scan_images_recursive_with_scandir(scan_dir, extensions=('.png',))
                    fallback_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(fallback_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ìµœì í™” 3: ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
        print("  ğŸ”„ ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì¤‘...")
        image_info = {}  # file_name â†’ image_info
        
        target_count = max_samples if max_samples else None
        if target_count:
            print(f"  ğŸ“Š ëª©í‘œ ì´ë¯¸ì§€ ìˆ˜: {target_count}ê°œ (ì œí•œ)")
        else:
            print(f"  ğŸ“Š ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)")
        
        # orjson ë¡œë“œëœ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ len() ì‚¬ìš© ê°€ëŠ¥
        if target_count and len(images) > target_count:
            print(f"ğŸ“Š {target_count}ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì´ {len(images)}ê°œ ì¤‘)")
            random.seed(42)
            random.shuffle(images)
            images = images[:target_count]
        
        matched_count = 0
        for img in images:
            img_file_name = img.get('file_name', '')
            
            # í™•ì¥ì ì¶”ê°€
            if img_file_name and not img_file_name.endswith('.png'):
                filename = f"{img_file_name}.png"
            else:
                filename = img_file_name
            
            # ğŸš€ ìµœì í™”ëœ ê²½ë¡œ ì°¾ê¸°
            img_path = optimized_find_image_path(filename, base_path, dataset_lookup_name, fallback_cache)
            if img_path:
                image_info[img_file_name] = {
                    'file_path': img_path,
                    'width': img.get('width', 1000),
                    'height': img.get('height', 1000),
                    'filename': filename,
                    'original_json_path': img.get('original_json_path', '')
                }
                matched_count += 1
        
        print(f"  âœ… ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì™„ë£Œ: {len(image_info)}ê°œ")

        # ğŸš€ ì–´ë…¸í…Œì´ì…˜ì„ image_id ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
        annotations = data.get('annotations', [])
        print("  ğŸ”„ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì¤‘...")
        image_id_to_filename = {}
        for img in images:
            fid = img.get('id')
            fname = img.get('file_name', '')
            image_id_to_filename[fid] = fname

        image_annotations = {}
        for ann in annotations:
            img_id = ann.get('image_id')
            key_fname = image_id_to_filename.get(img_id)
            if not key_fname:
                continue
            if key_fname not in image_annotations:
                image_annotations[key_fname] = []
            image_annotations[key_fname].append(ann)
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì™„ë£Œ: {len(image_annotations)}ê°œ ì´ë¯¸ì§€")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ JSON ë°ì´í„° í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        del data
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ì§€ë³„ ì–´ë…¸í…Œì´ì…˜ ì „ë‹¬)
        process_args = []
        for img_file_name, info in image_info.items():
            anns = image_annotations.get(img_file_name, [])
            process_args.append((img_file_name, info, anns))
        print(f"  ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(process_args)}ê°œ")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
        del images
        del image_info
        del fallback_cache
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ LMDB ìƒì„±
        create_parallel_lmdb_from_args(process_args, output_path, dataset_name, process_single_handwriting_image)
        
    except Exception as e:
        print(f"âŒ ì†ê¸€ì”¨ LMDB ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ============================================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def group_images_by_original_json(data):
    """ì´ë¯¸ì§€ë“¤ì„ original_json_pathë³„ë¡œ ê·¸ë£¹í™”"""
    groups = {}
    
    for img in data.get('images', []):
        original_path = img.get('original_json_path', '')
        if original_path not in groups:
            groups[original_path] = []
        groups[original_path].append(img)
    
    return groups

def process_json_by_groups(json_path, process_func, max_samples=None):
    """JSON íŒŒì¼ì„ ì›ë³¸ íŒŒì¼ë³„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬"""
    print(f"ğŸ“„ JSON íŒŒì¼ì„ ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬ ì¤‘: {json_path}")
    
    # JSON íŒŒì¼ ë¡œë“œ (fallback ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # ì›ë³¸ JSON íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
        groups = group_images_by_original_json(data)
        print(f"ğŸ“Š ì´ {len(groups)}ê°œì˜ ì›ë³¸ JSON íŒŒì¼ ê·¸ë£¹ ë°œê²¬")
        
        # ê° ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬
        total_processed = 0
        for original_path, images in groups.items():
            if max_samples and total_processed >= max_samples:
                break
                
            print(f"ğŸ” ê·¸ë£¹ ì²˜ë¦¬ ì¤‘: {os.path.basename(original_path)} ({len(images)}ê°œ ì´ë¯¸ì§€)")
            
            # ê·¸ë£¹ë³„ ë°ì´í„° êµ¬ì„±
            group_data = {
                'images': images,
                'annotations': [ann for ann in data.get('annotations', []) 
                              if any(img.get('original_json_path') == original_path 
                                    for img in images if img.get('id') == ann.get('image_id'))],
                'info': data.get('info', {}),
                'categories': data.get('categories', [])
            }
            
            # ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
            processed_count = process_func(group_data, original_path)
            total_processed += processed_count
            
            print(f"âœ… ê·¸ë£¹ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬ë¨ (ì´ {total_processed}ê°œ)")
        
        return total_processed
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

def test_fast_model_input(lmdb_path):
    """ìƒì„±ëœ LMDBê°€ FAST ëª¨ë¸ì˜ ì…ë ¥ í˜•ì‹ì— ë§ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ” FAST ëª¨ë¸ ì…ë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸: {lmdb_path}")
    
    try:
        dataset = FAST_LMDB(
            lmdb_path=lmdb_path,
            split='train',
            is_transform=False,
            img_size=(640, 640),
            short_size=640
        )
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
        
        # ëª‡ ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        for i in range(min(5, len(dataset))):
            print(f"\nğŸ§ª ìƒ˜í”Œ {i+1} í…ŒìŠ¤íŠ¸:")
            
            img, gt_info = dataset.get_image_and_gt(i)
            print(f"   - ì›ë³¸ ì´ë¯¸ì§€ í˜•íƒœ: {img.shape}")
            print(f"   - ë°”ìš´ë”© ë°•ìŠ¤ ìˆ˜: {len(gt_info['bboxes'])}")
            print(f"   - í…ìŠ¤íŠ¸ ìˆ˜: {len(gt_info['words'])}")
            print(f"   - íŒŒì¼ëª…: {gt_info['filename']}")
            
            if gt_info['bboxes']:
                print(f"   - ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸: {gt_info['words'][0]}")
                if len(gt_info['words']) > 1:
                    print(f"   - ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸: {gt_info['words'][1]}")
                if len(gt_info['words']) > 2:
                    print(f"   - ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸: {gt_info['words'][2]}")
        
        print(f"âœ… FAST ëª¨ë¸ ì…ë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ FAST ëª¨ë¸ ì…ë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì¶œë ¥ í˜•ì‹ ì„ íƒ: env ë˜ëŠ” ê¸°ë³¸ê°’(jsonl)
    output_format = os.getenv("FAST_OUTPUT_FORMAT", "jsonl").lower()
    max_samples_env = os.getenv("FAST_MAX_SAMPLES")
    try:
        max_samples_limit = int(max_samples_env) if max_samples_env is not None else None
    except ValueError:
        max_samples_limit = None
    use_jsonl = (output_format == "jsonl")

    print(
        "ğŸš€ ëª¨ë“  í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ train/valid "
        + ("JSONL" if use_jsonl else "LMDB")
        + " ìƒì„± (ì „ì²´ ë°ì´í„°, ì œí•œ ì—†ìŒ)"
    )
    print("=" * 60)
    
    # gvfs FTP ê²½ë¡œ í™•ì¸
    if not is_ftp_mounted():
        print("âŒ gvfs FTP ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
        print("ğŸ’¡ íŒŒì¼ ê´€ë¦¬ìì—ì„œ FTP ì„œë²„ì— ì ‘ì†í•˜ì—¬ gvfs ë§ˆìš´íŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”")
        return
    
    if not os.path.exists(FTP_BASE_PATH):
        print("âŒ gvfs FTP ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
        return
    
    print("âœ… gvfs FTP ê²½ë¡œ í™•ì¸ ì™„ë£Œ")
    
    # ğŸš€ ìµœì í™”ëœ lookup íŒŒì¼ ìƒíƒœ í™•ì¸ (pickle ìš°ì„ )
    print("\nğŸ” ìµœì í™”ëœ lookup íŒŒì¼ ìƒíƒœ í™•ì¸:")
    datasets = [
        "handwriting_train", "handwriting_valid", 
        "finance_logistics_train", "finance_logistics_valid",
        "ocr_public_train", "ocr_public_valid",
        "public_admin_train", "public_admin_train_partly", "public_admin_valid"
    ]
    
    available_count = 0
    pickle_count = 0
    py_count = 0
    
    for dataset in datasets:
        pkl_gz_file = f"FAST/lookup_{dataset}.pkl.gz"
        pkl_file = f"FAST/lookup_{dataset}.pkl"
        py_file = f"FAST/optimized_lookup_{dataset}.py"
        
        if os.path.exists(pkl_gz_file):
            print(f"  ğŸš€ {dataset} (ì••ì¶•ëœ pickle - ìµœê³ ì†)")
            available_count += 1
            pickle_count += 1
        elif os.path.exists(pkl_file):
            print(f"  âš¡ {dataset} (pickle - ê³ ì†)")
            available_count += 1
            pickle_count += 1
        elif os.path.exists(py_file):
            print(f"  ğŸŒ {dataset} (Python ëª¨ë“ˆ - ì €ì†)")
            available_count += 1
            py_count += 1
        else:
            print(f"  âš ï¸ {dataset} (fallback ì‚¬ìš©)")
    
    print(f"\nğŸ“Š ìµœì í™”ëœ lookup: {available_count}/{len(datasets)}ê°œ ì‚¬ìš© ê°€ëŠ¥")
    print(f"   ğŸš€ Pickle: {pickle_count}ê°œ (ê³ ì†)")
    print(f"   ğŸŒ Python: {py_count}ê°œ (ì €ì†)")
    
    if available_count == 0:
        print("ğŸ’¡ ftp_tree_viewer.pyë¥¼ ì‹¤í–‰í•´ì„œ ìµœì í™”ëœ lookup í•¨ìˆ˜ë“¤ì„ ìƒì„±í•˜ë©´ ì†ë„ê°€ ëŒ€í­ ê°œì„ ë©ë‹ˆë‹¤!")
        print("ğŸ’¡ ê·¸ ë‹¤ìŒ convert_lookup_to_pickle.pyë¥¼ ì‹¤í–‰í•´ì„œ pickleë¡œ ë³€í™˜í•˜ë©´ ë”ìš± ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
    elif pickle_count == 0 and py_count > 0:
        print("ğŸ’¡ convert_lookup_to_pickle.pyë¥¼ ì‹¤í–‰í•´ì„œ Python ëª¨ë“ˆì„ pickleë¡œ ë³€í™˜í•˜ë©´ 5-10ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
    elif pickle_count < len(datasets):
        print("ğŸ’¡ ì¼ë¶€ lookupë§Œ pickleë¡œ ìµœì í™”ë¨. ëˆ„ë½ëœ ê²ƒë“¤ì€ convert_lookup_to_pickle.pyë¡œ ë³€í™˜í•˜ì„¸ìš”!")
    else:
        print("ğŸš€ ëª¨ë“  lookupì´ pickleë¡œ ìµœì í™”ë¨! ìµœê³  ì„±ëŠ¥ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤!")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(LOCAL_OUTPUT_PATH, exist_ok=True)
    
    # ì´ë¯¸ ì™„ë£Œëœ ì‚°ì¶œë¬¼ í™•ì¸
    completed_lmdbs = []
    completed_jsonls = []
    lmdb_paths = [
        f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_train.lmdb",
        f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_valid.lmdb",
        f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train.lmdb",
        f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train_partly.lmdb",
        f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_valid.lmdb",
        f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_train.lmdb",
        f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_valid.lmdb",
        f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_train.lmdb",
        f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_valid.lmdb",
        f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_train.lmdb",
        f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_valid.lmdb"
    ]
    jsonl_paths = [
        f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_train.jsonl",
        f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_valid.jsonl",
        f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train.jsonl",
        f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train_partly.jsonl",
        f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_valid.jsonl",
        f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_train.jsonl",
        f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_valid.jsonl",
        f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_train.jsonl",
        f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_valid.jsonl",
        f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_train.jsonl",
        f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_valid.jsonl",
    ]
    
    for lmdb_path in lmdb_paths:
        if os.path.exists(lmdb_path):
            completed_lmdbs.append(lmdb_path)
            print(f"âœ… ì´ë¯¸ ì™„ë£Œë¨: {os.path.basename(lmdb_path)}")
    for jsonl_path in jsonl_paths:
        if os.path.exists(jsonl_path):
            completed_jsonls.append(jsonl_path)
            print(f"âœ… ì´ë¯¸ ì™„ë£Œë¨: {os.path.basename(jsonl_path)}")
    
    if use_jsonl:
        # JSONL ê²½ë¡œ ì§€ì •
        text_wild_train_jsonl = f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_train.jsonl"
        text_wild_valid_jsonl = f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_valid.jsonl"
        public_admin_train_jsonl = f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train.jsonl"
        public_admin_train_partly_jsonl = f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train_partly.jsonl"
        public_admin_valid_jsonl = f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_valid.jsonl"
        ocr_public_train_jsonl = f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_train.jsonl"
        ocr_public_valid_jsonl = f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_valid.jsonl"
        finance_train_jsonl = f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_train.jsonl"
        finance_valid_jsonl = f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_valid.jsonl"
        handwriting_train_jsonl = f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_train.jsonl"
        handwriting_valid_jsonl = f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_valid.jsonl"

        # Text in the wild
        if text_wild_train_jsonl not in completed_jsonls:
            base_path = f"{FTP_BASE_PATH}/13.í•œêµ­ì–´ê¸€ìì²´/04. Text in the wild_230209_add"
            json_path = f"{MERGED_JSON_PATH}/textinthewild_data_info.json"
            if os.path.exists(json_path):
                create_jsonl_text_in_wild_split(
                    base_path,
                    json_path,
                    text_wild_train_jsonl,
                    text_wild_valid_jsonl,
                    train_ratio=0.9,
                    max_samples=max_samples_limit,
                    random_seed=42,
                )
            else:
                print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        else:
            print("â­ï¸ Text in the wild train/valid JSONL ì´ë¯¸ ì™„ë£Œë¨")

        # Public Admin
        base_path = f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR"
        train_json_path = f"{MERGED_JSON_PATH}/public_admin_train_merged.json"
        valid_json_path = f"{MERGED_JSON_PATH}/public_admin_valid_merged.json"
        train_partly_json_path = f"{MERGED_JSON_PATH}/public_admin_train_partly_merged.json"

        if os.path.exists(train_json_path) and public_admin_train_jsonl not in completed_jsonls:
            create_jsonl_public_admin_from_json(base_path, train_json_path, public_admin_train_jsonl, "ê³µê³µí–‰ì •ë¬¸ì„œ Train", max_samples_limit)
        if os.path.exists(train_partly_json_path) and public_admin_train_partly_jsonl not in completed_jsonls:
            create_jsonl_public_admin_from_json(base_path, train_partly_json_path, public_admin_train_partly_jsonl, "ê³µê³µí–‰ì •ë¬¸ì„œ Train Partly", max_samples_limit)
        if os.path.exists(valid_json_path) and public_admin_valid_jsonl not in completed_jsonls:
            create_jsonl_public_admin_from_json(base_path, valid_json_path, public_admin_valid_jsonl, "ê³µê³µí–‰ì •ë¬¸ì„œ Valid", max_samples_limit)

        # OCR Public
        base_path = f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°"
        train_json_path = f"{MERGED_JSON_PATH}/ocr_public_train_merged.json"
        valid_json_path = f"{MERGED_JSON_PATH}/ocr_public_valid_merged.json"
        if os.path.exists(train_json_path) and ocr_public_train_jsonl not in completed_jsonls:
            create_jsonl_ocr_public_from_json(base_path, train_json_path, ocr_public_train_jsonl, "OCR ê³µê³µ Train", max_samples_limit)
        if os.path.exists(valid_json_path) and ocr_public_valid_jsonl not in completed_jsonls:
            create_jsonl_ocr_public_from_json(base_path, valid_json_path, ocr_public_valid_jsonl, "OCR ê³µê³µ Valid", max_samples_limit)

        # Finance & Logistics
        base_path = f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°"
        train_json_path = f"{MERGED_JSON_PATH}/finance_logistics_train_merged.json"
        valid_json_path = f"{MERGED_JSON_PATH}/finance_logistics_valid_merged.json"
        if os.path.exists(train_json_path) and finance_train_jsonl not in completed_jsonls:
            create_jsonl_finance_logistics_from_json(base_path, train_json_path, finance_train_jsonl, "ê¸ˆìœµë¬¼ë¥˜ Train", max_samples_limit)
        if os.path.exists(valid_json_path) and finance_valid_jsonl not in completed_jsonls:
            create_jsonl_finance_logistics_from_json(base_path, valid_json_path, finance_valid_jsonl, "ê¸ˆìœµë¬¼ë¥˜ Valid", max_samples_limit)

        # Handwriting
        base_path = f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°"
        train_json_path = f"{MERGED_JSON_PATH}/handwriting_train_merged.json"
        valid_json_path = f"{MERGED_JSON_PATH}/handwriting_valid_merged.json"
        if os.path.exists(train_json_path) and handwriting_train_jsonl not in completed_jsonls:
            create_jsonl_handwriting_from_json(base_path, train_json_path, handwriting_train_jsonl, "ì†ê¸€ì”¨ Train", max_samples_limit)
        if os.path.exists(valid_json_path) and handwriting_valid_jsonl not in completed_jsonls:
            create_jsonl_handwriting_from_json(base_path, valid_json_path, handwriting_valid_jsonl, "ì†ê¸€ì”¨ Valid", max_samples_limit)
    else:
        # ê° ë°ì´í„°ì…‹ë³„ë¡œ train/valid LMDB ìƒì„± (ì™„ë£Œëœ ê²ƒ ì œì™¸) - ì „ì²´ ë°ì´í„° ì²˜ë¦¬
        if f"{LOCAL_OUTPUT_PATH}/text_in_wild_annotations_train.lmdb" not in completed_lmdbs:
            create_text_in_wild_train_valid(max_samples=max_samples_limit)
        else:
            print("â­ï¸ Text in the wild train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
        if f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train.lmdb" not in completed_lmdbs:
            create_public_admin_train_valid(max_samples=max_samples_limit)
        else:
            print("â­ï¸ ê³µê³µí–‰ì •ë¬¸ì„œ OCR train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
        if f"{LOCAL_OUTPUT_PATH}/public_admin_annotations_train_partly.lmdb" not in completed_lmdbs:
            create_public_admin_train_partly(max_samples=max_samples_limit)
        else:
            print("â­ï¸ ê³µê³µí–‰ì •ë¬¸ì„œ OCR train_partly LMDB ì´ë¯¸ ì™„ë£Œë¨")
        
        if f"{LOCAL_OUTPUT_PATH}/ocr_public_annotations_train.lmdb" not in completed_lmdbs:
            create_ocr_public_train_valid(max_samples=max_samples_limit)
        else:
            print("â­ï¸ 023.OCR ë°ì´í„°(ê³µê³µ) train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
        if f"{LOCAL_OUTPUT_PATH}/finance_logistics_annotations_train.lmdb" not in completed_lmdbs:
            create_finance_logistics_train_valid(max_samples=max_samples_limit)
        else:
            print("â­ï¸ 025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜) train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
        if f"{LOCAL_OUTPUT_PATH}/handwriting_annotations_train.lmdb" not in completed_lmdbs:
            create_handwriting_train_valid(max_samples=max_samples_limit)
        else:
            print("â­ï¸ 053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    print("\n" + "=" * 60)
    if use_jsonl:
        print("âœ… ëª¨ë“  ë°ì´í„°ì…‹ train/valid JSONL ìƒì„± ì™„ë£Œ! (ì „ì²´ ë°ì´í„° ë³€í™˜)")
        print("\nğŸ“ ìƒì„±ëœ JSONL íŒŒì¼ë“¤:")
        for jsonl_path in jsonl_paths:
            if os.path.exists(jsonl_path):
                print(f"   - {jsonl_path}")
    else:
        print("âœ… ëª¨ë“  ë°ì´í„°ì…‹ train/valid LMDB ìƒì„± ì™„ë£Œ! (ì „ì²´ ë°ì´í„° ë³€í™˜)")
        print("\nğŸ“ ìƒì„±ëœ LMDB íŒŒì¼ë“¤:")
        for lmdb_path in lmdb_paths:
            if os.path.exists(lmdb_path):
                print(f"   - {lmdb_path}")

if __name__ == '__main__':
    main() 