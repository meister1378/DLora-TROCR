#!/usr/bin/env python3
"""
Multi LMDB ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ FAST ëª¨ë¸ í›ˆë ¨ (DeepSpeed ì ìš©)
"""

import argparse
import os
import sys
import time
import torch
import deepspeed
from torch.utils.data import DataLoader
from mmcv import Config
from tqdm import tqdm

# FAST ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from models import build_model
from dataset.fast.multi_lmdb_dataset import MultiLMDBDataset, ConcatLMDBDataset


def create_dataset(strategy, **kwargs):
    """í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜"""
    # LMDB ê²½ë¡œë“¤ 
    lmdb_paths = [
        "/mnt/nas/ocr_dataset/text_in_wild_train.lmdb",
        "/mnt/nas/ocr_dataset/public_admin_train.lmdb", 
        "/mnt/nas/ocr_dataset/ocr_public_train.lmdb",
        "/mnt/nas/ocr_dataset/finance_logistics_train.lmdb",
        "/mnt/nas/ocr_dataset/handwriting_train.lmdb"
    ]
    
    # ì¡´ì¬í•˜ëŠ” LMDBë§Œ í•„í„°ë§
    existing_paths = [path for path in lmdb_paths if os.path.exists(path)]
    print(f"ğŸ”§ í›ˆë ¨ ë°ì´í„°ì…‹: {len(existing_paths)}ê°œ LMDB ë°œê²¬")
    
    if strategy == 'concat':
        return ConcatLMDBDataset(existing_paths, **kwargs)
    else:
        # ê¸°ë³¸ì ìœ¼ë¡œ concat ì‚¬ìš©
        return ConcatLMDBDataset(existing_paths, **kwargs)


def create_validation_dataset(**kwargs):
    """ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜"""
    lmdb_paths = [
        "/mnt/nas/ocr_dataset/text_in_wild_valid.lmdb",
        "/mnt/nas/ocr_dataset/public_admin_valid.lmdb",
        "/mnt/nas/ocr_dataset/ocr_public_valid.lmdb", 
        "/mnt/nas/ocr_dataset/finance_logistics_valid.lmdb",
        "/mnt/nas/ocr_dataset/handwriting_valid.lmdb"
    ]
    
    # ì¡´ì¬í•˜ëŠ” LMDBë§Œ í•„í„°ë§
    existing_paths = [path for path in lmdb_paths if os.path.exists(path)]
    print(f"ğŸ”§ ê²€ì¦ ë°ì´í„°ì…‹: {len(existing_paths)}ê°œ Valid LMDB ê²°í•©")
    
    if existing_paths:
        return ConcatLMDBDataset(existing_paths, **kwargs)
    else:
        return None


def get_args():
    parser = argparse.ArgumentParser(description='DeepSpeed Multi LMDB FAST í›ˆë ¨')
    
    # ìš°ë¦¬ custom arguments ë¨¼ì € ì¶”ê°€
    parser.add_argument('--config', type=str, required=True,
                        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--strategy', type=str, default='concat',
                        choices=['balanced', 'weighted', 'selective', 'concat'],
                        help='ë°ì´í„°ì…‹ ê²°í•© ì „ëµ')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='ì‚¬ì „í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, default='./work_dirs/multi_lmdb_deepspeed',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    # TrOCRì™€ ë™ì¼í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì • (autoë¥¼ ìœ„í•œ ëª…ì‹œì  ì§€ì •)
    parser.add_argument('--per_device_train_batch_size', type=int, default=8,
                        help='í›ˆë ¨ì‹œ ë””ë°”ì´ìŠ¤ ë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--per_device_eval_batch_size', type=int, default=8,
                        help='í‰ê°€ì‹œ ë””ë°”ì´ìŠ¤ ë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=16,
                        help='ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                        help='í•™ìŠµë¥ ')
    
    # DeepSpeedê°€ í•„ìš”í•œ --local_rank ìˆ˜ë™ ì¶”ê°€
    parser.add_argument('--local_rank', type=int, default=0,
                        help='local rank for DeepSpeed')
    
    # ì´ì œ DeepSpeed arguments ì¶”ê°€ (--local_rank ì œì™¸)
    parser = deepspeed.add_config_arguments(parser)
    
    return parser.parse_args()


def main():
    args = get_args()
    
    # ì„¤ì • ë¡œë“œ
    cfg = Config.fromfile(args.config)
    
    print("ğŸš€ DeepSpeed Multi LMDB FAST í›ˆë ¨ ì‹œì‘!")
    print(f"ğŸ“ ì„¤ì • íŒŒì¼: {args.config}")
    print(f"ğŸ“Š ì „ëµ: {args.strategy}")
    print(f"âš¡ DeepSpeed ì„¤ì •: {args.deepspeed_config}")
    
    # í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ”§ í›ˆë ¨ ë°ì´í„°ì…‹: {args.strategy} ì „ëµìœ¼ë¡œ ëª¨ë“  LMDB ê²°í•©")
    train_dataset = create_dataset(
        'weighted' if args.strategy == 'weighted' else 'concat',
        split='train',
        is_transform=True,
        img_size=(640, 640),
        short_size=640
    )
    
    # ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± (ì„ íƒì )
    print(f"ğŸ”§ ê²€ì¦ ë°ì´í„°ì…‹: 5ê°œ Valid LMDB ê²°í•©")
    val_dataset = create_validation_dataset(
        split='test',
        is_transform=False,
        img_size=(640, 640),
        short_size=640
    )
    
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_dataset):,}ê°œ ì´ë¯¸ì§€")
    print(f"   ğŸ’¡ ì°¸ê³ : ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ì€ {len(train_dataset)*25:,}ê°œ ì •ë„ (ì´ë¯¸ì§€ë‹¹ í‰ê·  25ê°œ)")
    if val_dataset:
        print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ ì´ë¯¸ì§€")
        print(f"   ğŸ’¡ ì°¸ê³ : ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ì€ {len(val_dataset)*25:,}ê°œ ì •ë„")
    else:
        print("ğŸ“Š ê²€ì¦ ë°ì´í„°: ì—†ìŒ (Train ë°ì´í„°ë§Œ ì‚¬ìš©)")
    
    # ëª¨ë¸ ìƒì„±
    print("ğŸ”§ FAST ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = build_model(cfg.model)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if args.checkpoint and os.path.exists(args.checkpoint):
        print(f"ğŸ“¦ ì‚¬ì „í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {args.checkpoint}")
        checkpoint = torch.load(args.checkpoint, map_location='cpu')
        
        # EMA ë˜ëŠ” ì§ì ‘ state_dict í™•ì¸
        if 'ema' in checkpoint:
            state_dict = checkpoint['ema']
            print("   - EMA ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©")
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            print("   - ì¼ë°˜ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©")
        else:
            state_dict = checkpoint
            print("   - ì²´í¬í¬ì¸íŠ¸ ìì²´ë¥¼ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ë¡œ ì‚¬ìš©")
        
        # í‚¤ì—ì„œ 'module.' ì œê±°
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("module.", "")
            new_state_dict[new_key] = value
        
        # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë¡œë“œ
        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ (ëˆ„ë½: {len(missing_keys)}, ì˜ˆìƒì™¸: {len(unexpected_keys)})")
    else:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.checkpoint}")
        print("   - ì²´í¬í¬ì¸íŠ¸ ì—†ì´ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # DeepSpeed ì´ˆê¸°í™” (ë§ˆì´í¬ë¡œ ë°°ì¹˜ 4ë¡œ 105,374 ë°°ì¹˜)
    model_engine, optimizer, _, scheduler = deepspeed.initialize(
        args=args,
        model=model
    )
    
    # ë§ˆì´í¬ë¡œ ë°°ì¹˜ í¬ê¸°ë¡œ DataLoader ìƒì„± (ë°°ì¹˜ ìˆ˜ 105,374)
    micro_batch_size = args.per_device_train_batch_size  # 4 (GPU ë©”ëª¨ë¦¬ ê³ ì •)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=micro_batch_size,  # 4 (ë°°ì¹˜ ìˆ˜ 105,374)
        shuffle=True,
        num_workers=8,  # 4 â†’ 8ë¡œ ì¦ê°€ (CPU ì½”ì–´ í™œìš©)
        pin_memory=True,  # False â†’ True (GPU ì „ì†¡ ì†ë„ í–¥ìƒ)
        drop_last=True,
        persistent_workers=True,  # False â†’ True (ì›Œì»¤ ì¬ì‚¬ìš©)
        prefetch_factor=4  # 1 â†’ 4ë¡œ ì¦ê°€ (ë¯¸ë¦¬ ë¡œë”©)
    )
    
    # ê²€ì¦ ë°ì´í„°ë¡œë” (ë§ˆì´í¬ë¡œ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©)
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset, 
            batch_size=micro_batch_size,  # 4 (GPU ë©”ëª¨ë¦¬ ê³ ì •)
            shuffle=False,
            num_workers=8,  # 4 â†’ 8ë¡œ ì¦ê°€ (CPU í™œìš©)  
            pin_memory=True,  # False â†’ True (GPU ì „ì†¡ ì†ë„ í–¥ìƒ)
            persistent_workers=True  # False â†’ True (ì›Œì»¤ ì¬ì‚¬ìš©)
        )
    
    print(f"ğŸ”„ ë°°ì¹˜ í¬ê¸°: {model_engine.train_micro_batch_size_per_gpu()}")
    print(f"ğŸ”„ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
    if val_loader:
        print(f"ğŸ”„ ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
    else:
        print("ğŸ”„ ê²€ì¦ ë°°ì¹˜ ìˆ˜: ì—†ìŒ")
    
    # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ë°©ì‹)
    effective_batch_size = model_engine.train_micro_batch_size_per_gpu() * model_engine.gradient_accumulation_steps()
    
    print(f"ğŸ”§ DeepSpeed ì„¤ì •:")
    print(f"   - ë§ˆì´í¬ë¡œ ë°°ì¹˜ í¬ê¸°: {model_engine.train_micro_batch_size_per_gpu()}")
    print(f"   - ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {model_engine.gradient_accumulation_steps()}")
    print(f"   - íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: {effective_batch_size}")
    print(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {model_engine.train_micro_batch_size_per_gpu()} ë°°ì¹˜ í¬ê¸°ë§Œ")
    print(f"   - í˜¼í•© ì •ë°€ë„: {model_engine.fp16_enabled()}")
    
    # í›ˆë ¨ ë£¨í”„
    print("ğŸš€ DeepSpeed Multi LMDB í›ˆë ¨ ì‹œì‘!")
    
    total_epochs = 10  # HuggingFace ìŠ¤íƒ€ì¼
    # ì—í¬í¬ ì§„í–‰ë¥  í‘œì‹œ
    epoch_pbar = tqdm(range(total_epochs), desc="ğŸ¯ ì—í¬í¬", unit="epoch")
    
    for epoch in epoch_pbar:
        epoch_pbar.set_description(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{total_epochs}")
        
        # í›ˆë ¨
        model_engine.train()
        train_loss = 0.0
        start_time = time.time()
        
        # ë°°ì¹˜ ì§„í–‰ë¥  í‘œì‹œ
        batch_pbar = tqdm(train_loader, desc=f"ğŸ“š í›ˆë ¨ ì¤‘", unit="batch", leave=False)
        
        for batch_idx, batch in enumerate(batch_pbar):
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ ë° GPUë¡œ ì´ë™
            imgs = batch['imgs'].cuda()
            gt_texts = batch['gt_texts'].cuda() if 'gt_texts' in batch and batch['gt_texts'] is not None else None
            gt_kernels = batch['gt_kernels'].cuda() if 'gt_kernels' in batch and batch['gt_kernels'] is not None else None
            training_masks = batch['training_masks'].cuda() if 'training_masks' in batch and batch['training_masks'] is not None else None
            gt_instances = batch['gt_instances'].cuda() if 'gt_instances' in batch and batch['gt_instances'] is not None else None
            
            # Forward pass
            try:
                outputs = model_engine(
                    imgs,
                    gt_texts=gt_texts,
                    gt_kernels=gt_kernels,
                    training_masks=training_masks,
                    gt_instances=gt_instances
                )
                
                # ì†ì‹¤ ê³„ì‚°
                loss_text = outputs['loss_text'].mean()
                loss_kernels = outputs['loss_kernels'].mean()
                loss_emb = outputs['loss_emb'].mean()
                
                total_loss = loss_text + loss_kernels + loss_emb
                
                # DeepSpeed backward (ìë™ gradient accumulation)
                model_engine.backward(total_loss)
                model_engine.step()  # DeepSpeedê°€ ìë™ìœ¼ë¡œ gradient accumulation ì²˜ë¦¬
                train_loss += total_loss.item()
                
                # tqdm ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                avg_loss = train_loss / (batch_idx + 1)
                elapsed = time.time() - start_time
                batches_per_sec = (batch_idx + 1) / elapsed if elapsed > 0 else 0
                
                batch_pbar.set_postfix({
                    'Loss': f"{total_loss.item():.4f}",
                    'Avg': f"{avg_loss:.4f}",
                    'Text': f"{loss_text.item():.3f}",
                    'Kernel': f"{loss_kernels.item():.3f}",
                    'Emb': f"{loss_emb.item():.3f}",
                    'Speed': f"{batches_per_sec:.1f}b/s",
                    'GPU_Mem': f"ë°°ì¹˜{model_engine.train_micro_batch_size_per_gpu()}ê³ ì •"
                })
                
                # ì¤‘ìš”í•œ ë§ˆì¼ìŠ¤í†¤ë§Œ print ì¶œë ¥
                if (batch_idx + 1) % 1000 == 0:
                    tqdm.write(f"âœ… ë°°ì¹˜ {batch_idx+1:,} - Loss: {total_loss.item():.4f} (GPU ë©”ëª¨ë¦¬: ë°°ì¹˜ {model_engine.train_micro_batch_size_per_gpu()} ê³ ì •)")
            
            except Exception as e:
                tqdm.write(f"âŒ í›ˆë ¨ ì˜¤ë¥˜ (ë°°ì¹˜ {batch_idx}): {e}")
                continue
        
        # ë°°ì¹˜ progress bar ë‹«ê¸°
        batch_pbar.close()
        
        # ì—í¬í¬ í†µê³„
        epoch_time = time.time() - start_time
        avg_train_loss = train_loss / len(train_loader)
        
        # ì—í¬í¬ progress bar ì—…ë°ì´íŠ¸
        epoch_pbar.set_postfix({
            'Train_Loss': f"{avg_train_loss:.4f}",
            'Time': f"{epoch_time:.1f}s"
        })
        
        tqdm.write(f"ğŸ“Š ì—í¬í¬ {epoch+1} ì™„ë£Œ ({epoch_time:.1f}ì´ˆ)")
        tqdm.write(f"   - í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
        
        # ê²€ì¦ (10 ì—í¬í¬ë§ˆë‹¤)
        if val_loader and (epoch + 1) % 2 == 0:  # ë” ìì£¼ ê²€ì¦
            model_engine.eval()
            val_loss = 0.0
            val_start = time.time()
            
            tqdm.write(f"\nğŸ” Validation ì‹œì‘ (ì—í¬í¬ {epoch+1})")
            
            with torch.no_grad():
                val_pbar = tqdm(val_loader, desc="ğŸ” ê²€ì¦ ì¤‘", unit="batch", leave=False)
                for batch in val_pbar:
                    imgs = batch['imgs']
                    gt_texts = batch['gt_texts'] if 'gt_texts' in batch else None
                    gt_kernels = batch['gt_kernels'] if 'gt_kernels' in batch else None
                    training_masks = batch['training_masks'] if 'training_masks' in batch else None
                    gt_instances = batch['gt_instances'] if 'gt_instances' in batch else None
                    
                    # ê²€ì¦ ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ (íš¨ìœ¨ì ì¸ ë°©ì‹)
                    imgs = imgs.cuda()
                    gt_texts = gt_texts.cuda() if gt_texts is not None else None
                    gt_kernels = gt_kernels.cuda() if gt_kernels is not None else None
                    training_masks = training_masks.cuda() if training_masks is not None else None
                    gt_instances = gt_instances.cuda() if gt_instances is not None else None
                    
                    try:
                        outputs = model_engine(
                            imgs,
                            gt_texts=gt_texts,
                            gt_kernels=gt_kernels,
                            training_masks=training_masks,
                            gt_instances=gt_instances
                        )
                        
                        loss_text = outputs['loss_text'].mean()
                        loss_kernels = outputs['loss_kernels'].mean()
                        loss_emb = outputs['loss_emb'].mean()
                        
                        total_loss = loss_text + loss_kernels + loss_emb
                        val_loss += total_loss.item()
                        
                        # validation progress bar ì—…ë°ì´íŠ¸
                        val_pbar.set_postfix({'Val_Loss': f"{total_loss.item():.4f}"})
                    except:
                        continue
                
                val_pbar.close()
            
            avg_val_loss = val_loss / len(val_loader)
            val_time = time.time() - val_start
            tqdm.write(f"ğŸ“Š ê²€ì¦ ì™„ë£Œ ({val_time:.1f}ì´ˆ)")
            tqdm.write(f"   - í‰ê·  ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (5 ì—í¬í¬ë§ˆë‹¤)
        if (epoch + 1) % 5 == 0:
            checkpoint_dir = f"{args.output_dir}/checkpoint_latest"
            os.makedirs(checkpoint_dir, exist_ok=True)
            model_engine.save_checkpoint(checkpoint_dir)
            tqdm.write(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_dir} (ì—í¬í¬ {epoch+1})")
    
    # ì—í¬í¬ progress bar ë‹«ê¸°
    epoch_pbar.close()
    
    tqdm.write("âœ… í›ˆë ¨ ì™„ë£Œ!")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_checkpoint = f"{args.output_dir}/final_model"
    os.makedirs(final_checkpoint, exist_ok=True)
    model_engine.save_checkpoint(final_checkpoint)
    tqdm.write(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_checkpoint}")
    print(f"ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥: {final_checkpoint}")


if __name__ == '__main__':
    main() 