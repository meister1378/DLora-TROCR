#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
OCR ë°ì´í„°ì…‹ ì™„ì „ ë§¤í•‘ ê·œì¹™ ë¶„ì„ ë° ìµœì í™” í•¨ìˆ˜ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)
"""

import os
import sys
import orjson
import bigjson
from pathlib import Path
from collections import defaultdict
import random
import re

# FTP ë§ˆìš´íŠ¸ëœ ë°ì´í„°ì…‹ ê¸°ë³¸ ê²½ë¡œ
FTP_BASE_PATH = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"
MERGED_JSON_PATH = "/home/mango/ocr_test/FAST/json_merged"

def analyze_complete_mapping_rules(json_path, base_path, dataset_name, sample_count=None):
    """ì™„ì „í•œ ë§¤í•‘ ê·œì¹™ ë¶„ì„ ë° ìµœì í™” í•¨ìˆ˜ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)"""
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {dataset_name} ì™„ì „ ë§¤í•‘ ê·œì¹™ ë¶„ì„ (ìˆ˜ì •ëœ ë²„ì „)")
    print(f"{'='*60}")
    
    # ê²½ë¡œ ê²€ì¦ ë° ìˆ˜ì •
    corrected_base_path = fix_base_path(base_path, dataset_name)
    print(f"ğŸ“‚ ìˆ˜ì •ëœ ë² ì´ìŠ¤ ê²½ë¡œ: {corrected_base_path}")
    
    # 1. JSON êµ¬ì¡° ë¶„ì„
    json_patterns = analyze_json_structure_enhanced(json_path, dataset_name, sample_count)
    
    # 2. ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ (ì œí•œ ì—†ì´)
    file_patterns = analyze_directory_structure_enhanced(corrected_base_path, dataset_name)
    
    # 3. ì‹¤ì œ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„ (ìƒˆë¡œ ì¶”ê°€)
    actual_filename_patterns = analyze_actual_filenames(corrected_base_path, dataset_name)
    
    # 4. ë§¤í•‘ ê·œì¹™ ìƒì„± (ê°œì„ ëœ ë²„ì „)
    mapping_rules = create_mapping_rules_enhanced(json_patterns, file_patterns, actual_filename_patterns, dataset_name)
    
    # 5. ë§¤í•‘ ê·œì¹™ í…ŒìŠ¤íŠ¸
    test_mapping_accuracy_enhanced(mapping_rules, json_patterns, file_patterns, dataset_name)
    
    # 6. ìµœì í™” í•¨ìˆ˜ ì½”ë“œ ìƒì„±
    generate_optimized_lookup_function(mapping_rules, dataset_name)
    
    return mapping_rules

def fix_base_path(base_path, dataset_name):
    """ë°ì´í„°ì…‹ ì´ë¦„ì— ë”°ë¼ ì˜¬ë°”ë¥¸ ë² ì´ìŠ¤ ê²½ë¡œ ì„¤ì •"""
    if "ì†ê¸€ì”¨" in dataset_name:
        if "Train" in dataset_name:
            return f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°/1.Training/ì›ì²œë°ì´í„°"
        else:
            return f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°/2.Validation/ì›ì²œë°ì´í„°"
    elif "OCRê³µê³µ" in dataset_name:
        if "Train" in dataset_name:
            return f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/01.ì›ì²œë°ì´í„°"
        else:
            return f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/01.ì›ì²œë°ì´í„°"
    elif "ê¸ˆìœµë¬¼ë¥˜" in dataset_name:
        if "Train" in dataset_name:
            return f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/01.ì›ì²œë°ì´í„°"
        else:
            return f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/01.ì›ì²œë°ì´í„°"
    
    return base_path

def analyze_actual_filenames(base_path, dataset_name):
    """ì‹¤ì œ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„ (ì œí•œ ì—†ì´ ì „ì²´ ìŠ¤ìº”)"""
    print(f"\nğŸ” {dataset_name} ì‹¤ì œ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„:")
    
    if not os.path.exists(base_path):
        print(f"âŒ ê²½ë¡œ ì—†ìŒ: {base_path}")
        return {}
    
    filename_patterns = defaultdict(list)
    directory_counts = defaultdict(int)
    sample_filenames = []
    total_files = 0
    
    print(f"  ğŸ”„ ì „ì²´ ë””ë ‰í† ë¦¬ ë¬´ì œí•œ ìŠ¤ìº” ì¤‘...")
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                relative_dir = os.path.relpath(root, base_path)
                directory_counts[relative_dir] += 1
                
                # íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„
                if total_files < 200:  # ìƒ˜í”Œ ìˆ˜ì§‘
                    sample_filenames.append((file, relative_dir))
                
                # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
                category = extract_category_from_filename(file, dataset_name)
                if category:
                    filename_patterns[category].append((file, relative_dir))
                
                total_files += 1
    
    print(f"  âœ… ì‹¤ì œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ: {total_files:,}ê°œ íŒŒì¼")
    print(f"    ğŸ“Š ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ìˆ˜:")
    
    # ìƒìœ„ 10ê°œ ë””ë ‰í† ë¦¬ ì¶œë ¥
    top_dirs = sorted(directory_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    for dir_path, count in top_dirs:
        print(f"      ğŸ“‚ {dir_path}: {count:,}ê°œ")
    
    # ì‹¤ì œ íŒŒì¼ëª… ìƒ˜í”Œ ì¶œë ¥
    print(f"    ğŸ“ ì‹¤ì œ íŒŒì¼ëª… ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
    for i, (filename, dir_path) in enumerate(sample_filenames[:10]):
        print(f"      {i+1}. {filename} (ğŸ“‚ {dir_path})")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ íŒŒì¼ ì˜ˆì‹œ
    print(f"    ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ íŒŒì¼ ì˜ˆì‹œ:")
    for category, files in filename_patterns.items():
        if files:
            example_file, example_dir = files[0]
            print(f"      {category}: {example_file} (ğŸ“‚ {example_dir})")
    
    return {
        'total_files': total_files,
        'directory_counts': dict(directory_counts),
        'filename_patterns': dict(filename_patterns),
        'sample_filenames': sample_filenames[:100]  # ìƒìœ„ 100ê°œë§Œ ì €ì¥
    }

def analyze_json_structure_enhanced(json_path, dataset_name, sample_count=None):
    """í–¥ìƒëœ JSON êµ¬ì¡° ë¶„ì„"""
    print(f"\nğŸ“„ {dataset_name} JSON êµ¬ì¡° ì •ë°€ ë¶„ì„:")
    
    if not os.path.exists(json_path):
        print(f"âŒ JSON íŒŒì¼ ì—†ìŒ: {json_path}")
        return {}
    
    data = None
    file_handle = None
    
    try:
        file_size_gb = os.path.getsize(json_path) / (1024**3)
        print(f"  íŒŒì¼ í¬ê¸°: {file_size_gb:.2f} GB")
        
        # íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ë¡œë” ì„ íƒ
        if file_size_gb > 1.0:
            print("  ğŸ”„ bigjsonìœ¼ë¡œ ë¡œë“œ...")
            file_handle = open(json_path, 'rb')
            data = bigjson.load(file_handle)
        else:
            print("  ğŸ”„ orjsonìœ¼ë¡œ ë¡œë“œ...")
            with open(json_path, 'rb') as f:
                data = orjson.loads(f.read())
        
        images = data.get('images', [])
        annotations = data.get('annotations', [])
        
        print(f"  ğŸ“Š ë¡œë“œëœ ë°ì´í„°: ì´ë¯¸ì§€ {len(images) if isinstance(images, list) else 'bigjson Array'}, "
              f"ì–´ë…¸í…Œì´ì…˜ {len(annotations) if isinstance(annotations, list) else 'bigjson Array'}")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (ë” ë„“ì€ ë²”ìœ„ì—ì„œ)
        patterns = {
            'file_names': [],
            'sub_datasets': [],
            'categories': defaultdict(list),
            'path_patterns': defaultdict(list),
            'total_samples': 0
        }
        
        # ì „ì²´ ë°ì´í„° ë˜ëŠ” ìƒ˜í”Œë§
        if isinstance(images, list):
            total_images = len(images)
            if sample_count is None:
                # ì „ì²´ ì²˜ë¦¬
                sample_indices = range(total_images)
                print(f"  ğŸ”„ ì „ì²´ {total_images}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
            elif sample_count < total_images:
                # ì „ì²´ êµ¬ê°„ì—ì„œ ê· ë“± ìƒ˜í”Œë§
                step = max(1, total_images // sample_count)
                sample_indices = [i * step for i in range(sample_count)]
                print(f"  ğŸ”„ {len(sample_indices)}ê°œ ìƒ˜í”Œ ë¶„ì„ ì¤‘ (ë„“ì€ ë²”ìœ„)...")
            else:
                sample_indices = range(total_images)
                print(f"  ğŸ”„ ì „ì²´ {total_images}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
        else:
            # bigjson Arrayì˜ ê²½ìš°
            if sample_count is None:
                print(f"  ğŸ”„ bigjson Array ì „ì²´ ë¶„ì„ ì¤‘ (ìˆœì°¨ ì²˜ë¦¬)...")
                sample_indices = None  # íŠ¹ë³„ ì²˜ë¦¬
            else:
                # bigjson Arrayì˜ ê²½ìš° ë” ë„“ì€ ë²”ìœ„ì—ì„œ ìƒ˜í”Œë§
                sample_indices = [i * 1000 for i in range(sample_count)]
                print(f"  ğŸ”„ {len(sample_indices)}ê°œ ìƒ˜í”Œ ë¶„ì„ ì¤‘ (ë„“ì€ ë²”ìœ„)...")
        
        
        if sample_indices is None:
            # bigjson Array ì „ì²´ ì²˜ë¦¬ (ìˆœì°¨)
            i = 0
            while True:
                try:
                    img = images[i]
                    if img is None:
                        break
                    
                    # ë°ì´í„°ì…‹ë³„ íŒŒì¼ëª… í•„ë“œ
                    if "ê³µê³µí–‰ì •" in dataset_name:
                        file_name = img.get('image.file.name', '')
                    else:
                        file_name = img.get('file_name', '')
                    
                    sub_dataset = img.get('sub_dataset', '')
                    original_path = img.get('original_json_path', '')
                    
                    if file_name:
                        patterns['file_names'].append(file_name)
                        
                        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                        category = extract_category_from_filename(file_name, dataset_name)
                        if category:
                            patterns['categories'][category].append(file_name)
                        
                        # ê²½ë¡œ íŒ¨í„´ ì¶”ì¶œ
                        if original_path:
                            path_pattern = extract_path_pattern(original_path)
                            patterns['path_patterns'][path_pattern].append(file_name)
                    
                    if sub_dataset:
                        patterns['sub_datasets'].append(sub_dataset)
                    
                    patterns['total_samples'] += 1
                    
                    # ì§„í–‰ìƒí™© ì¶œë ¥ (10000ê°œë§ˆë‹¤)
                    if i % 10000 == 0:
                        print(f"    ğŸ“Š JSON ë¶„ì„ ì§„í–‰: {patterns['total_samples']:,}ê°œ")
                    
                    i += 1
                    
                except (IndexError, TypeError) as e:
                    break
        else:
            # ì¸ë±ìŠ¤ ê¸°ë°˜ ì²˜ë¦¬
            for i in sample_indices:
                try:
                    img = images[i]
                    
                    # ë°ì´í„°ì…‹ë³„ íŒŒì¼ëª… í•„ë“œ
                    if "ê³µê³µí–‰ì •" in dataset_name:
                        file_name = img.get('image.file.name', '')
                    else:
                        file_name = img.get('file_name', '')
                    
                    sub_dataset = img.get('sub_dataset', '')
                    original_path = img.get('original_json_path', '')
                    
                    if file_name:
                        patterns['file_names'].append(file_name)
                        
                        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                        category = extract_category_from_filename(file_name, dataset_name)
                        if category:
                            patterns['categories'][category].append(file_name)
                        
                        # ê²½ë¡œ íŒ¨í„´ ì¶”ì¶œ
                        if original_path:
                            path_pattern = extract_path_pattern(original_path)
                            patterns['path_patterns'][path_pattern].append(file_name)
                    
                    if sub_dataset:
                        patterns['sub_datasets'].append(sub_dataset)
                    
                    patterns['total_samples'] += 1
                    
                except (IndexError, TypeError) as e:
                    break
        
        print(f"  âœ… JSON ë¶„ì„ ì™„ë£Œ: {patterns['total_samples']}ê°œ ìƒ˜í”Œ")
        print(f"    ğŸ“‹ ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬: {list(patterns['categories'].keys())}")
        print(f"    ğŸ“‚ ê²½ë¡œ íŒ¨í„´: {len(patterns['path_patterns'])}ê°œ")
        
        # JSON íŒŒì¼ëª… ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"    ğŸ“ JSON íŒŒì¼ëª… ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):")
        for i, fname in enumerate(patterns['file_names'][:10]):
            print(f"      {i+1}. {fname}")
        
        return patterns
        
    except Exception as e:
        print(f"  âŒ JSON ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}
    
    finally:
        if file_handle:
            try:
                file_handle.close()
            except:
                pass

def analyze_directory_structure_enhanced(base_path, dataset_name):
    """í–¥ìƒëœ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„ (ì œí•œ ì—†ìŒ)"""
    print(f"\nğŸ“ {dataset_name} ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ë°€ ë¶„ì„ (ë¬´ì œí•œ):")
    
    if not os.path.exists(base_path):
        print(f"âŒ ê²½ë¡œ ì—†ìŒ: {base_path}")
        return {}
    
    # ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ íŒ¨í„´ ìˆ˜ì§‘
    dir_patterns = {}
    category_locations = defaultdict(list)
    filename_to_path = {}
    path_templates = []
    
    print(f"  ğŸ”„ ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¬´ì œí•œ ìŠ¤ìº”...")
    file_count = 0
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                full_path = os.path.join(root, file)
                relative_path = os.path.relpath(full_path, base_path)
                
                # íŒŒì¼ëª… â†’ ê²½ë¡œ ë§¤í•‘
                filename_to_path[file] = full_path
                
                # í™•ì¥ì ì—†ëŠ” íŒŒì¼ëª…ë„ ë§¤í•‘
                name_without_ext = os.path.splitext(file)[0]
                filename_to_path[name_without_ext] = full_path
                
                # ì¹´í…Œê³ ë¦¬ë³„ ìœ„ì¹˜ ë§¤í•‘
                category = extract_category_from_filename(file, dataset_name)
                if category:
                    dir_path = os.path.dirname(relative_path)
                    category_locations[category].append(dir_path)
                
                # ê²½ë¡œ í…œí”Œë¦¿ ìƒì„±
                path_template = create_path_template(relative_path, file, dataset_name)
                if path_template not in path_templates:
                    path_templates.append(path_template)
                
                file_count += 1
                
                # ì§„í–‰ìƒí™© ì¶œë ¥ (10000ê°œë§ˆë‹¤)
                if file_count % 10000 == 0:
                    print(f"    ğŸ“Š ìŠ¤ìº” ì§„í–‰: {file_count:,}ê°œ")
    
    print(f"  âœ… ë””ë ‰í† ë¦¬ ë¶„ì„ ì™„ë£Œ: {file_count:,}ê°œ íŒŒì¼")
    print(f"    ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ìœ„ì¹˜:")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ ê²½ë¡œ ì¶œë ¥
    for category, locations in category_locations.items():
        unique_locations = list(set(locations))
        print(f"      {category}: {len(unique_locations)}ê°œ ìœ„ì¹˜")
        for loc in unique_locations[:3]:  # ìƒìœ„ 3ê°œë§Œ
            print(f"        ğŸ“‚ {loc}")
    
    return {
        'filename_to_path': filename_to_path,
        'category_locations': dict(category_locations),
        'path_templates': path_templates,
        'total_files': file_count
    }

def extract_category_from_filename(filename, dataset_name):
    """íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (í™•ì¥ëœ ë²„ì „)"""
    if "ì†ê¸€ì”¨" in dataset_name:
        for cat in ["4TO", "4PO", "4PR", "4TR"]:
            if f"_{cat}_" in filename:
                return cat
    elif "OCRê³µê³µ" in dataset_name:
        for cat in ["AF", "CST", "CT", "DI", "EN", "EV", "WF"]:
            if f"{cat}_" in filename or filename.startswith(cat):
                return cat
    elif "ê¸ˆìœµë¬¼ë¥˜" in dataset_name:
        for cat in ["BL", "PL", "NV"]:
            if f"_{cat}_" in filename:
                return cat
        if "_F_" in filename:  # ê¸ˆìœµ
            return "F"
    elif "ê³µê³µí–‰ì •" in dataset_name:
        # ê³µê³µí–‰ì •ì€ ì¹´í…Œê³ ë¦¬ê°€ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì— ìˆìŒ
        return "ADMIN"
    return None

def extract_path_pattern(original_path):
    """original_json_pathì—ì„œ íŒ¨í„´ ì¶”ì¶œ"""
    if not original_path:
        return "unknown"
    
    # ê²½ë¡œì—ì„œ íŒ¨í„´ ì¶”ì¶œ
    parts = original_path.split('/')
    
    # ì¤‘ìš”í•œ ë¶€ë¶„ë“¤ ì¶”ì¶œ
    patterns = []
    for part in parts:
        if any(keyword in part.lower() for keyword in ['training', 'validation', 'train', 'valid']):
            patterns.append('SPLIT')
        elif any(keyword in part.lower() for keyword in ['ì›ì²œ', 'source', 'ë°ì´í„°']):
            patterns.append('SOURCE')
        elif any(keyword in part.lower() for keyword in ['ë¼ë²¨', 'label']):
            patterns.append('LABEL')
    
    return '_'.join(patterns) if patterns else 'unknown'

def create_path_template(relative_path, filename, dataset_name):
    """ìƒëŒ€ ê²½ë¡œë¥¼ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜"""
    # íŒŒì¼ëª…ì„ {FILENAME}ìœ¼ë¡œ ëŒ€ì²´
    template = relative_path.replace(filename, "{FILENAME}")
    
    # ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ëŒ€ì²´
    category = extract_category_from_filename(filename, dataset_name)
    if category:
        template = template.replace(category, "{CATEGORY}")
    
    # ê³µí†µ íŒ¨í„´ë“¤ ëŒ€ì²´
    template = re.sub(r'\d{4,}', '{NUMBER}', template)  # 4ìë¦¬ ì´ìƒ ìˆ«ì
    template = re.sub(r'TS\d+', 'TS{N}', template)  # TS1, TS2 ë“±
    template = re.sub(r'VS\d*', 'VS{N}', template)  # VS, VS1 ë“±
    
    return template

def create_mapping_rules_enhanced(json_patterns, file_patterns, actual_filename_patterns, dataset_name):
    """í–¥ìƒëœ ë§¤í•‘ ê·œì¹™ ìƒì„±"""
    print(f"\nğŸ¯ {dataset_name} í–¥ìƒëœ ë§¤í•‘ ê·œì¹™ ìƒì„±:")
    
    rules = {
        'dataset_name': dataset_name,
        'direct_lookup': {},  # filename â†’ full_path
        'category_rules': {},  # category â†’ path_pattern
        'fallback_patterns': [],  # ìš°ì„ ìˆœìœ„ë³„ ê²€ìƒ‰ íŒ¨í„´
        'optimization_code': ""
    }
    
    if not json_patterns or not file_patterns:
        print("  âŒ íŒ¨í„´ ë°ì´í„° ë¶€ì¡±")
        return rules
    
    # ì‹¤ì œ íŒŒì¼ëª…ê³¼ JSON íŒŒì¼ëª… ë¹„êµ ë¶„ì„
    print(f"  ğŸ” ì‹¤ì œ íŒŒì¼ëª… vs JSON íŒŒì¼ëª… ë§¤í•‘ ë¶„ì„:")
    filename_to_path = file_patterns.get('filename_to_path', {})
    json_filenames = json_patterns['file_names']  # ì „ì²´ í…ŒìŠ¤íŠ¸
    
    successful_mappings = 0
    mapping_examples = []
    
    for json_filename in json_filenames:
        found_path = None
        
        # 1. ì§ì ‘ ë§¤í•‘ ì‹œë„
        if json_filename in filename_to_path:
            found_path = filename_to_path[json_filename]
            rules['direct_lookup'][json_filename] = found_path
            successful_mappings += 1
            mapping_examples.append((json_filename, found_path, "ì§ì ‘"))
        else:
            # 2. í™•ì¥ì ì¶”ê°€ ì‹œë„
            for ext in ['.png', '.jpg', '.jpeg']:
                candidate = f"{json_filename}{ext}"
                if candidate in filename_to_path:
                    found_path = filename_to_path[candidate]
                    rules['direct_lookup'][json_filename] = found_path
                    successful_mappings += 1
                    mapping_examples.append((json_filename, found_path, "í™•ì¥ì"))
                    break
            
            # 3. ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            if not found_path:
                json_base = json_filename.replace('IMG_OCR_53_', '').replace('IMG_OCR_', '')
                for actual_filename, actual_path in filename_to_path.items():
                    if json_base in actual_filename or actual_filename in json_base:
                        found_path = actual_path
                        rules['direct_lookup'][json_filename] = found_path
                        successful_mappings += 1
                        mapping_examples.append((json_filename, found_path, "ë¶€ë¶„ë§¤ì¹­"))
                        break
    
    direct_success_rate = successful_mappings / len(json_filenames) * 100
    print(f"    ğŸ“Š ë§¤í•‘ ì„±ê³µë¥ : {direct_success_rate:.1f}% ({successful_mappings}/{len(json_filenames)})")
    
    # ì„±ê³µ ì‚¬ë¡€ ì¶œë ¥
    if mapping_examples:
        print(f"    âœ… ì„±ê³µ ë§¤í•‘ ì‚¬ë¡€ (ì²˜ìŒ 5ê°œ):")
        for i, (json_name, file_path, method) in enumerate(mapping_examples[:5]):
            print(f"      {i+1}. [{method}] {json_name} â†’ {os.path.basename(file_path)}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê·œì¹™ ìƒì„±
    category_locations = file_patterns.get('category_locations', {})
    for category, locations in category_locations.items():
        # ê°€ì¥ ë¹ˆë²ˆí•œ ìœ„ì¹˜ë¥¼ ëŒ€í‘œ ê²½ë¡œë¡œ ì„ íƒ
        location_counts = defaultdict(int)
        for loc in locations:
            location_counts[loc] += 1
        
        if location_counts:
            most_common_location = max(location_counts, key=location_counts.get)
            rules['category_rules'][category] = most_common_location
            print(f"    {category} â†’ {most_common_location}")
    
    # í´ë°± íŒ¨í„´ ìƒì„±
    path_templates = file_patterns.get('path_templates', [])
    rules['fallback_patterns'] = sorted(set(path_templates), key=lambda x: x.count('{'))
    
    print(f"  ğŸ“‹ ìƒì„±ëœ ê·œì¹™:")
    print(f"    - ì§ì ‘ ë§¤í•‘: {len(rules['direct_lookup'])}ê°œ")
    print(f"    - ì¹´í…Œê³ ë¦¬ ê·œì¹™: {len(rules['category_rules'])}ê°œ")
    print(f"    - í´ë°± íŒ¨í„´: {len(rules['fallback_patterns'])}ê°œ")
    
    return rules

def test_mapping_accuracy_enhanced(rules, json_patterns, file_patterns, dataset_name):
    """í–¥ìƒëœ ë§¤í•‘ ê·œì¹™ì˜ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª {dataset_name} í–¥ìƒëœ ë§¤í•‘ ê·œì¹™ ì •í™•ë„ í…ŒìŠ¤íŠ¸:")
    
    if not json_patterns or not rules:
        print("  âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±")
        return
    
    test_files = json_patterns['file_names']  # ì „ì²´ í…ŒìŠ¤íŠ¸
    success_count = 0
    failure_cases = []
    success_cases = []
    
    for test_file in test_files:
        found_path = None
        
        # 1. ì§ì ‘ ë£©ì—… ì‹œë„
        if test_file in rules['direct_lookup']:
            found_path = rules['direct_lookup'][test_file]
        else:
            # 2. ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
            category = extract_category_from_filename(test_file, dataset_name)
            if category and category in rules['category_rules']:
                category_dir = rules['category_rules'][category]
                # ë² ì´ìŠ¤ ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                base_path = list(file_patterns.get('filename_to_path', {}).values())[0] if file_patterns.get('filename_to_path') else ""
                if base_path:
                    base_path = os.path.dirname(base_path)
                    for ext in ['.png', '.jpg', '.jpeg']:
                        candidate_path = os.path.join(base_path, category_dir, f"{test_file}{ext}")
                        if os.path.exists(candidate_path):
                            found_path = candidate_path
                            break
        
        if found_path and os.path.exists(found_path):
                success_count += 1
                success_cases.append((test_file, found_path))
        else:
            failure_cases.append(test_file)
    
    accuracy = success_count / len(test_files) * 100
    print(f"  ğŸ“Š í–¥ìƒëœ ë§¤í•‘ ì •í™•ë„: {accuracy:.1f}% ({success_count}/{len(test_files)})")
    
    if success_cases:
        print(f"  âœ… ì„±ê³µ ì‚¬ë¡€ (ì²˜ìŒ 3ê°œ):")
        for i, (test_file, found_path) in enumerate(success_cases[:3]):
            print(f"    {i+1}. {test_file} â†’ {os.path.basename(found_path)}")
    
    if failure_cases:
        print(f"  âŒ ì‹¤íŒ¨ ì‚¬ë¡€ (ì²˜ìŒ 5ê°œ):")
        for case in failure_cases[:5]:
            print(f"    - {case}")

def dataset_name_to_english(dataset_name):
    """ë°ì´í„°ì…‹ ì´ë¦„ì„ ì˜ì–´ë¡œ ë³€í™˜"""
    name_mapping = {
        'ì†ê¸€ì”¨_Train': 'handwriting_train',
        'ì†ê¸€ì”¨_Valid': 'handwriting_valid', 
        'OCRê³µê³µ_Train': 'ocr_public_train',
        'OCRê³µê³µ_Valid': 'ocr_public_valid',
        'ê¸ˆìœµë¬¼ë¥˜_Train': 'finance_logistics_train',
        'ê¸ˆìœµë¬¼ë¥˜_Valid': 'finance_logistics_valid',
        'ê³µê³µí–‰ì •_Train': 'public_admin_train',
        'ê³µê³µí–‰ì •_Train_Partly': 'public_admin_train_partly',
        'ê³µê³µí–‰ì •_Valid': 'public_admin_valid',
        'TextInWild': 'text_in_wild'
    }
    return name_mapping.get(dataset_name, dataset_name.lower().replace(' ', '_'))

def generate_optimized_lookup_function(rules, dataset_name):
    """ìµœì í™”ëœ ì¡°íšŒ í•¨ìˆ˜ ì½”ë“œ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)"""
    print(f"\nğŸš€ {dataset_name} ìµœì í™” í•¨ìˆ˜ ì½”ë“œ ìƒì„±:")
    
    english_name = dataset_name_to_english(dataset_name)
    function_name = f"lookup_{english_name}"
    
    # í•¨ìˆ˜ ì½”ë“œ ìƒì„±
    code = f'''
def {function_name}(filename, base_path):
    """
    {dataset_name} íŒŒì¼ëª…ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜í•˜ëŠ” ìµœì í™”ëœ í•¨ìˆ˜
    Generated by ftp_tree_viewer.py
    """
    # 1. ì§ì ‘ ë§¤í•‘ (ê°€ì¥ ë¹ ë¦„)
    direct_mappings = {{
'''
    
    # ì§ì ‘ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ì¶”ê°€ (ì „ì²´)
    for filename, path in rules['direct_lookup'].items():
        code += f'        "{filename}": "{path}",\n'
    
    code += f'''    }}
    
    if filename in direct_mappings:
        return direct_mappings[filename]
    
    # 2. ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë§¤í•‘
    category_rules = {{
'''
    
    # ì¹´í…Œê³ ë¦¬ ê·œì¹™ ì¶”ê°€
    for category, location in rules['category_rules'].items():
        code += f'        "{category}": "{location}",\n'
    
    code += f'''    }}
    
    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
    category = None'''
    
    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ë¡œì§ ì¶”ê°€
    if "ì†ê¸€ì”¨" in dataset_name:
        code += '''
    for cat in ["4TO", "4PO", "4PR", "4TR"]:
        if f"_{cat}_" in filename:
            category = cat
            break'''
    elif "OCRê³µê³µ" in dataset_name:
        code += '''
    for cat in ["AF", "CST", "CT", "DI", "EN", "EV", "WF"]:
        if f"{cat}_" in filename or filename.startswith(cat):
            category = cat
            break'''
    elif "ê¸ˆìœµë¬¼ë¥˜" in dataset_name:
        code += '''
    for cat in ["BL", "PL", "NV"]:
        if f"_{cat}_" in filename:
            category = cat
            break
    if "_F_" in filename:
        category = "F"'''
    
    code += '''
    
    # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²½ë¡œ ìƒì„±
    if category and category in category_rules:
        category_dir = category_rules[category]
        for ext in ['.png', '.jpg', '.jpeg']:
            candidate_path = os.path.join(base_path, category_dir, f"{filename}{ext}")
            if os.path.exists(candidate_path):
                return candidate_path
    
    # 3. í´ë°±: ì „ì²´ ìŠ¤ìº” (ëŠë¦¼)
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                if filename in file or file.startswith(filename):
                    return os.path.join(root, file)
    
    return None
'''
    
    rules['optimization_code'] = code
    
    print(f"  âœ… í•¨ìˆ˜ ì½”ë“œ ìƒì„± ì™„ë£Œ: {function_name}")
    
    # íŒŒì¼ë¡œ ì €ì¥ (ë””ë ‰í† ë¦¬ ìƒì„±)
    output_dir = "FAST"
    os.makedirs(output_dir, exist_ok=True)
    output_file = f"{output_dir}/optimized_lookup_{english_name}.py"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n")
            f.write(f'"""{dataset_name} ìµœì í™”ëœ íŒŒì¼ ì¡°íšŒ í•¨ìˆ˜"""\n\n')
            f.write("import os\n\n")
            f.write(code)
        
        print(f"  ğŸ’¾ ì½”ë“œ ì €ì¥ ì„±ê³µ: {output_file}")
    except Exception as e:
        print(f"  âŒ ì½”ë“œ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ - ëª¨ë“  ë°ì´í„°ì…‹ ë¶„ì„"""
    print("ğŸš€ OCR ë°ì´í„°ì…‹ ì™„ì „ ë§¤í•‘ ê·œì¹™ ë¶„ì„ ë° ìµœì í™” í•¨ìˆ˜ ìƒì„± (ì „ì²´ ë°ì´í„°ì…‹)")
    print("=" * 60)
    
    # FTP ë§ˆìš´íŠ¸ í™•ì¸
    if not os.path.exists(FTP_BASE_PATH):
        print("âŒ FTP ë§ˆìš´íŠ¸ ì—†ìŒ")
        return
    
    print("âœ… FTP ë§ˆìš´íŠ¸ í™•ì¸ ì™„ë£Œ")
    
    # ë¶„ì„í•  ëª¨ë“  ë°ì´í„°ì…‹ (train/valid í¬í•¨)
    datasets = [
        # ì†ê¸€ì”¨ OCR ë°ì´í„°
        {
            'name': 'ì†ê¸€ì”¨_Train',
            'json_path': f"{MERGED_JSON_PATH}/handwriting_train_merged.json",
            'base_path': f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°/1.Training/ì›ì²œë°ì´í„°"
        },
        {
            'name': 'ì†ê¸€ì”¨_Valid',
            'json_path': f"{MERGED_JSON_PATH}/handwriting_valid_merged.json",
            'base_path': f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°/2.Validation/ì›ì²œë°ì´í„°"
        },
        
        # OCR ê³µê³µ ë°ì´í„°
        {
            'name': 'OCRê³µê³µ_Train',
            'json_path': f"{MERGED_JSON_PATH}/ocr_public_train_merged.json",
            'base_path': f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/01.ì›ì²œë°ì´í„°"
        },
        {
            'name': 'OCRê³µê³µ_Valid',
            'json_path': f"{MERGED_JSON_PATH}/ocr_public_valid_merged.json",
            'base_path': f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/01.ì›ì²œë°ì´í„°"
        },
        
        # ê¸ˆìœµë¬¼ë¥˜ ë°ì´í„°
        {
            'name': 'ê¸ˆìœµë¬¼ë¥˜_Train',
            'json_path': f"{MERGED_JSON_PATH}/finance_logistics_train_merged.json",
            'base_path': f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/01.ì›ì²œë°ì´í„°"
        },
        {
            'name': 'ê¸ˆìœµë¬¼ë¥˜_Valid',
            'json_path': f"{MERGED_JSON_PATH}/finance_logistics_valid_merged.json",
            'base_path': f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/01.ì›ì²œë°ì´í„°"
        },
        
        # ê³µê³µí–‰ì •ë¬¸ì„œ OCR
        {
            'name': 'ê³µê³µí–‰ì •_Train',
            'json_path': f"{MERGED_JSON_PATH}/public_admin_train_merged.json",
            'base_path': f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR/Training"
        },
        {
            'name': 'ê³µê³µí–‰ì •_Train_Partly',
            'json_path': f"{MERGED_JSON_PATH}/public_admin_train_partly_merged.json",
            'base_path': f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR/Training"
        },
        {
            'name': 'ê³µê³µí–‰ì •_Valid',
            'json_path': f"{MERGED_JSON_PATH}/public_admin_valid_merged.json",
            'base_path': f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR/Validation"
        },
        
        # Text in the Wild (í•œêµ­ì–´ê¸€ìì²´)
        {
            'name': 'TextInWild',
            'json_path': f"{MERGED_JSON_PATH}/textinthewild_data_info.json",
            'base_path': f"{FTP_BASE_PATH}/13.í•œêµ­ì–´ê¸€ìì²´/04. Text in the wild_230209_add"
        }
    ]
    
    # ì „ì²´ ë§¤í•‘ ê·œì¹™
    all_mapping_rules = {}
    successful_datasets = []
    failed_datasets = []
    
    # ê° ë°ì´í„°ì…‹ ë¶„ì„
    for i, dataset in enumerate(datasets):
        print(f"\n{'='*60}")
        print(f"ğŸ¯ ì§„í–‰ìƒí™©: {i+1}/{len(datasets)} - {dataset['name']}")
        print(f"{'='*60}")
        
        # ì´ë¯¸ ìƒì„±ëœ lookup íŒŒì¼ í™•ì¸
        english_name = dataset_name_to_english(dataset['name'])
        lookup_file = f"FAST/optimized_lookup_{english_name}.py"
        
        if os.path.exists(lookup_file):
            print(f"â­ï¸ {dataset['name']}: ì´ë¯¸ ìƒì„±ëœ lookup íŒŒì¼ ìŠ¤í‚µ - {lookup_file}")
            successful_datasets.append(dataset['name'])
            continue
        
        if os.path.exists(dataset['json_path']):
            try:
                mapping_rules = analyze_complete_mapping_rules(
                    dataset['json_path'],
                    dataset['base_path'],
                    dataset['name']
                )
                all_mapping_rules[dataset['name']] = mapping_rules
                
                # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                if mapping_rules and mapping_rules.get('direct_lookup'):
                    successful_datasets.append(dataset['name'])
                    print(f"âœ… {dataset['name']}: ë§¤í•‘ ì„±ê³µ!")
                else:
                    failed_datasets.append(dataset['name'])
                    print(f"âŒ {dataset['name']}: ë§¤í•‘ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"âŒ {dataset['name']} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                failed_datasets.append(dataset['name'])
        else:
            print(f"âš ï¸ {dataset['name']} JSON íŒŒì¼ ì—†ìŒ: {dataset['json_path']}")
            failed_datasets.append(dataset['name'])
    
    # ì¢…í•© ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*80}")
    print("ğŸ‰ ì „ì²´ ë°ì´í„°ì…‹ ë§¤í•‘ ê·œì¹™ ë¶„ì„ ì™„ë£Œ!")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
    print(f"   ğŸ“ˆ ì„±ê³µ: {len(successful_datasets)}ê°œ ë°ì´í„°ì…‹")
    print(f"   ğŸ“‰ ì‹¤íŒ¨: {len(failed_datasets)}ê°œ ë°ì´í„°ì…‹")
    print(f"   ğŸ“‹ ì „ì²´: {len(datasets)}ê°œ ë°ì´í„°ì…‹")
    
    if successful_datasets:
        print(f"\nâœ… ì„±ê³µí•œ ë°ì´í„°ì…‹:")
        for name in successful_datasets:
            rules = all_mapping_rules.get(name, {})
            direct_count = len(rules.get('direct_lookup', {}))
            category_count = len(rules.get('category_rules', {}))
            print(f"   ğŸ¯ {name}: ì§ì ‘ë§¤í•‘ {direct_count}ê°œ, ì¹´í…Œê³ ë¦¬ê·œì¹™ {category_count}ê°œ")
    
    if failed_datasets:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ë°ì´í„°ì…‹:")
        for name in failed_datasets:
            print(f"   ğŸ’¥ {name}")
    
    # ì „ì²´ ìµœì í™” í•¨ìˆ˜ í†µí•© ì½”ë“œ ìƒì„±
    generate_unified_optimization_code(all_mapping_rules)
    
    print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. âœ… ì „ì²´ ë°ì´í„°ì…‹ ë§¤í•‘ ê·œì¹™ ë¶„ì„ ì™„ë£Œ")
    print(f"   2. ğŸ”„ create_all_datasets_500_clean.pyì— ìµœì í™” í•¨ìˆ˜ í†µí•©")
    print(f"   3. ğŸš€ bigjson + ë³‘ë ¬ì²˜ë¦¬ ìµœì í™” ì ìš©")
    print(f"   4. ğŸ§ª ì‹¤ì œ LMDB ìƒì„± í…ŒìŠ¤íŠ¸")

def generate_unified_optimization_code(all_mapping_rules):
    """ëª¨ë“  ë°ì´í„°ì…‹ì˜ ìµœì í™” í•¨ìˆ˜ë¥¼ í†µí•©í•œ ì½”ë“œ ìƒì„±"""
    print(f"\nğŸš€ í†µí•© ìµœì í™” í•¨ìˆ˜ ì½”ë“œ ìƒì„±:")
    
    unified_code = '''#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ëª¨ë“  OCR ë°ì´í„°ì…‹ì˜ í†µí•© ìµœì í™” ì¡°íšŒ í•¨ìˆ˜
Generated by ftp_tree_viewer.py
"""

import os

class OCRDatasetOptimizedLookup:
    """ëª¨ë“  OCR ë°ì´í„°ì…‹ì— ëŒ€í•œ ìµœì í™”ëœ íŒŒì¼ ì¡°íšŒ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™” - ëª¨ë“  ë°ì´í„°ì…‹ì˜ ì§ì ‘ ë§¤í•‘ í…Œì´ë¸” ë¡œë“œ"""
        self.dataset_mappings = {
'''
    
    # ê° ë°ì´í„°ì…‹ë³„ ì§ì ‘ ë§¤í•‘ ì¶”ê°€
    for dataset_name, rules in all_mapping_rules.items():
        if rules and rules.get('direct_lookup'):
            unified_code += f'            "{dataset_name}": {{\n'
            
            # ì§ì ‘ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ì¶”ê°€ (ì „ì²´)
            direct_mappings = rules.get('direct_lookup', {})
            for filename, path in direct_mappings.items():
                unified_code += f'                "{filename}": "{path}",\n'
            
            unified_code += f'            }},\n'
    
    unified_code += '''        }
        
        self.category_rules = {
'''
    
    # ê° ë°ì´í„°ì…‹ë³„ ì¹´í…Œê³ ë¦¬ ê·œì¹™ ì¶”ê°€
    for dataset_name, rules in all_mapping_rules.items():
        if rules and rules.get('category_rules'):
            unified_code += f'            "{dataset_name}": {{\n'
            
            category_rules = rules.get('category_rules', {})
            for category, location in category_rules.items():
                unified_code += f'                "{category}": "{location}",\n'
            
            unified_code += f'            }},\n'
    
    unified_code += '''        }
    
    def lookup_file(self, filename, dataset_name, base_path):
        """í†µí•© íŒŒì¼ ì¡°íšŒ í•¨ìˆ˜"""
        # 1. ì§ì ‘ ë§¤í•‘ ì‹œë„
        dataset_mappings = self.dataset_mappings.get(dataset_name, {})
        if filename in dataset_mappings:
            return dataset_mappings[filename]
        
        # 2. í™•ì¥ì ì¶”ê°€ ì‹œë„
        for ext in ['.png', '.jpg', '.jpeg']:
            candidate = f"{filename}{ext}"
            if candidate in dataset_mappings:
                return dataset_mappings[candidate]
        
        # 3. ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì¡°íšŒ
        category = self.extract_category(filename, dataset_name)
        if category:
            dataset_category_rules = self.category_rules.get(dataset_name, {})
            if category in dataset_category_rules:
                category_dir = dataset_category_rules[category]
                for ext in ['.png', '.jpg', '.jpeg']:
                    candidate_path = os.path.join(base_path, category_dir, f"{filename}{ext}")
                    if os.path.exists(candidate_path):
                        return candidate_path
        
        # 4. í´ë°±: ë¶€ë¶„ ë§¤ì¹­
        filename_base = filename.replace('IMG_OCR_53_', '').replace('IMG_OCR_', '')
        for mapped_file, mapped_path in dataset_mappings.items():
            if filename_base in mapped_file or mapped_file in filename_base:
                return mapped_path
        
        # 5. ìµœí›„ì˜ ìˆ˜ë‹¨: ì „ì²´ ìŠ¤ìº”
        return self.fallback_scan(filename, base_path)
    
    def extract_category(self, filename, dataset_name):
        """íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        if "ì†ê¸€ì”¨" in dataset_name:
            for cat in ["4TO", "4PO", "4PR", "4TR"]:
                if f"_{cat}_" in filename:
                    return cat
        elif "OCRê³µê³µ" in dataset_name:
            for cat in ["AF", "CST", "CT", "DI", "EN", "EV", "WF"]:
                if f"{cat}_" in filename or filename.startswith(cat):
                    return cat
        elif "ê¸ˆìœµë¬¼ë¥˜" in dataset_name:
            for cat in ["BL", "PL", "NV"]:
                if f"_{cat}_" in filename:
                    return cat
            if "_F_" in filename:
                return "F"
        elif "ê³µê³µí–‰ì •" in dataset_name:
            return "ADMIN"
        return None
    
    def fallback_scan(self, filename, base_path):
        """í´ë°±: ì „ì²´ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
        for root, dirs, files in os.walk(base_path):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    if filename in file or file.startswith(filename):
                        return os.path.join(root, file)
        return None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ocr_lookup = OCRDatasetOptimizedLookup()

def optimized_file_lookup(filename, dataset_name, base_path):
    """ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•œ í†µí•© ìµœì í™” ì¡°íšŒ í•¨ìˆ˜"""
    return ocr_lookup.lookup_file(filename, dataset_name, base_path)
'''
    
    # íŒŒì¼ ì €ì¥
    output_dir = "FAST"
    os.makedirs(output_dir, exist_ok=True)
    output_file = f"{output_dir}/unified_ocr_lookup_optimizer.py"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(unified_code)
        
        print(f"  âœ… í†µí•© ìµœì í™” í´ë˜ìŠ¤ ìƒì„± ì™„ë£Œ")
        print(f"  ğŸ’¾ íŒŒì¼ ì €ì¥: {output_file}")
        print(f"  ğŸ“Š í¬í•¨ëœ ë°ì´í„°ì…‹: {len(all_mapping_rules)}ê°œ")
        
        # í†µê³„ ì¶œë ¥
        total_direct_mappings = sum(len(rules.get('direct_lookup', {})) for rules in all_mapping_rules.values())
        total_category_rules = sum(len(rules.get('category_rules', {})) for rules in all_mapping_rules.values())
        
        print(f"  ğŸ“ˆ ì „ì²´ ì§ì ‘ ë§¤í•‘: {total_direct_mappings:,}ê°œ")
        print(f"  ğŸ“ˆ ì „ì²´ ì¹´í…Œê³ ë¦¬ ê·œì¹™: {total_category_rules}ê°œ")
        
    except Exception as e:
        print(f"  âŒ í†µí•© ì½”ë“œ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == '__main__':
    main() 