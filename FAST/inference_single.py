#!/usr/bin/env python3
"""
FAST ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹¨ì¼ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ê²€ì¶œ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import argparse
import os
import sys
import cv2
import numpy as np
from PIL import Image
import mmcv
from mmcv import Config

# FAST ëª¨ë¸ ê´€ë ¨ imports
sys.path.append('.')
from dataset import build_data_loader
from models import build_model
from models.utils import fuse_module, rep_model_convert
from utils import ResultFormat, AverageMeter
from dataset.utils import get_img, scale_aligned_short
import torchvision.transforms as transforms
import json

def preprocess_image(image_path, short_size=640):
    """
    ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ FAST ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ì „ì²˜ë¦¬
    """
    # ì´ë¯¸ì§€ ë¡œë“œ
    img = get_img(image_path, read_type='cv2')
    original_img = img.copy()
    
    # í¬ê¸° ì¡°ì •
    img = scale_aligned_short(img, short_size=short_size)
    
    # PILë¡œ ë³€í™˜ í›„ ì •ê·œí™”
    img_pil = Image.fromarray(img)
    img_pil = img_pil.convert('RGB')
    
    # í…ì„œ ë³€í™˜ ë° ì •ê·œí™”
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    img_tensor = transform(img_pil).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    
    # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
    img_meta = {
        'filename': [os.path.basename(image_path)],
        'org_img_size': [original_img.shape[:2]],  # [H, W]
        'img_size': [img.shape[:2]]  # [H, W]
    }
    
    return img_tensor, img_meta, original_img

def load_model_and_checkpoint(config_path, checkpoint_path):
    """
    FAST ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œ
    """
    # ì„¤ì • ë¡œë“œ
    cfg = Config.fromfile(config_path)
    
    # ëª¨ë¸ ìƒì„±
    model = build_model(cfg.model)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if os.path.isfile(checkpoint_path):
        print(f"Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # state_dict ì¶”ì¶œ (ema ì‚¬ìš© ì‹œ ema, ì•„ë‹ˆë©´ state_dict)
        if 'ema' in checkpoint:
            state_dict = checkpoint['ema']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # í‚¤ì—ì„œ 'module.' ì œê±°
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("module.", "")
            new_state_dict[new_key] = value
        
        model.load_state_dict(new_state_dict)
        print("âœ… Checkpoint loaded successfully!")
    else:
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    # ëª¨ë¸ ìµœì í™”
    model = rep_model_convert(model)
    model = fuse_module(model)
    model.eval()
    
    return model, cfg, device

def inference_single_image(model, img_tensor, img_meta, cfg, device):
    """
    ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰
    """
    # ë°ì´í„°ë¥¼ GPUë¡œ
    img_tensor = img_tensor.to(device)
    
    # ì¶”ë¡ 
    with torch.no_grad():
        data = {
            'imgs': img_tensor,
            'img_metas': img_meta,
            'cfg': cfg
        }
        outputs = model(**data)
    
    return outputs

def visualize_results(img, results, output_path, min_score=0.5):
    """
    ê²€ì¶œ ê²°ê³¼ë¥¼ ì‹œê°í™”
    """
    img_vis = img.copy()
    
    for result in results['results']:
        bboxes = result['bboxes']
        scores = result['scores']
        
        for bbox, score in zip(bboxes, scores):
            if score > min_score:
                # bboxëŠ” [x1, y1, x2, y2, x3, y3, x4, y4] í˜•íƒœ
                bbox = np.array(bbox).reshape(-1, 2).astype(np.int32)
                
                # í´ë¦¬ê³¤ ê·¸ë¦¬ê¸°
                cv2.polylines(img_vis, [bbox], True, (0, 255, 0), 2)
                
                # ìŠ¤ì½”ì–´ í‘œì‹œ
                cv2.putText(img_vis, f'{score:.3f}', 
                           (int(bbox[0][0]), int(bbox[0][1]-5)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
    
    # ê²°ê³¼ ì €ì¥
    cv2.imwrite(output_path, img_vis)
    print(f"âœ… ì‹œê°í™” ê²°ê³¼ ì €ì¥: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='FAST ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ ')
    parser.add_argument('--config', default='config/fast/ic15/fast_sample_finetune_test.py',
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--checkpoint', default='checkpoint_7ep.pth',
                       help='ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--image', default='5350034-2011-0001-0019.jpg',
                       help='ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ')
    parser.add_argument('--output', default='output_detection.jpg',
                       help='ì¶œë ¥ ì´ë¯¸ì§€ ê²½ë¡œ')
    parser.add_argument('--min_score', type=float, default=0.5,
                       help='ìµœì†Œ ê²€ì¶œ ìŠ¤ì½”ì–´')
    parser.add_argument('--short_size', type=int, default=640,
                       help='ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸°')
    
    args = parser.parse_args()
    
    print("=== FAST í…ìŠ¤íŠ¸ ê²€ì¶œ ì¶”ë¡  ===")
    print(f"Config: {args.config}")
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Image: {args.image}")
    print(f"Output: {args.output}")
    
    # ëª¨ë¸ ë¡œë“œ
    print("\n1. ëª¨ë¸ ë¡œë”©...")
    model, cfg, device = load_model_and_checkpoint(args.config, args.checkpoint)
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    print("\n2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬...")
    img_tensor, img_meta, original_img = preprocess_image(args.image, args.short_size)
    print(f"Original image shape: {original_img.shape}")
    print(f"Processed image shape: {img_tensor.shape}")
    
    # ì¶”ë¡ 
    print("\n3. ì¶”ë¡  ìˆ˜í–‰...")
    results = inference_single_image(model, img_tensor, img_meta, cfg, device)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n4. ê²°ê³¼ ì²˜ë¦¬...")
    num_detections = len(results['results'][0]['bboxes'])
    print(f"ê²€ì¶œëœ í…ìŠ¤íŠ¸ ì˜ì—­ ìˆ˜: {num_detections}")
    
    # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    results_json = {
        'image': args.image,
        'detections': []
    }
    
    for bbox, score in zip(results['results'][0]['bboxes'], results['results'][0]['scores']):
        if score > args.min_score:
            results_json['detections'].append({
                'bbox': bbox.tolist() if hasattr(bbox, 'tolist') else bbox,
                'score': float(score)
            })
    
    with open('detection_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_json, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ê²€ì¶œ ê²°ê³¼ JSON ì €ì¥: detection_results.json")
    
    # ì‹œê°í™”
    print("\n5. ê²°ê³¼ ì‹œê°í™”...")
    visualize_results(original_img, results, args.output, args.min_score)
    
    print("\nğŸ‰ ì¶”ë¡  ì™„ë£Œ!")
    print(f"ì´ {len(results_json['detections'])}ê°œì˜ í…ìŠ¤íŠ¸ ì˜ì—­ì„ ê²€ì¶œí–ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main() 