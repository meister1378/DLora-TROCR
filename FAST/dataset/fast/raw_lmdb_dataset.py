#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Raw LMDB Dataset
ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ëŠ” ë°ì´í„°ì…‹ í´ë˜ìŠ¤
"""

import os
import sys
import pickle
import numpy as np
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
from torch.utils import data

try:
    import lmdb
except ImportError:
    print("âŒ LMDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install lmdb' ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

from dataset.utils import shrink
from dataset.utils import get_vocabulary
from dataset.utils import random_crop_padding_v2 as random_crop_padding
from dataset.utils import random_scale, random_horizontal_flip, random_rotate
from dataset.utils import scale_aligned_short


class RawLMDBDataset(data.Dataset):
    """
    ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬ ì—†ì´ ë¡œë“œí•˜ëŠ” LMDB ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    
    íŠ¹ì§•:
    - ì›ë³¸ ì´ë¯¸ì§€ ê·¸ëŒ€ë¡œ ë¡œë“œ
    - ì „ì²˜ë¦¬ëŠ” ëª¨ë¸ forward ì‹œì—ë§Œ ì ìš©
    - bbox ì¢Œí‘œë„ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€
    """
    
    def __init__(self, lmdb_path, split='train', max_word_num=200, read_type='cv2'):
        """
        Args:
            lmdb_path (str): LMDB ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            split (str): 'train' ë˜ëŠ” 'test'
            max_word_num (int): ìµœëŒ€ ë‹¨ì–´ ìˆ˜
            read_type (str): ì´ë¯¸ì§€ ì½ê¸° ë°©ì‹ ('cv2' ë˜ëŠ” 'pil')
        """
        self.lmdb_path = lmdb_path
        self.split = split
        self.max_word_num = max_word_num
        self.read_type = read_type
        
        # LMDB í™˜ê²½ ì—´ê¸°
        print(f"ğŸ—‚ï¸ ì›ë³¸ LMDB ë°ì´í„°ë² ì´ìŠ¤ ì—´ê¸°: {lmdb_path}")
        if not os.path.exists(lmdb_path):
            raise FileNotFoundError(f"LMDB ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lmdb_path}")
        
        self.env = lmdb.open(lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)
        
        # ë°ì´í„° í¬ê¸° í™•ì¸
        with self.env.begin(write=False) as txn:
            num_samples_key = 'num-samples'.encode()
            self.length = int(txn.get(num_samples_key).decode())
        
        print(f"ğŸ“Š ì›ë³¸ LMDB ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   - ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {self.length}")
        
        # ì–´íœ˜ ì‚¬ì „ ë¡œë“œ
        self.voc, self.char2id, self.id2char = get_vocabulary('LOWERCASE')
        self.max_word_len = 32
    
    def __len__(self):
        return self.length
    
    def __del__(self):
        if hasattr(self, 'env'):
            self.env.close()
    
    def get_raw_image_and_gt(self, index):
        """ì›ë³¸ ì´ë¯¸ì§€ì™€ GT ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ ì—†ì´ ê°€ì ¸ì˜¤ê¸°"""
        with self.env.begin(write=False) as txn:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img_key = f'image-{index:09d}'.encode()
            img_data = txn.get(img_key)
            if img_data is None:
                raise KeyError(f"ì´ë¯¸ì§€ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_key}")
            
            # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
            img_np = np.frombuffer(img_data, dtype=np.uint8)
            img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
            if img is None:
                raise ValueError(f"ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: index {index}")
            
            # BGR -> RGB ë³€í™˜
            img = img[:, :, [2, 1, 0]]
            
            # GT ë°ì´í„° ë¡œë“œ
            gt_key = f'gt-{index:09d}'.encode()
            gt_data = txn.get(gt_key)
            if gt_data is None:
                raise KeyError(f"GT í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gt_key}")
            
            # pickleë¡œ ì§ë ¬í™”ëœ GT ë°ì´í„° ë³µì›
            gt_info = pickle.loads(gt_data)
            
        return img, gt_info
    
    def __getitem__(self, index):
        """ì›ë³¸ ë°ì´í„° ë¡œë“œ (ì „ì²˜ë¦¬ ì—†ìŒ)"""
        img, gt_info = self.get_raw_image_and_gt(index)
        
        # GT ì •ë³´ ì¶”ì¶œ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
        bboxes = np.array(gt_info['bboxes'])  # ì •ê·œí™”ëœ ì¢Œí‘œ (ì›ë³¸)
        words = gt_info['words']
        
        if bboxes.shape[0] > self.max_word_num:
            bboxes = bboxes[:self.max_word_num]
            words = words[:self.max_word_num]
        
        # ì›ë³¸ ì´ë¯¸ì§€ì™€ GT ì •ë³´ë§Œ ë°˜í™˜ (ì „ì²˜ë¦¬ ì—†ìŒ)
        data = dict(
            raw_img=img,  # ì›ë³¸ ì´ë¯¸ì§€ (numpy array)
            raw_bboxes=bboxes,  # ì›ë³¸ bbox ì¢Œí‘œ
            raw_words=words,  # ì›ë³¸ í…ìŠ¤íŠ¸
            raw_gt_info=gt_info  # ì „ì²´ GT ì •ë³´
        )
        
        return data


class RawConcatLMDBDataset(data.Dataset):
    """
    ì—¬ëŸ¬ ì›ë³¸ LMDB ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, lmdb_paths, split='train', **kwargs):
        """
        Args:
            lmdb_paths (list): LMDB íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            split (str): 'train' ë˜ëŠ” 'test'
            **kwargs: RawLMDBDataset ìƒì„±ìì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìë“¤
        """
        self.datasets = []
        self.dataset_lengths = []
        
        for lmdb_path in lmdb_paths:
            print(f"ğŸ“‚ ì›ë³¸ LMDB ë¡œë“œ ì¤‘: {lmdb_path}")
            dataset = RawLMDBDataset(
                lmdb_path=lmdb_path,
                split=split,
                **kwargs
            )
            self.datasets.append(dataset)
            self.dataset_lengths.append(len(dataset))
        
        # ëˆ„ì  ê¸¸ì´ ê³„ì‚°
        self.cumulative_lengths = np.cumsum([0] + self.dataset_lengths)
        
        print(f"ğŸ¯ ì´ ì›ë³¸ ë°ì´í„°ì…‹ ìˆ˜: {len(self.datasets)}")
        print(f"ğŸ“Š ì´ ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {sum(self.dataset_lengths)}")
        
        # ê° ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥
        for i, (dataset, length) in enumerate(zip(self.datasets, self.dataset_lengths)):
            print(f"   ë°ì´í„°ì…‹ {i+1}: {length} ìƒ˜í”Œ")
    
    def __len__(self):
        return sum(self.dataset_lengths)
    
    def __getitem__(self, index):
        """ì „ì²´ ì¸ë±ìŠ¤ë¥¼ ë°ì´í„°ì…‹ë³„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
        # ì–´ë–¤ ë°ì´í„°ì…‹ì— ì†í•˜ëŠ”ì§€ ì°¾ê¸°
        dataset_idx = np.searchsorted(self.cumulative_lengths, index, side='right') - 1
        local_index = index - self.cumulative_lengths[dataset_idx]
        
        return self.datasets[dataset_idx][local_index] 