#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FAST LMDB Dataset
LMDB í˜•íƒœë¡œ ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” FAST ë°ì´í„°ì…‹ í´ë˜ìŠ¤
"""

import os
import sys
import random
import pickle
import numpy as np
import cv2
import mmcv
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image, ImageFilter
from torch.utils import data
from tqdm import tqdm

try:
    import lmdb
except ImportError:
    print("âŒ LMDB íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install lmdb' ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

from dataset.utils import shrink
from dataset.utils import get_vocabulary
from dataset.utils import random_crop_padding_v2 as random_crop_padding
from dataset.utils import random_scale, random_horizontal_flip, random_rotate
from dataset.utils import scale_aligned_short


class FAST_LMDB(data.Dataset):
    """
    LMDB í˜•íƒœë¡œ ì €ì¥ëœ ë°ì´í„°ë¥¼ ìœ„í•œ FAST ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    
    LMDB êµ¬ì¡°:
    - ì´ë¯¸ì§€: key='image-{:09d}'.format(idx), value=image_bytes
    - GT: key='gt-{:09d}'.format(idx), value=pickle.dumps(annotations)
    - ê¸¸ì´: key='num-samples', value=str(total_samples)
    """
    
    def __init__(self, lmdb_path, split='train', is_transform=False, img_size=None, 
                 short_size=736, pooling_size=9, with_rec=False, read_type='cv2',
                 repeat_times=1, report_speed=False):
        """
        Args:
            lmdb_path (str): LMDB ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            split (str): 'train' ë˜ëŠ” 'test'
            is_transform (bool): ë°ì´í„° ì¦ê°• í™œì„±í™” ì—¬ë¶€
            img_size (tuple/int): ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
            short_size (int): ìµœì†Œ ë³€ì˜ í¬ê¸°
            pooling_size (int): í’€ë§ í¬ê¸°
            with_rec (bool): ì¸ì‹ íƒœìŠ¤í¬ í¬í•¨ ì—¬ë¶€
            read_type (str): ì´ë¯¸ì§€ ì½ê¸° ë°©ì‹ ('cv2' ë˜ëŠ” 'pil')
            repeat_times (int): ë°ì´í„° ë°˜ë³µ ë°°ìˆ˜
            report_speed (bool): ì†ë„ ì¸¡ì • ëª¨ë“œ
        """
        self.lmdb_path = lmdb_path
        self.split = split
        self.is_transform = is_transform
        self.img_size = img_size if (img_size is None or isinstance(img_size, tuple)) else (img_size, img_size)
        self.pooling_size = pooling_size
        self.short_size = short_size
        self.with_rec = with_rec
        self.read_type = read_type
        self.repeat_times = repeat_times

        # í’€ë§ ë ˆì´ì–´ ì´ˆê¸°í™”
        self.pad = nn.ZeroPad2d(padding=(pooling_size - 1) // 2)
        self.pooling = nn.MaxPool2d(kernel_size=pooling_size, stride=1)
        self.overlap_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)

        # LMDB í™˜ê²½ ì—´ê¸°
        print(f"ğŸ—‚ï¸ LMDB ë°ì´í„°ë² ì´ìŠ¤ ì—´ê¸°: {lmdb_path}")
        if not os.path.exists(lmdb_path):
            raise FileNotFoundError(f"LMDB ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lmdb_path}")
        
        self.env = lmdb.open(lmdb_path, readonly=True, lock=False, readahead=False, meminit=False)
        
        # ë°ì´í„° í¬ê¸° í™•ì¸
        with self.env.begin(write=False) as txn:
            num_samples_key = 'num-samples'.encode()
            self.length = int(txn.get(num_samples_key).decode())
        
        # ë°˜ë³µ ë°°ìˆ˜ ì ìš©
        self.total_length = self.length * repeat_times
        
        print(f"ğŸ“Š LMDB ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   - ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {self.length}")
        print(f"   - ë°˜ë³µ ë°°ìˆ˜: {repeat_times}")
        print(f"   - ì´ ê¸¸ì´: {self.total_length}")
        
        # ì–´íœ˜ ì‚¬ì „ ë¡œë“œ
        self.voc, self.char2id, self.id2char = get_vocabulary('LOWERCASE')
        self.max_word_num = 200
        self.max_word_len = 32

    def __len__(self):
        return self.total_length

    def __del__(self):
        """ì†Œë©¸ìì—ì„œ LMDB í™˜ê²½ ë‹«ê¸°"""
        if hasattr(self, 'env'):
            self.env.close()

    def get_image_and_gt(self, index):
        """LMDBì—ì„œ ì´ë¯¸ì§€ì™€ GT ë°ì´í„°ë¥¼ ë¡œë“œ"""
        # ë°˜ë³µ ì¸ë±ìŠ¤ ì²˜ë¦¬
        real_index = index % self.length
        
        with self.env.begin(write=False) as txn:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img_key = f'image-{real_index:09d}'.encode()
            img_data = txn.get(img_key)
            if img_data is None:
                raise KeyError(f"ì´ë¯¸ì§€ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_key}")
            
            # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            img_np = np.frombuffer(img_data, dtype=np.uint8)
            if self.read_type == 'cv2':
                img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
                if img is None:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: index {real_index}, ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´")
                    # ë‹¤ë¥¸ ìœ íš¨í•œ ì¸ë±ìŠ¤ë¡œ ì¬ì‹œë„
                    return self.get_image_and_gt((index + 1) % len(self))
                img = img[:, :, [2, 1, 0]]  # BGR -> RGB
            else:  # PIL
                img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
                if img is None:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: index {real_index}, ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´")
                    # ë‹¤ë¥¸ ìœ íš¨í•œ ì¸ë±ìŠ¤ë¡œ ì¬ì‹œë„
                    return self.get_image_and_gt((index + 1) % len(self))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # GT ë°ì´í„° ë¡œë“œ
            gt_key = f'gt-{real_index:09d}'.encode()
            gt_data = txn.get(gt_key)
            if gt_data is None:
                raise KeyError(f"GT í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gt_key}")
            
            # pickleë¡œ ì§ë ¬í™”ëœ GT ë°ì´í„° ë³µì›
            gt_info = pickle.loads(gt_data)
            
        return img, gt_info

    def min_pooling(self, input_tensor):
        """ì˜¤ë²„ë© ì˜ì—­ì„ ì²˜ë¦¬í•˜ëŠ” Min Pooling"""
        input_tensor = torch.tensor(input_tensor, dtype=torch.float)
        temp = input_tensor.sum(dim=0).to(torch.uint8)
        overlap = (temp > 1).to(torch.float32).unsqueeze(0).unsqueeze(0)
        overlap = self.overlap_pool(overlap).squeeze(0).squeeze(0)

        B = input_tensor.size(0)
        h_sum = input_tensor.sum(dim=2) > 0
        
        h_sum_ = h_sum.long() * torch.arange(h_sum.shape[1], 0, -1)
        h_min = torch.argmax(h_sum_, 1, keepdim=True)
        h_sum_ = h_sum.long() * torch.arange(1, h_sum.shape[1] + 1)
        h_max = torch.argmax(h_sum_, 1, keepdim=True)

        w_sum = input_tensor.sum(dim=1) > 0
        w_sum_ = w_sum.long() * torch.arange(w_sum.shape[1], 0, -1)
        w_min = torch.argmax(w_sum_, 1, keepdim=True)
        w_sum_ = w_sum.long() * torch.arange(1, w_sum.shape[1] + 1)
        w_max = torch.argmax(w_sum_, 1, keepdim=True)

        for i in range(B):
            region = input_tensor[i:i + 1, h_min[i]:h_max[i] + 1, w_min[i]:w_max[i] + 1]
            region = self.pad(region)
            region = -self.pooling(-region)
            input_tensor[i:i + 1, h_min[i]:h_max[i] + 1, w_min[i]:w_max[i] + 1] = region

        x = input_tensor.sum(dim=0).to(torch.uint8)
        x[overlap > 0] = 0  # overlapping regions
        return x.numpy()

    def prepare_train_data(self, index):
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        img, gt_info = self.get_image_and_gt(index)
        
        # GT ì •ë³´ ì¶”ì¶œ
        bboxes = np.array(gt_info['bboxes'])  # ì •ê·œí™”ëœ ì¢Œí‘œ
        words = gt_info['words']
        
        if bboxes.shape[0] > self.max_word_num:
            bboxes = bboxes[:self.max_word_num]
            words = words[:self.max_word_num]

        # ë°ì´í„° ì¦ê°•
        if self.is_transform:
            img = random_scale(img, self.short_size, scales=[0.5, 2.0], aspects=[0.9, 1.1])

        # GT ë§ˆìŠ¤í¬ ìƒì„±
        gt_instance = np.zeros(img.shape[0:2], dtype='uint8')
        training_mask = np.ones(img.shape[0:2], dtype='uint8')
        
        if bboxes.shape[0] > 0:
            # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì‹¤ì œ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            bboxes = np.reshape(bboxes * ([img.shape[1], img.shape[0]] * 4),
                                (bboxes.shape[0], -1, 2)).astype('int32')
            
            for i in range(bboxes.shape[0]):
                if words[i] == '###':
                    cv2.drawContours(training_mask, [bboxes[i]], -1, 0, -1)
                else:
                    cv2.drawContours(gt_instance, [bboxes[i]], -1, i + 1, -1)

        # ì»¤ë„ ìƒì„±
        gt_kernels = []
        for i in range(len(bboxes)):
            gt_kernel = np.zeros(img.shape[0:2], dtype='uint8')
            if words[i] != '###':
                cv2.drawContours(gt_kernel, [bboxes[i]], -1, 1, -1)
                gt_kernels.append(gt_kernel)
            else:
                if len(gt_kernels) == 0:
                    gt_kernels.append(gt_kernel)
        
        if len(gt_kernels) > 0:
            gt_kernels = np.array(gt_kernels)
            gt_kernel = self.min_pooling(gt_kernels)
        else:
            gt_kernel = np.zeros(img.shape[0:2], dtype='uint8')

        # ì»¤ë„ ìˆ˜ì¶•
        shrink_kernel_scale = 0.1
        gt_kernel_shrinked = np.zeros(img.shape[0:2], dtype='uint8')
        kernel_bboxes = shrink(bboxes, shrink_kernel_scale)
        for i in range(bboxes.shape[0]):
            if words[i] != '###':
                cv2.drawContours(gt_kernel_shrinked, [kernel_bboxes[i]], -1, 1, -1)
        gt_kernel = np.maximum(gt_kernel, gt_kernel_shrinked)

        # ê¸°í•˜í•™ì  ë³€í™˜
        if self.is_transform:
            imgs = [img, gt_instance, training_mask, gt_kernel]

            if not self.with_rec:
                imgs = random_horizontal_flip(imgs)
            imgs = random_rotate(imgs, random_angle=30)
            imgs = random_crop_padding(imgs, self.img_size)
            img, gt_instance, training_mask, gt_kernel = imgs[0], imgs[1], imgs[2], imgs[3]

        # í…ìŠ¤íŠ¸ ë§ˆìŠ¤í¬
        gt_text = gt_instance.copy()
        gt_text[gt_text > 0] = 1

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img = Image.fromarray(img)
        img = img.convert('RGB')
        if self.is_transform:
            if random.random() < 0.5:
                img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))
            img = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)(img)

        img = transforms.ToTensor()(img)
        img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)
        gt_text = torch.from_numpy(gt_text).long()
        gt_kernel = torch.from_numpy(gt_kernel).long()
        training_mask = torch.from_numpy(training_mask).long()
        gt_instance = torch.from_numpy(gt_instance).long()

        data = dict(
            imgs=img,
            gt_texts=gt_text,
            gt_kernels=gt_kernel,
            training_masks=training_mask,
            gt_instances=gt_instance,
        )

        return data

    def prepare_test_data(self, index):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        img, gt_info = self.get_image_and_gt(index)
        filename = gt_info.get('filename', f'image_{index:06d}')

        img_meta = dict(
            org_img_size=np.array(img.shape[:2])
        )

        img = scale_aligned_short(img, self.short_size)
        img_meta.update(dict(
            img_size=np.array(img.shape[:2]),
            filename=filename
        ))

        img = Image.fromarray(img)
        img = img.convert('RGB')
        img = transforms.ToTensor()(img)
        img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)

        data = dict(
            imgs=img,
            img_metas=img_meta
        )

        return data

    def __getitem__(self, index):
        """ë°ì´í„° ë¡œë“œ"""
        if self.split == 'train':
            return self.prepare_train_data(index)
        elif self.split == 'test':
            return self.prepare_test_data(index)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” split: {self.split}")


def create_lmdb_dataset(image_dir, gt_dir, output_path, annotation_parser='ic15'):
    """
    ì¼ë°˜ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ LMDB í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        image_dir (str): ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        gt_dir (str): GT íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ  
        output_path (str): ì¶œë ¥ LMDB ê²½ë¡œ
        annotation_parser (str): GT íŒŒì‹± ë°©ì‹ ('ic15', 'ic17mlt', 'text_in_wild', 'ocr_public', 'handwriting_ocr', 'public_admin_ocr')
    """
    import json
    
    print(f"ğŸ”„ LMDB ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    print(f"   - ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {image_dir}")
    print(f"   - GT ë””ë ‰í† ë¦¬: {gt_dir}")
    print(f"   - ì¶œë ¥ ê²½ë¡œ: {output_path}")
    print(f"   - íŒŒì„œ íƒ€ì…: {annotation_parser}")
    
    # LMDB í™˜ê²½ ìƒì„±
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    env = lmdb.open(output_path, map_size=1099511627776)  # 1TB
    
    if annotation_parser == 'text_in_wild':
        # Text in the wild ë°ì´í„°ì…‹ ì²˜ë¦¬ (í•˜ë‚˜ì˜ JSON íŒŒì¼ì— ëª¨ë“  ì •ë³´)
        json_files = [f for f in os.listdir(gt_dir) if f.endswith('.json')]
        if not json_files:
            raise FileNotFoundError(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gt_dir}")
        
        json_path = os.path.join(gt_dir, json_files[0])
        print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        images_info = {img['id']: img for img in data['images']}
        
        # ì´ë¯¸ì§€ë³„ë¡œ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”
        image_annotations = {}
        for ann in data['annotations']:
            img_id = ann['image_id']
            if img_id not in image_annotations:
                image_annotations[img_id] = []
            image_annotations[img_id].append(ann)
        
        print(f"ğŸ“Š ì´ {len(images_info)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        with env.begin(write=True) as txn:
            idx = 0
            for img_id, img_info in tqdm(images_info.items(), desc="Text in the wild ì²˜ë¦¬ ì¤‘", total=len(images_info)):
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                img_path = os.path.join(image_dir, img_info['file_name'])
                if not os.path.exists(img_path):
                    print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}")
                    continue
                
                with open(img_path, 'rb') as f:
                    img_data = f.read()
                
                # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
                bboxes = []
                words = []
                if img_id in image_annotations:
                    for ann in image_annotations[img_id]:
                        # bbox: [x, y, width, height] -> [x1, y1, x2, y2, x3, y3, x4, y4]
                        x, y, w, h = ann['bbox']
                        x1, y1, x2, y2 = x, y, x + w, y + h
                        
                        # ì •ê·œí™”
                        img_w, img_h = img_info['width'], img_info['height']
                        normalized_coords = [x1/img_w, y1/img_h, x2/img_w, y1/img_h, 
                                           x2/img_w, y2/img_h, x1/img_w, y2/img_h]
                        
                        bboxes.append(normalized_coords)
                        words.append(ann['text'])
                
                gt_info = {
                    'bboxes': bboxes,
                    'words': words,
                    'filename': img_info['file_name']
                }
                
                # LMDBì— ì €ì¥
                img_key = f'image-{idx:09d}'.encode()
                gt_key = f'gt-{idx:09d}'.encode()
                
                txn.put(img_key, img_data)
                txn.put(gt_key, pickle.dumps(gt_info))
                idx += 1
            
            # ì´ ìƒ˜í”Œ ìˆ˜ ì €ì¥
            txn.put('num-samples'.encode(), str(idx).encode())
    
    elif annotation_parser in ['ocr_public', 'handwriting_ocr']:
        # 023.OCR ë°ì´í„°(ê³µê³µ), 053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„° ì²˜ë¦¬
        # ê° ì´ë¯¸ì§€ë§ˆë‹¤ ê°œë³„ JSON íŒŒì¼
        img_names = []
        for ext in ['.jpg', '.png', '.JPG', '.PNG', '.jpeg', '.JPEG']:
            img_names.extend([f for f in os.listdir(image_dir) if f.endswith(ext)])
        
        print(f"ğŸ“Š ì´ {len(img_names)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        with env.begin(write=True) as txn:
            idx = 0
            for img_name in tqdm(img_names, desc=f"{annotation_parser} ì²˜ë¦¬ ì¤‘", total=len(img_names)):
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                img_path = os.path.join(image_dir, img_name)
                with open(img_path, 'rb') as f:
                    img_data = f.read()
                
                # JSON íŒŒì¼ ë¡œë“œ
                json_name = img_name.split('.')[0] + '.json'
                json_path = os.path.join(gt_dir, json_name)
                
                bboxes = []
                words = []
                if os.path.exists(json_path):
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
                    img_w = data['Images']['width']
                    img_h = data['Images']['height']
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ì²˜ë¦¬
                    bbox_key = 'Bbox' if annotation_parser == 'ocr_public' else 'bbox'
                    for bbox_info in data[bbox_key]:
                        # x: [x1, x1, x2, x2], y: [y1, y2, y1, y2] -> [x1, y1, x2, y1, x2, y2, x1, y2]
                        x_coords = bbox_info['x']
                        y_coords = bbox_info['y']
                        
                        # ì •ê·œí™”
                        normalized_coords = [
                            x_coords[0]/img_w, y_coords[0]/img_h,  # x1, y1
                            x_coords[2]/img_w, y_coords[0]/img_h,  # x2, y1
                            x_coords[2]/img_w, y_coords[1]/img_h,  # x2, y2
                            x_coords[0]/img_w, y_coords[1]/img_h   # x1, y2
                        ]
                        
                        bboxes.append(normalized_coords)
                        words.append(bbox_info['data'])
                else:
                    print(f"âš ï¸ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
                
                gt_info = {
                    'bboxes': bboxes,
                    'words': words,
                    'filename': img_name
                }
                
                # LMDBì— ì €ì¥
                img_key = f'image-{idx:09d}'.encode()
                gt_key = f'gt-{idx:09d}'.encode()
                
                txn.put(img_key, img_data)
                txn.put(gt_key, pickle.dumps(gt_info))
                idx += 1
            
            # ì´ ìƒ˜í”Œ ìˆ˜ ì €ì¥
            txn.put('num-samples'.encode(), str(idx).encode())
    
    elif annotation_parser == 'public_admin_ocr':
        # ê³µê³µí–‰ì •ë¬¸ì„œ OCR ë°ì´í„° ì²˜ë¦¬
        img_names = []
        for ext in ['.jpg', '.png', '.JPG', '.PNG', '.jpeg', '.JPEG']:
            img_names.extend([f for f in os.listdir(image_dir) if f.endswith(ext)])
        
        print(f"ğŸ“Š ì´ {len(img_names)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        with env.begin(write=True) as txn:
            idx = 0
            for img_name in tqdm(img_names, desc="ê³µê³µí–‰ì •ë¬¸ì„œ OCR ì²˜ë¦¬ ì¤‘", total=len(img_names)):
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                img_path = os.path.join(image_dir, img_name)
                with open(img_path, 'rb') as f:
                    img_data = f.read()
                
                # JSON íŒŒì¼ ë¡œë“œ
                json_name = img_name.split('.')[0] + '.json'
                json_path = os.path.join(gt_dir, json_name)
                
                bboxes = []
                words = []
                if os.path.exists(json_path):
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
                    img_w = data['images'][0]['image.width']
                    img_h = data['images'][0]['image.height']
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ì²˜ë¦¬
                    for ann in data['annotations']:
                        # annotation.bbox: [x, y, width, height] -> [x1, y1, x2, y2, x3, y3, x4, y4]
                        x, y, w, h = ann['annotation.bbox']
                        x1, y1, x2, y2 = x, y, x + w, y + h
                        
                        # ì •ê·œí™”
                        normalized_coords = [x1/img_w, y1/img_h, x2/img_w, y1/img_h, 
                                           x2/img_w, y2/img_h, x1/img_w, y2/img_h]
                        
                        bboxes.append(normalized_coords)
                        words.append(ann['annotation.text'])
                else:
                    print(f"âš ï¸ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
                
                gt_info = {
                    'bboxes': bboxes,
                    'words': words,
                    'filename': img_name
                }
                
                # LMDBì— ì €ì¥
                img_key = f'image-{idx:09d}'.encode()
                gt_key = f'gt-{idx:09d}'.encode()
                
                txn.put(img_key, img_data)
                txn.put(gt_key, pickle.dumps(gt_info))
                idx += 1
            
            # ì´ ìƒ˜í”Œ ìˆ˜ ì €ì¥
            txn.put('num-samples'.encode(), str(idx).encode())
    
    else:
        # ê¸°ì¡´ IC15, IC17MLT ë“±ì˜ txt íŒŒì¼ ì²˜ë¦¬
        img_names = []
        for ext in ['.jpg', '.png', '.JPG', '.PNG', '.jpeg', '.JPEG']:
            img_names.extend([f for f in os.listdir(image_dir) if f.endswith(ext)])
        
        print(f"ğŸ“Š ì´ {len(img_names)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        with env.begin(write=True) as txn:
            for idx, img_name in enumerate(tqdm(img_names, desc=f"{annotation_parser} ì²˜ë¦¬ ì¤‘", total=len(img_names))):
                
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì¸ì½”ë”©
                img_path = os.path.join(image_dir, img_name)
                with open(img_path, 'rb') as f:
                    img_data = f.read()
                
                # GT íŒŒì¼ íŒŒì‹±
                gt_name = img_name.split('.')[0] + '.txt'
                if annotation_parser == 'ic17mlt':
                    gt_name = 'gt_' + gt_name
                gt_path = os.path.join(gt_dir, gt_name)
                
                # GT ë°ì´í„° íŒŒì‹± (ê¸°ë³¸ì ì¸ IC15 í˜•ì‹)
                if os.path.exists(gt_path):
                    with open(gt_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    bboxes = []
                    words = []
                    for line in lines:
                        line = line.strip()
                        if not line:
                            continue
                        
                        parts = line.split(',')
                        if len(parts) >= 8:
                            # IC15 í˜•ì‹: x1,y1,x2,y2,x3,y3,x4,y4,text
                            coords = [int(x) for x in parts[:8]]
                            text = ','.join(parts[8:]) if len(parts) > 8 else '???'
                            
                            # ì •ê·œí™” (LMDB ì €ì¥ì„ ìœ„í•´ ì´ë¯¸ì§€ í¬ê¸° í•„ìš”)
                            img_cv = cv2.imread(img_path)
                            h, w = img_cv.shape[:2]
                            normalized_coords = [c / w if i % 2 == 0 else c / h for i, c in enumerate(coords)]
                            
                            bboxes.append(normalized_coords)
                            words.append(text)
                    
                    gt_info = {
                        'bboxes': bboxes,
                        'words': words,
                        'filename': img_name
                    }
                else:
                    print(f"âš ï¸ GT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gt_path}")
                    gt_info = {'bboxes': [], 'words': [], 'filename': img_name}
                
                # LMDBì— ì €ì¥
                img_key = f'image-{idx:09d}'.encode()
                gt_key = f'gt-{idx:09d}'.encode()
                
                txn.put(img_key, img_data)
                txn.put(gt_key, pickle.dumps(gt_info))
            
            # ì´ ìƒ˜í”Œ ìˆ˜ ì €ì¥
            txn.put('num-samples'.encode(), str(len(img_names)).encode())
    
    env.close()
    print(f"âœ… LMDB ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {output_path}")


if __name__ == '__main__':
    # ì‚¬ìš© ì˜ˆì‹œ
    print("ğŸ§ª FAST LMDB ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸")
    
    # ì˜ˆì‹œ: ê¸°ì¡´ ë°ì´í„°ë¥¼ LMDBë¡œ ë³€í™˜
    # create_lmdb_dataset(
    #     image_dir='/path/to/images',
    #     gt_dir='/path/to/gt',
    #     output_path='/path/to/output.lmdb'
    # )
    
    # ì˜ˆì‹œ: LMDB ë°ì´í„°ì…‹ ë¡œë“œ
    # dataset = FAST_LMDB(
    #     lmdb_path='/path/to/dataset.lmdb',
    #     split='train',
    #     is_transform=True
    # )
    # print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}") 