#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import subprocess
import time
import gc
from pathlib import Path
from tqdm import tqdm
from tqdm.contrib.concurrent import thread_map
from concurrent.futures import ThreadPoolExecutor, as_completed
import math
import numba
from numba import jit, prange

# FTP ì„¤ì •
FTP_BASE_PATH = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"
LOCAL_OUTPUT_PATH = "/mnt/nas/ocr_dataset/json_data"
BACKUP_OUTPUT_PATH = "/home/mango/ocr_test/FAST/json_merged"  # ê¸°ì¡´ ê²½ë¡œë„ ë°±ì—…ìš©ìœ¼ë¡œ ì‚¬ìš©

@jit(nopython=True, cache=True)
def calculate_file_size_mb(file_size_bytes):
    """íŒŒì¼ í¬ê¸°ë¥¼ MBë¡œ ë³€í™˜ (numba ìµœì í™”)"""
    return file_size_bytes / (1024.0 * 1024.0)

@jit(nopython=True, cache=True)
def check_step_save_condition(current_count, step_count, step_size=100000):
    """10000ê°œ ë‹¨ìœ„ ì €ì¥ ì¡°ê±´ í™•ì¸ (numba ìµœì í™”)"""
    return current_count >= (step_count + 1) * step_size

@jit(nopython=True, cache=True)
def calculate_id_offsets(images_count, annotations_count):
    """ID ì˜¤í”„ì…‹ ê³„ì‚° (numba ìµœì í™”)"""
    return images_count, annotations_count

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    try:
        subprocess.run(['sync'], check=False)
        subprocess.run(['echo', '1'], stdout=subprocess.PIPE, check=False)
        subprocess.run(['tee', '/proc/sys/vm/drop_caches'], input=b'1', check=False)
    except:
        pass

def force_cleanup_memory():
    """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    for _ in range(3):
        gc.collect()
    try:
        subprocess.run(['sync'], check=False)
        subprocess.run(['echo', '3'], stdout=subprocess.PIPE, check=False)
        subprocess.run(['tee', '/proc/sys/vm/drop_caches'], input=b'3', check=False)
    except:
        pass

def remount_ftp_for_large_file():
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•´ gvfs ê²½ë¡œ ì¬í™•ì¸"""
    gvfs_path = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"
    
    # gvfs ê²½ë¡œê°€ ì—¬ì „íˆ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if os.path.exists(gvfs_path):
        print("âœ… gvfs FTP ê²½ë¡œ ì¬í™•ì¸ ì™„ë£Œ")
        return True
    else:
        print("âŒ gvfs FTP ê²½ë¡œê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
        print("ğŸ’¡ íŒŒì¼ ê´€ë¦¬ìì—ì„œ FTP ì„œë²„ì— ì¬ì ‘ì†í•´ì£¼ì„¸ìš”")
        return False

def setup_ftp_mount():
    """gvfs FTP ê²½ë¡œ í™•ì¸"""
    print("ğŸ”„ gvfs FTP ê²½ë¡œ í™•ì¸ ì¤‘...")
    
    # gvfsë¥¼ ì‚¬ìš©í•œ FTP ì ‘ê·¼
    gvfs_path = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"
    
    # gvfs ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if os.path.exists(gvfs_path):
        print("âœ… gvfs FTP ê²½ë¡œ í™•ì¸ ì™„ë£Œ")
        return True
    else:
        print("âŒ gvfs FTP ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        print("ğŸ’¡ íŒŒì¼ ê´€ë¦¬ìì—ì„œ FTP ì„œë²„ì— ì ‘ì†í•˜ì—¬ gvfs ë§ˆìš´íŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”")
        return False

def load_json_simple(json_path):
    """ê°„ë‹¨í•œ JSON íŒŒì¼ ë¡œë“œ (ë©”ëª¨ë¦¬ ë§¤í•‘ ì‚¬ìš©)"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        return None

def load_json_mmap(json_path):
    """ë©”ëª¨ë¦¬ ë§¤í•‘ì„ ì‚¬ìš©í•œ JSON íŒŒì¼ ë¡œë“œ (ë” ë¹ ë¦„)"""
    try:
        import mmap
        with open(json_path, 'r', encoding='utf-8') as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                data = json.loads(mm.read().decode('utf-8'))
        return data
    except Exception as e:
        # ë©”ëª¨ë¦¬ ë§¤í•‘ ì‹¤íŒ¨ì‹œ ì¼ë°˜ ë°©ë²•ìœ¼ë¡œ í´ë°±
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except:
            return None

def process_json_file(json_file, dataset_name):
    """ê°œë³„ JSON íŒŒì¼ ì²˜ë¦¬"""
    data = load_json_mmap(str(json_file))
    if not data:
        return None
    
    original_path = str(json_file)
    result = {
        'images': [],
        'annotations': [],
        'original_path': original_path,
        'sub_dataset': json_file.stem
    }
    
    if 'public_admin' in dataset_name:
        # ê³µê³µí–‰ì •ë¬¸ì„œ: images, annotations êµ¬ì¡°
        result['images'] = [
            {**img, 'dataset': dataset_name, 'sub_dataset': json_file.stem, 'original_json_path': original_path}
            for img in data.get('images', [])
        ]
        result['annotations'] = [
            {**ann, 'dataset': dataset_name, 'sub_dataset': json_file.stem, 'original_json_path': original_path}
            for ann in data.get('annotations', [])
        ]
    
    elif any(name in dataset_name for name in ['ocr_public', 'finance_logistics', 'handwriting']):
        # OCR ê³µê³µ/ê¸ˆìœµ/ì†ê¸€ì”¨: Images, bbox êµ¬ì¡°
        if 'Images' in data:
            img_info = data['Images']
            result['images'] = [{
                'dataset': dataset_name,
                'sub_dataset': json_file.stem,
                'original_json_path': original_path,
                'width': img_info.get('width', 0),
                'height': img_info.get('height', 0),
                'file_name': img_info.get('filename', json_file.stem)
            }]
            
            # bbox ì •ë³´ ì²˜ë¦¬
            bbox_key = 'bbox' if 'bbox' in data else 'Bbox'
            if bbox_key in data:
                result['annotations'] = [
                    {
                        'dataset': dataset_name,
                        'sub_dataset': json_file.stem,
                        'original_json_path': original_path,
                        'bbox': bbox_info.get('x', []) + bbox_info.get('y', []),
                        'text': bbox_info.get('data', '')
                    }
                    for bbox_info in data[bbox_key]
                ]
    
    return result

def save_intermediate_result(dataset_data, dataset_name, step_count):
    """10000ê°œ ë‹¨ìœ„ë¡œ ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
    output_path = os.path.join(LOCAL_OUTPUT_PATH, f"{dataset_name}_100000_step_save_{step_count:03d}.json")
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_data, f, ensure_ascii=False, indent=2)
        
        # numba ìµœì í™”ëœ íŒŒì¼ í¬ê¸° ê³„ì‚°
        file_size_mb = calculate_file_size_mb(os.path.getsize(output_path))
        print(f"100000_step_save_{step_count:03d}: {len(dataset_data['images']):,}ê°œ ì´ë¯¸ì§€, {file_size_mb:.1f}MB")
        
    except Exception as e:
        print(f"âŒ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")

def load_json_with_retry(json_path):
    """JSON íŒŒì¼ ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            return data
        except json.JSONDecodeError as e:
            if attempt < max_retries - 1:
                cleanup_memory()
                force_cleanup_memory()
                if not remount_ftp_for_large_file():
                    return None
                time.sleep(3)
            else:
                return load_json_via_local_download(json_path)
        except Exception as e:
            if attempt < max_retries - 1:
                cleanup_memory()
                force_cleanup_memory()
                if not remount_ftp_for_large_file():
                    return None
                time.sleep(3)
            else:
                return load_json_via_local_download(json_path)
    return None

def load_json_via_local_download(json_path):
    """ë¡œì»¬ ë‹¤ìš´ë¡œë“œë¡œ JSON íŒŒì¼ ë¡œë“œ"""
    temp_json_path = "/tmp/temp_json_file.json"
    try:
        subprocess.run(['cp', json_path, temp_json_path], check=True)
        with open(temp_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        os.remove(temp_json_path)
        return data
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        if os.path.exists(temp_json_path):
            os.remove(temp_json_path)
        return None

def find_latest_step_save(dataset_name):
    """ê°€ì¥ ìµœê·¼ ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì°¾ê¸°"""
    step_files = []
    for file in os.listdir(LOCAL_OUTPUT_PATH):
        if file.startswith(f"{dataset_name}_100000_step_save_") and file.endswith('.json'):
            step_files.append(file)
    
    if not step_files:
        return None, 0
    
    # íŒŒì¼ëª…ì—ì„œ step ë²ˆí˜¸ ì¶”ì¶œí•˜ì—¬ ì •ë ¬
    step_files.sort(key=lambda x: int(x.split('_')[-1].replace('.json', '')))
    latest_file = step_files[-1]
    latest_step = int(latest_file.split('_')[-1].replace('.json', ''))
    
    return os.path.join(LOCAL_OUTPUT_PATH, latest_file), latest_step

def load_latest_step_data(dataset_name):
    """ê°€ì¥ ìµœê·¼ ì¤‘ê°„ ì €ì¥ íŒŒì¼ ë¡œë“œ"""
    latest_file, latest_step = find_latest_step_save(dataset_name)
    
    if latest_file and os.path.exists(latest_file):
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“‚ ì¤‘ê°„ ì €ì¥ íŒŒì¼ ë¡œë“œ: {latest_file} (step {latest_step})")
            print(f"ğŸ“Š í˜„ì¬ ì§„í–‰ ìƒí™©: {len(data['images']):,}ê°œ ì´ë¯¸ì§€")
            return data, latest_step
        except Exception as e:
            print(f"âŒ ì¤‘ê°„ ì €ì¥ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, 0
    
    return None, 0

def merge_json_datasets():
    """ê° í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ë³„ë¡œ JSON íŒŒì¼ ìƒì„± (100000ê°œì”© ì¤‘ê°„ ì €ì¥)"""
    print("ğŸš€ í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ë³„ JSON íŒŒì¼ ìƒì„± ì‹œì‘ (100000ê°œì”© ì¤‘ê°„ ì €ì¥)")
    print("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(LOCAL_OUTPUT_PATH, exist_ok=True)
    
    # create_all_datasets_500.py ê¸°ì¤€ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •
    datasets = [
        {
            "name": "public_admin_train",
            "path": f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR/Training/[ë¼ë²¨]train/01.ë¼ë²¨ë§ë°ì´í„°(Json)",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "public_admin_train_partly",
            "path": f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR/Training/[ë¼ë²¨]train_partly_labling",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "public_admin_valid",
            "path": f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR/Validation/[ë¼ë²¨]validation/01.ë¼ë²¨ë§ë°ì´í„°(Json)",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "ocr_public_train",
            "path": f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "ocr_public_valid",
            "path": f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/02.ë¼ë²¨ë§ë°ì´í„°",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "finance_logistics_train",
            "path": f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Training/02.ë¼ë²¨ë§ë°ì´í„°",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "finance_logistics_valid",
            "path": f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°/Validation/02.ë¼ë²¨ë§ë°ì´í„°",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "handwriting_train",
            "path": f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°/1.Training/ë¼ë²¨ë§ë°ì´í„°",
            "type": "multi",
            "pattern": "**/*.json"
        },
        {
            "name": "handwriting_valid",
            "path": f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°/2.Validation/ë¼ë²¨ë§ë°ì´í„°",
            "type": "multi",
            "pattern": "**/*.json"
        }
    ]
    
    created_files = []
    
    # ì´ë¯¸ ì™„ë£Œëœ ë°ì´í„°ì…‹ë“¤ í™•ì¸
    completed_datasets = []
    for dataset in datasets:
        # ìƒˆë¡œìš´ ê²½ë¡œ í™•ì¸
        output_file = os.path.join(LOCAL_OUTPUT_PATH, f"{dataset['name']}_merged.json")
        # ê¸°ì¡´ ê²½ë¡œë„ í™•ì¸
        old_output_file = os.path.join(BACKUP_OUTPUT_PATH, f"{dataset['name']}_merged.json")
        
        if os.path.exists(output_file):
            # numba ìµœì í™”ëœ íŒŒì¼ í¬ê¸° ê³„ì‚°
            file_size = calculate_file_size_mb(os.path.getsize(output_file))
            print(f"âœ… {dataset['name']} ì´ë¯¸ ì™„ë£Œë¨ (íŒŒì¼ í¬ê¸°: {file_size:.2f} MB)")
            completed_datasets.append(dataset['name'])
        elif os.path.exists(old_output_file):
            # ê¸°ì¡´ ê²½ë¡œì— ìˆëŠ” íŒŒì¼ í™•ì¸
            file_size = calculate_file_size_mb(os.path.getsize(old_output_file))
            print(f"âœ… {dataset['name']} ì´ë¯¸ ì™„ë£Œë¨ (ê¸°ì¡´ ê²½ë¡œ, íŒŒì¼ í¬ê¸°: {file_size:.2f} MB)")
            completed_datasets.append(dataset['name'])
        else:
            print(f"ğŸ”„ {dataset['name']} ì²˜ë¦¬ í•„ìš”")
    
    print(f"\nğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°ì…‹: {len(datasets) - len(completed_datasets)}ê°œ")
    print(f"ğŸ“Š ê±´ë„ˆë›¸ ë°ì´í„°ì…‹: {len(completed_datasets)}ê°œ")
    
    for dataset in datasets:
        # ì´ë¯¸ ì™„ë£Œëœ ë°ì´í„°ì…‹ì€ ê±´ë„ˆë›°ê¸°
        if dataset['name'] in completed_datasets:
            print(f"\nâ­ï¸ {dataset['name']} ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì™„ë£Œë¨)")
            continue
            
        print(f"\nğŸ“Š {dataset['name']} ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        
        # ê° ë°ì´í„°ì…‹ë³„ë¡œ ìƒˆë¡œìš´ ë°ì´í„° êµ¬ì¡° ìƒì„±
        dataset_data = {
            "images": [],
            "annotations": [],
            "categories": [],
            "info": {
                "description": f"Korean OCR Dataset - {dataset['name']}",
                "version": "1.0",
                "year": 2024,
                "contributor": "OCR Test Project"
            }
        }
        
        if dataset['type'] == 'single':
            # ë‹¨ì¼ JSON íŒŒì¼
            json_path = dataset['path']
            if os.path.exists(json_path):
                data = load_json_with_retry(json_path)
                if data:
                    # ì´ë¯¸ì§€ ID ì¬ë§¤í•‘
                    image_id_offset = len(dataset_data['images'])
                    annotation_id_offset = len(dataset_data['annotations'])
                    
                    # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
                    for i, img in enumerate(data.get('images', [])):
                        # idê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        if 'id' not in img:
                            img['id'] = image_id_offset + i
                        else:
                            img['id'] += image_id_offset
                        img['dataset'] = dataset['name']
                        dataset_data['images'].append(img)
                    
                    # ì–´ë…¸í…Œì´ì…˜ ì •ë³´ ì¶”ê°€
                    for i, ann in enumerate(data.get('annotations', [])):
                        # idê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        if 'id' not in ann:
                            ann['id'] = annotation_id_offset + i
                        else:
                            ann['id'] += annotation_id_offset
                        
                        # image_idê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ idë¥¼ ì‚¬ìš©
                        if 'image_id' not in ann:
                            ann['image_id'] = image_id_offset
                        else:
                            ann['image_id'] += image_id_offset
                            
                        ann['dataset'] = dataset['name']
                        dataset_data['annotations'].append(ann)
                    
                    # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                    for cat in data.get('categories', []):
                        if cat not in dataset_data['categories']:
                            dataset_data['categories'].append(cat)
                    
                    print(f"âœ… {dataset['name']}: {len(data.get('images', []))}ê°œ ì´ë¯¸ì§€, {len(data.get('annotations', []))}ê°œ ì–´ë…¸í…Œì´ì…˜")
                else:
                    print(f"âŒ {dataset['name']} JSON ë¡œë“œ ì‹¤íŒ¨")
            else:
                print(f"âŒ {dataset['name']} ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {json_path}")
        
        elif dataset['type'] == 'multi':
            # ì—¬ëŸ¬ JSON íŒŒì¼
            dataset_path = dataset['path']
    
            if os.path.exists(dataset_path):
                # ì¤‘ê°„ ì €ì¥ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
                resume_data, resume_step = load_latest_step_data(dataset['name'])
                
                if resume_data:
                    # ì¤‘ê°„ ì €ì¥ íŒŒì¼ì—ì„œ ì´ì–´ì„œ ì‹œì‘
                    dataset_data = resume_data
                    step_count = resume_step
                    print(f"ğŸ”„ {dataset['name']} ì¤‘ê°„ ì €ì¥ íŒŒì¼ì—ì„œ ì´ì–´ì„œ ì‹œì‘ (step {step_count})")
                else:
                    # ìƒˆë¡œ ì‹œì‘
                    step_count = 0
                
                # os.scandirì„ ì‚¬ìš©í•œ ì¬ê·€ì  íŒŒì¼ ê²€ìƒ‰
                json_files = []
                
                def scan_directory_recursive(directory):
                    try:
                        with os.scandir(directory) as entries:
                            for entry in entries:
                                if entry.is_file() and entry.name.endswith('.json'):
                                    json_files.append(Path(entry.path))
                                elif entry.is_dir():
                                    scan_directory_recursive(entry.path)
                    except Exception as e:
                        print(f"âŒ scandir ì‹¤íŒ¨: {e}")
                
                scan_directory_recursive(dataset_path)
                print(f"ğŸ“ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")
                
                # ìŠ¤ë ˆë“œ ìˆ˜ ì„¤ì • (ë””ë²„ê·¸ ê²°ê³¼ ê¸°ë°˜ - 1ê°œê°€ ìµœê³  ì„±ëŠ¥)
                num_workers = 20  # ë””ë²„ê·¸ì—ì„œ ê°€ì¥ ë¹ ë¥¸ ì„±ëŠ¥
                print(f"ğŸ”§ ìŠ¤ë ˆë“œ ìˆ˜: {num_workers}ê°œ")
                
                # ìˆœì°¨ ì²˜ë¦¬í•˜ë©´ì„œ 10000ê°œì”© ì¤‘ê°„ ì €ì¥
                image_id_offset = len(dataset_data['images'])
                annotation_id_offset = len(dataset_data['annotations'])
                
                # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ë§Œí¼ ê±´ë„ˆë›°ê¸°
                processed_count = step_count * 100000
                if processed_count > 0:
                    print(f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ {processed_count:,}ê°œ íŒŒì¼ ê±´ë„ˆë›°ê¸°")
                
                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê°œì„  (ì›ë˜ ë¡œì§ ìœ ì§€)
                with ThreadPoolExecutor(max_workers=num_workers) as executor:
                    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ë§Œí¼ ê±´ë„ˆë›°ê¸°
                    remaining_files = json_files[processed_count:]
                    
                    # ë³‘ë ¬ë¡œ future ìƒì„±
                    futures = {executor.submit(process_json_file, json_file, dataset['name']): json_file 
                             for json_file in remaining_files}
                    
                    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œí•˜ë©´ì„œ ë³‘ë ¬ ì²˜ë¦¬
                    for future in tqdm(as_completed(futures), total=len(futures), desc=f"{dataset['name']} ì²˜ë¦¬"):
                        json_file = futures[future]
                        result = future.result()
                        
                        if result:
                            # ì´ë¯¸ì§€ ID ì¬ë§¤í•‘ (ì›ë˜ ë¡œì§ ìœ ì§€)
                            for j, img in enumerate(result['images']):
                                if 'id' not in img:
                                    img['id'] = image_id_offset + j
                                else:
                                    img['id'] += image_id_offset
                                dataset_data['images'].append(img)
                            
                            # ì–´ë…¸í…Œì´ì…˜ ID ì¬ë§¤í•‘ (ì›ë˜ ë¡œì§ ìœ ì§€)
                            for j, ann in enumerate(result['annotations']):
                                if 'id' not in ann:
                                    ann['id'] = annotation_id_offset + j
                                else:
                                    ann['id'] += annotation_id_offset
                                
                                if 'image_id' not in ann:
                                    ann['image_id'] = image_id_offset
                                else:
                                    ann['image_id'] += image_id_offset
                                
                                dataset_data['annotations'].append(ann)
                            
                            # numba ìµœì í™”ëœ ID ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸ (ì›ë˜ ë¡œì§ ìœ ì§€)
                            image_id_offset, annotation_id_offset = calculate_id_offsets(
                                len(dataset_data['images']), 
                                len(dataset_data['annotations'])
                            )
                            
                            # numba ìµœì í™”ëœ 10000ê°œ ë‹¨ìœ„ ì €ì¥ ì¡°ê±´ í™•ì¸ (ì›ë˜ ë¡œì§ ìœ ì§€)
                            if check_step_save_condition(len(dataset_data['images']), step_count):
                                step_count += 1 
                                save_intermediate_result(dataset_data, dataset['name'], step_count)
            else:
                print(f"âŒ {dataset['name']} ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dataset_path}")
        
        # ê° ë°ì´í„°ì…‹ë³„ë¡œ ê°œë³„ íŒŒì¼ ì €ì¥
        if dataset_data['images'] or dataset_data['annotations']:
            output_path = os.path.join(LOCAL_OUTPUT_PATH, f"{dataset['name']}_merged.json")
            print(f"\nğŸ’¾ {dataset['name']} ìµœì¢… ì €ì¥ ì¤‘: {output_path}")
            
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(dataset_data, f, ensure_ascii=False, indent=2)
                
                # numba ìµœì í™”ëœ íŒŒì¼ í¬ê¸° í™•ì¸
                file_size_mb = calculate_file_size_mb(os.path.getsize(output_path))
                print(f"âœ… {dataset['name']} ì €ì¥ ì™„ë£Œ!")
                print(f"ğŸ“Š {len(dataset_data['images']):,}ê°œ ì´ë¯¸ì§€")
                print(f"ğŸ“Š {len(dataset_data['annotations']):,}ê°œ ì–´ë…¸í…Œì´ì…˜")
                print(f"ğŸ“Š {len(dataset_data['categories'])}ê°œ ì¹´í…Œê³ ë¦¬")
                print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
                print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
                
                created_files.append(output_path)
                
            except Exception as e:
                print(f"âŒ {dataset['name']} íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()
    
    # ìƒì„±ëœ íŒŒì¼ ëª©ë¡ ë°˜í™˜
    print(f"\nğŸ‰ ëª¨ë“  ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for file_path in created_files:
        print(f"   - {file_path}")
    
    return created_files

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ JSON ë³‘í•© ë„êµ¬ (100000ê°œì”© ì¤‘ê°„ ì €ì¥ + Numba ìµœì í™”)")
    print("=" * 60)
    
    # gvfs FTP ê²½ë¡œ í™•ì¸
    if not setup_ftp_mount():
        print("âŒ gvfs FTP ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
        return
    
    # gvfs ê²½ë¡œ í™•ì¸
    if not os.path.exists(FTP_BASE_PATH):
        print("âŒ gvfs FTP ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
        return
    
    print("âœ… gvfs FTP ê²½ë¡œ í™•ì¸ ì™„ë£Œ")
    
    # ê° ë°ì´í„°ì…‹ë³„ JSON íŒŒì¼ ìƒì„±
    created_files = merge_json_datasets()
    
    if created_files:
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ìƒì„±ëœ JSON íŒŒì¼ë“¤: {len(created_files)}ê°œ")
        print("\nğŸ’¡ ì´ì œ ì´ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì—¬ LMDBë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ JSON íŒŒì¼ ìƒì„± ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
