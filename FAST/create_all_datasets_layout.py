#ê°€ì¥ì •ìƒ2512101700
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ëª¨ë“  í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ì„ train/validë¡œ ë¶„ë¦¬í•˜ì—¬ LMDB ìƒì„±í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸
ì „ì²´ ë°ì´í„°ì…‹ ë³€í™˜ (ì œí•œ ì—†ìŒ)
ë°ì´í„°ì…‹ë³„ ì „ìš© í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©ìœ¼ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„ 
"""

import os
import sys
import json
import pickle
import time
import numpy as np
import cv2
# import torch
from tqdm import tqdm
import lmdb
import queue
import random
import sqlite3
import gc
import subprocess
from pathlib import Path
import orjson
import ijson  # ìŠ¤íŠ¸ë¦¬ë° JSON íŒŒì‹±
# import bigjson  # ì œê±°ë¨ - orjson ì‚¬ìš©
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp
from tqdm.contrib.concurrent import process_map, thread_map
import psutil
import time
from io import BytesIO
try:
    from turbojpeg import TurboJPEG  # libjpeg-turbo ê°€ì† ì¸ì½”ë”
except Exception:
    TurboJPEG = None

# PIL for EXIF orientation handling (optional)
try:
    from PIL import Image, ImageOps
except Exception:
    Image = None
    ImageOps = None

# FAST ëª¨ë“ˆ import
sys.path.append('.')
sys.path.append('FAST')  # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ë“¤ì„ ìœ„í•œ ê²½ë¡œ
from dataset.fast.fast_lmdb import FAST_LMDB
from paddleocr import LayoutDetection  # LayoutDetection í†µí•©
try:
    from paddleocr import TableCellsDetection  # í…Œì´ë¸” ì…€ íƒì§€
except Exception:
    TableCellsDetection = None
try:
    import paddle  # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì§„ë‹¨ìš© (ì„ íƒ)
except Exception:
    paddle = None
try:
    from pyinpaint import Inpaint  # Inpaint ë§ˆìŠ¤í‚¹
except Exception:
    Inpaint = None
try:
    from paddleocr import PaddleOCR  # íšŒì „ íƒì§€(Angle CLS)
except Exception:
    PaddleOCR = None

# ğŸš€ ìµœì í™”ëœ lookup ë”•ì…”ë„ˆë¦¬ë“¤ (pickle ë°©ì‹)
optimized_lookups = {}

# bbox ë””ë²„ê·¸ ì¶œë ¥ í”Œë˜ê·¸ (ì „ì—­ ë³€ìˆ˜)
bbox_debug_flags = {
    'text_in_wild': False,
    'public_admin': False,
    'ocr_public': False,
    'finance_logistics': False,
    'handwriting': False
}

# ===== LayoutDetection ì „ì—­ ë¦¬ì†ŒìŠ¤ =====
LAYOUT_MODEL = None
LAYOUT_MODEL_LOCK = threading.Lock()
LAYOUT_LABELS_TO_USE = {'text', 'paragraph_title', 'figure_title', 'doc_title', 'vision_footnote', 'number', 'abstract', 'aside_text', 'reference_content','vertical_text', 'table'}
LAYOUT_THRESHOLD = 0.5
TABLE_THRESHOLD = 0.3
# í…Œì´ë¸” ë¼ë²¨ ê²€ì¶œì€ ë” ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
TABLE_LAYOUT_THRESHOLD = float(os.environ.get('FAST_TABLE_LAYOUT_THR', '0.3'))
LAYOUT_MODEL_NAME = os.environ.get('FAST_LAYOUT_MODEL', 'PP-DocLayoutV2')
LAYOUT_DEVICE = os.environ.get('FAST_LAYOUT_DEVICE', 'gpu').strip().lower()  # 'gpu' | 'cpu'
TABLE_DEVICE = os.environ.get('FAST_TABLE_DEVICE', 'gpu').strip().lower()    # 'gpu' | 'cpu'
GPU_ID = os.environ.get('FAST_GPU_ID', None)  # e.g., '0'

# ===== ë©”ëª¨ë¦¬ ì•ˆì „ ê°€ë“œ(ì½”ë“œ ê³ ì •ê°’) =====
# - ì…€ ê²€ì¶œìš© í…Œì´ë¸” í¬ë¡­ì˜ ìµœì¥ë³€ ìƒí•œ
CELL_CROP_MAX_SIDE = 1280
# - ì „ì—­ í…Œì´ë¸” ë°°ì²˜ íì˜ ìµœëŒ€ ëŒ€ê¸° í¬ë¡­ ìˆ˜
TABLE_AGG_MAX_PENDING = 64

# ===== ì „ì—­ ì˜ˆì¸¡ ìºì‹œ (GPU ì„ ê³„ì‚° ê²°ê³¼ ì €ì¥) =====
PREDICTION_CACHE = {}  # key: img_path -> {'layout': [...], 'tables': [...]]}
PRED_CACHE_LOCK = threading.Lock()
# ìºì‹œ ìƒí•œ (ë©”ëª¨ë¦¬ í­ì£¼ ë°©ì§€) - ê³ ì •ê°’ìœ¼ë¡œ ìš´ì˜
PRED_CACHE_MAX = 256

def _cache_update(img_path, layout=None, tables=None, table_cells=None):
    """ì „ì—­ ì˜ˆì¸¡ ìºì‹œì— ì•ˆì „í•˜ê²Œ ì“°ê³ , ìƒí•œì„ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ í•­ëª©ë¶€í„° ì œê±°."""
    try:
        with PRED_CACHE_LOCK:
            entry = PREDICTION_CACHE.get(img_path) or {}
            if layout is not None:
                entry['layout'] = layout
            if tables is not None:
                entry['tables'] = tables
            if table_cells is not None:
                entry['table_cells'] = table_cells
            # dictëŠ” ì‚½ì… ìˆœì„œ ë³´ì¡´(Python 3.7+)
            if img_path in PREDICTION_CACHE:
                # ì¬ì‚½ì…í•˜ì—¬ ìµœì‹ ìœ¼ë¡œ ê°±ì‹ 
                del PREDICTION_CACHE[img_path]
            PREDICTION_CACHE[img_path] = entry
            # ìƒí•œ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ í•­ëª©ë¶€í„° ì œê±°
            while len(PREDICTION_CACHE) > PRED_CACHE_MAX:
                try:
                    old_key = next(iter(PREDICTION_CACHE))
                    del PREDICTION_CACHE[old_key]
                except Exception:
                    break
    except Exception:
        pass

# ===== OpenCV ì“°ë ˆë“œ ìˆ˜ ì œí•œ(ê³¼ë„í•œ ì˜¤ë²„ì„œë¸ŒìŠ¤í¬ë¦½ì…˜ ë°©ì§€) =====
try:
    # OpenCV ë‚´ë¶€ ìŠ¤ë ˆë“œë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ì œí•œ(ë©”ëª¨ë¦¬ í”¼í¬ ì™„í™”)
    cv2.setNumThreads(2)
except Exception:
    pass

# ===== JPEG ì¸ì½”ë”© ê°€ì†/í’ˆì§ˆ ì„¤ì • =====
JPEG_QUALITY = int(os.environ.get("FAST_JPEG_QUALITY", "80"))
JPEG_OPTIMIZE = int(os.environ.get("FAST_JPEG_OPTIMIZE", "0"))
JPEG_PROGRESSIVE = int(os.environ.get("FAST_JPEG_PROGRESSIVE", "0"))
_jpeg = None
if TurboJPEG is not None:
    try:
        _jpeg = TurboJPEG()
    except Exception:
        _jpeg = None

def fast_encode_jpg(img):
    """
    ë¹ ë¥¸ JPEG ì¸ì½”ë”©:
    - turbojpeg ì‚¬ìš© ê°€ëŠ¥ ì‹œ turbojpegë¡œ ì¸ì½”ë”©
    - ê·¸ ì™¸ì—ëŠ” OpenCV imencode + ë‚®ì€ ì˜¤ë²„í—¤ë“œ ì˜µì…˜ ì‚¬ìš©
    ë°˜í™˜: (ok: bool, buf: bytes-like)
    """
    if _jpeg is not None:
        try:
            buf = _jpeg.encode(
                img,
                quality=JPEG_QUALITY,
                progressive=bool(JPEG_PROGRESSIVE)
            )
            return True, buf
        except Exception:
            pass
    # OpenCV ê²½ë¡œ
    try:
        flags = [
            int(cv2.IMWRITE_JPEG_QUALITY), int(max(1, min(100, JPEG_QUALITY))),
            int(cv2.IMWRITE_JPEG_PROGRESSIVE), int(bool(JPEG_PROGRESSIVE)),
            int(cv2.IMWRITE_JPEG_OPTIMIZE), int(bool(JPEG_OPTIMIZE)),
        ]
        ok, buf = cv2.imencode('.jpg', img, flags)
        return ok, bytes(buf) if ok else (False, None)
    except Exception:
        return False, None

# ===== ì „ì—­ GPU í”„ë¦¬í˜ì¹˜ ì›Œì»¤(ì§€ì† ì‹¤í–‰) =====
GPU_PREFETCH_QUEUE = None
GPU_PREFETCH_THREAD = None
GPU_PREFETCH_STOP = threading.Event()
GPU_PREFETCH_BATCH = int(os.environ.get("FAST_LAYOUT_BATCH", "64"))
GPU_PREFETCH_QUEUE_MAX = int(os.environ.get("FAST_PREFETCH_QUEUE", "4096"))
PREFETCH_TABLES = int(os.environ.get("FAST_PREFETCH_TABLES", "0"))  # 1ì´ë©´ í…Œì´ë¸” ì…€ê¹Œì§€ ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì¸¡

def _gpu_prefetch_worker():
    """ì „ì—­ íì—ì„œ ê²½ë¡œë¥¼ ë½‘ì•„ ë°°ì¹˜ ì˜ˆì¸¡ â†’ PREDICTION_CACHE ì €ì¥ì„ ì§€ì† ìˆ˜í–‰."""
    global GPU_PREFETCH_QUEUE
    try:
        model = get_layout_model()
    except Exception:
        model = None
    pending = []
    seen = set()
    while not GPU_PREFETCH_STOP.is_set():
        try:
            # íì—ì„œ ë¹ ë¥´ê²Œ ìµœëŒ€í•œ ëª¨ì•„ì„œ ë°°ì¹˜ êµ¬ì„±
            try:
                p = GPU_PREFETCH_QUEUE.get(timeout=0.05)
                if p and p not in seen and os.path.exists(p):
                    seen.add(p)
                    pending.append(p)
            except Exception:
                pass
            # ë°°ì¹˜ê°€ ì°¨ê±°ë‚˜, stop ìƒíƒœì—ì„œ ì”ì—¬ ì²˜ë¦¬
            if (len(pending) >= GPU_PREFETCH_BATCH) or (GPU_PREFETCH_STOP.is_set() and pending):
                batch = pending[:GPU_PREFETCH_BATCH]
                pending = pending[GPU_PREFETCH_BATCH:]
                # ì˜ˆì¸¡ í˜¸ì¶œ
                out_list = []
                if model and batch:
                    try:
                        with LAYOUT_MODEL_LOCK:
                            out_list = model.predict(batch, batch_size=len(batch), layout_nms=True, threshold=LAYOUT_THRESHOLD)
                    except Exception:
                        out_list = []
                # ìºì‹œì— ë°˜ì˜
                for pth, res in zip(batch, out_list or [None]*len(batch)):
                    boxes = []
                    try:
                        for b in getattr(res, 'boxes', []):
                            label = b.get('label')
                            coord = b.get('coordinate')
                            if label in LAYOUT_LABELS_TO_USE and isinstance(coord, (list, tuple)) and len(coord) == 4:
                                boxes.append({
                                    'label': label,
                                    'coordinate': [float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])],
                                    'score': float(b.get('score', 1.0))
                                })
                    except Exception:
                        boxes = []
                    tables = [b for b in boxes if isinstance(b.get('label'), str) and b.get('label').lower() == 'table']
                    _cache_update(pth, layout=boxes, tables=tables)
        except Exception:
            # ì›Œì»¤ëŠ” ì ˆëŒ€ ì£½ì§€ ì•Šë„ë¡ ëª¨ë“  ì˜ˆì™¸ ì‚¼í‚´
            pass
    # ë£¨í”„ ì¢…ë£Œ í›„ ì”ì—¬ ì²˜ë¦¬
    if pending:
        try:
            with LAYOUT_MODEL_LOCK:
                out_list = model.predict(pending, batch_size=len(pending), layout_nms=True, threshold=LAYOUT_THRESHOLD)
        except Exception:
            out_list = []
        for pth, res in zip(pending, out_list or [None]*len(pending)):
            boxes = []
            try:
                for b in getattr(res, 'boxes', []):
                    label = b.get('label')
                    coord = b.get('coordinate')
                    if label in LAYOUT_LABELS_TO_USE and isinstance(coord, (list, tuple)) and len(coord) == 4:
                        boxes.append({
                            'label': label,
                            'coordinate': [float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])],
                            'score': float(b.get('score', 1.0))
                        })
            except Exception:
                boxes = []
            tables = [b for b in boxes if isinstance(b.get('label'), str) and b.get('label').lower() == 'table']
            _cache_update(pth, layout=boxes, tables=tables)

def _layout_predict_batch_numpy(img_paths, threshold):
    """ì—¬ëŸ¬ ì´ë¯¸ì§€ 'ê²½ë¡œ ë¦¬ìŠ¤íŠ¸'ë¥¼ í•œ ë²ˆì— predictí•˜ê³  ìºì‹œì— ì €ì¥."""
    if not img_paths:
        return
    model = get_layout_model()
    _log_verbose(f"[layout/batch] start paths={len(img_paths)} thr={threshold}")
    # ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
    # ìœ íš¨ ê²½ë¡œë§Œ
    keeps = [p for p in img_paths if p and os.path.exists(p)]
    if not keeps:
        _log_verbose(f"[layout/batch] no valid paths")
        return
    # ë³´ìˆ˜ì  ë°°ì¹˜ í¬ê¸° ê³ ì •
    bs = min(len(keeps), 8)
    t0 = time.time()
    try:
        with LAYOUT_MODEL_LOCK:
            out_list = model.predict(keeps, batch_size=bs, layout_nms=True, threshold=threshold)
    except Exception as e:
        _log_verbose(f"[layout/batch] predict error: {e}")
        out_list = [None] * len(keeps)
    t1 = time.time()
    _log_verbose(f"[layout/batch] predict done: n={len(keeps)} bs={bs} ms={(t1-t0)*1000:.1f}")
    # ê²°ê³¼ êµ¬ì¡° ë””ë²„ê·¸ (ìƒìœ„ 3ê°œ)
    try:
        for i, r in enumerate(out_list[:3] or []):
            _debug_inspect_layout_result(r, tag=f"batch[{i}]")
    except Exception:
        pass
    # ìºì‹œ ë°˜ì˜
    total_boxes = 0
    total_tables = 0
    for p, res in zip(keeps, out_list or []):
        boxes = []
        try:
            raw_list = _extract_layout_boxes(res)
            for b in (raw_list or []):
                label = b.get('label')
                coord = b.get('coordinate')
                if isinstance(label, str) and label in LAYOUT_LABELS_TO_USE and isinstance(coord, (list, tuple)) and len(coord) == 4:
                    boxes.append({
                        'label': label,
                        'coordinate': [float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])],
                        'score': float(b.get('score', 1.0))
                    })
        except Exception:
            boxes = []
        tables = [b for b in boxes if isinstance(b.get('label'), str) and b.get('label').lower() == 'table']
        total_boxes += len(boxes)
        total_tables += len(tables)
        _cache_update(p, layout=boxes, tables=tables)
    _log_verbose(f"[layout/batch] cache saved: images={len(keeps)} boxes={total_boxes} tables={total_tables}")

def _start_gpu_prefetch_worker(batch_size=None):
    """GPU í”„ë¦¬í˜ì¹˜ ì›Œì»¤ë¥¼ 1íšŒë§Œ ê¸°ë™."""
    global GPU_PREFETCH_QUEUE, GPU_PREFETCH_THREAD, GPU_PREFETCH_BATCH
    if GPU_PREFETCH_THREAD is not None and GPU_PREFETCH_THREAD.is_alive():
        return
    if batch_size and isinstance(batch_size, int) and batch_size > 0:
        GPU_PREFETCH_BATCH = batch_size
    GPU_PREFETCH_STOP.clear()
    GPU_PREFETCH_QUEUE = queue.Queue(maxsize=GPU_PREFETCH_QUEUE_MAX)
    GPU_PREFETCH_THREAD = threading.Thread(target=_gpu_prefetch_worker, name="GPU-Prefetch-Worker", daemon=True)
    GPU_PREFETCH_THREAD.start()

def _stop_gpu_prefetch_worker():
    """GPU í”„ë¦¬í˜ì¹˜ ì›Œì»¤ ì¢…ë£Œ."""
    global GPU_PREFETCH_THREAD, GPU_PREFETCH_QUEUE
    try:
        GPU_PREFETCH_STOP.set()
        if GPU_PREFETCH_THREAD is not None:
            GPU_PREFETCH_THREAD.join(timeout=2.0)
    except Exception:
        pass
    finally:
        GPU_PREFETCH_THREAD = None
        GPU_PREFETCH_QUEUE = None

def _gpu_prefetch_enqueue(paths):
    """ê²½ë¡œë“¤ì„ ì „ì—­ íì— ë¹„ì°¨ë‹¨ìœ¼ë¡œ ë‹´ëŠ”ë‹¤(ì¤‘ë³µ í—ˆìš©, ì›Œì»¤ì—ì„œ ì œê±°)."""
    global GPU_PREFETCH_QUEUE
    if not paths:
        return
    if GPU_PREFETCH_QUEUE is None:
        return
    for p in paths:
        try:
            if p and os.path.exists(p):
                GPU_PREFETCH_QUEUE.put_nowait(p)
        except Exception:
            # íê°€ ê°€ë“ ì°¨ë©´ ë“œë(ë‹¤ìŒ ì²­í¬ì—ì„œ ë‹¤ì‹œ ì‹œë„ë  ê²ƒ)
            pass

# ===== íšŒì „(Angle) ê°ì§€ ë¦¬ì†ŒìŠ¤/ìºì‹œ =====
ROT_MODEL = None
ROT_MODEL_LOCK = threading.Lock()
ROTATION_CACHE = {}  # key: img_path -> angle (0/90/180/270 or float)
ROT_CACHE_LOCK = threading.Lock()

def get_rotation_model():
	"""íšŒì „ ê°ì§€ ë¹„í™œì„±í™”: ë” ì´ìƒ PaddleOCR ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ."""
	return None

def _parse_angle_from_ocr_result(one_output):
	"""PaddleOCR ê²°ê³¼ì—ì„œ doc_preprocessor_res.angle ì¶”ì¶œ. ì‹¤íŒ¨ ì‹œ 0 ë°˜í™˜."""
	try:
		dp = one_output.get('doc_preprocessor_res', {})
		angle = dp.get('angle', 0)
		if isinstance(angle, (int, float)):
			return int(angle) % 360
	except Exception:
		pass
	return 0

def _detect_rotation_batch(img_paths, batch_size=8):
	"""ë¹„í™œì„±í™”: íšŒì „ ê°ë„ ì„ ê³„ì‚° ì‚¬ìš© ì•ˆ í•¨."""
	return

def _prefetch_rotations_for_args(args_list, path_extractor, batch_size=8):
	"""args ë¦¬ìŠ¤íŠ¸ì—ì„œ ê²½ë¡œë¥¼ ì¶”ì¶œí•´ íšŒì „ ê°ë„ë¥¼ GPU ë°°ì¹˜ë¡œ ì„ ê³„ì‚°."""
	if not args_list or path_extractor is None:
		return
	paths = []
	for arg in args_list:
		try:
			p = path_extractor(arg)
		except Exception:
			p = None
		if not p or not os.path.exists(p):
			continue
		with ROT_CACHE_LOCK:
			if p in ROTATION_CACHE:
				continue
		paths.append(p)
	if not paths:
		return
	# ì…ë ¥ ìˆœì„œ ìœ ì§€í•œ ì¤‘ë³µ ì œê±°
	paths = list(dict.fromkeys(paths))
	_detect_rotation_batch(paths, batch_size=batch_size)

def _apply_rotation_if_needed(crop_img, original_img_path):
	"""íšŒì „ ë³´ì • ë¹„í™œì„±í™”(ë” ì´ìƒ PaddleOCR ê¸°ë°˜ íšŒì „ ê°ì§€ ì‚¬ìš© ì•ˆ í•¨)."""
	if crop_img is None or crop_img.size == 0:
		return crop_img
	return crop_img

# ===== Debug/Test ëª¨ë“œ (ìƒ˜í”Œ ì œí•œ ë° ì´ë¯¸ì§€/ë¼ë²¨ ì €ì¥) =====
# í™˜ê²½ë³€ìˆ˜ FAST_DEBUG ë˜ëŠ” DEBUG_MODE ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì°¸ì´ë©´ ë””ë²„ê·¸ ëª¨ë“œ
DEBUG_MODE = str(os.environ.get('FAST_DEBUG') or os.environ.get('DEBUG_MODE') or '0').lower() in ('1', 'true', 'yes', 'y')
DEBUG_SAMPLE_LIMIT = 500
STRICT_ID_ORDER = os.environ.get('FAST_STRICT_ID_ORDER', '1') == '1'  # ìœ„ì¹˜ ë¬´ì‹œ, id ìˆœì„œë§Œ ì‚¬ìš©
# ì§„í–‰ ë¡œê·¸ í† ê¸€: FAST_VERBOSEê°€ 1/true/yes/yë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
VERBOSE_LOG = str(os.environ.get('FAST_VERBOSE', '0')).lower() in ('1', 'true', 'yes', 'y')

def _log_verbose(msg):
    try:
        if VERBOSE_LOG:
            print(msg)
    except Exception:
        pass

def _inpaint_preserve_regions(crop_img, preserve_polys, feather_ratio=0.06, dilate_ratio=0.12):
    """
    preserve_polys: list[np.ndarray(N,2)] crop ì¢Œí‘œê³„ í´ë¦¬ê³¤ë“¤(ë³´ì¡´ ì˜ì—­)
    ë³´ì¡´ ì˜ì—­ ì™¸ë¥¼ ì¸í˜ì¸íŠ¸í•˜ê³ , ë³´ì¡´ ì˜ì—­ì€ feather ë¸”ë Œë”©ìœ¼ë¡œ ì›ë³¸ ìœ ì§€.
    """
    # ì¸í˜ì¸íŠ¸ ì „ì²´ ìŠ¤í‚µ ì˜µì…˜
    try:
        if int(os.environ.get("FAST_INPAINT", "1")) == 0:
            return crop_img
    except Exception:
        pass
    if crop_img is None or crop_img.size == 0:
        return crop_img
    Hc, Wc = crop_img.shape[:2]
    if Hc == 0 or Wc == 0:
        return crop_img
    mask = np.ones((Hc, Wc), dtype=np.uint8) * 255
    preserve = np.zeros((Hc, Wc), dtype=np.uint8)
    if preserve_polys:
        try:
            cv2.fillPoly(mask, preserve_polys, 0)
            cv2.fillPoly(preserve, preserve_polys, 1)
        except Exception:
            pass
        # í°íŠ¸ ê°€ì¥ìë¦¬ ë³´ì¡´ì„ ìœ„í•´ ì†Œí­ í™•ëŒ€ í›„ ë§ˆìŠ¤í¬ ë°˜ì „
        try:
            # ë³´ì¡´ í´ë¦¬ê³¤ í‰ê·  ë†’ì´ë¡œ íŒ½ì°½ ì»¤ë„ ê²°ì •
            hs = []
            for poly in preserve_polys:
                if poly.size == 0:
                    continue
                y1 = max(0, np.min(poly[:, 1])); y2 = min(Hc, np.max(poly[:, 1]))
                hs.append(max(1, int(y2 - y1)))
            median_h = float(np.median(hs)) if hs else 8.0
            dilate_px = int(max(2, round(dilate_ratio * median_h)))
            ksz = max(1, dilate_px * 2 + 1)
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksz, ksz))
            preserve_d = cv2.dilate(preserve, kernel, iterations=1)
            mask = np.where(preserve_d > 0, 0, 255).astype(np.uint8)
        except Exception:
            pass
    crop_orig = crop_img.copy()
    # ì¸í˜ì¸íŠ¸
    bg_restored = None
    if 'Inpaint' in globals() and Inpaint is not None:
        try:
            inp = Inpaint(crop_img, mask)
            out = inp()
            if out is not None and out.shape == crop_img.shape:
                bg_restored = out
        except Exception:
            bg_restored = None
    if bg_restored is None:
        try:
            mask_cv = (mask > 0).astype(np.uint8) * 255
            bg_restored = cv2.inpaint(crop_img, mask_cv, 3, cv2.INPAINT_TELEA)
        except Exception:
            bg_restored = None
    if bg_restored is None:
        return crop_img
    # ë³´ì¡´ ì˜ì—­ì€ ì›ë³¸, ë‚˜ë¨¸ì§€ëŠ” ì¸í˜ì¸íŠ¸ ê²°ê³¼ë¥¼ feather ë¸”ë Œë”©
    m = (mask == 0).astype(np.float32)
    try:
        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ìŠ¤í‚µ ì˜µì…˜
        if int(os.environ.get("FAST_GBLUR", "1")) != 0:
            feather_px = max(1, int(round(feather_ratio * (median_h if 'median_h' in locals() else 12.0))))
            kf = max(1, feather_px * 2 + 1)
            m = cv2.GaussianBlur(m, (kf, kf), 0)
    except Exception:
        pass
    if len(crop_img.shape) == 3:
        m3 = np.repeat(m[:, :, None], 3, axis=2)
    else:
        m3 = m
    blended = (crop_orig.astype(np.float32) * m3 + bg_restored.astype(np.float32) * (1.0 - m3)).clip(0, 255).astype(crop_orig.dtype)
    return blended

def _decode_image_bytes(img_bytes):
    """ë°”ì´íŠ¸ë¥¼ OpenCV ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©."""
    try:
        arr = np.frombuffer(img_bytes, dtype=np.uint8)
        img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
        return img
    except Exception:
        return None

def _assign_words_to_layout(word_aabbs, layout_aabbs, min_overlap_ratio=0.5):
    """ê° ë‹¨ì–´ë¥¼ ê°€ì¥ í¬ê²Œ ê²¹ì¹˜ëŠ” ë ˆì´ì•„ì›ƒì— í• ë‹¹. ë°˜í™˜: word_index -> layout_index (ë¯¸í• ë‹¹ì€ -1)."""
    assigned = [-1] * len(word_aabbs)
    for wi, wa in enumerate(word_aabbs):
        wa_area = _area(wa)
        if wa_area <= 0:
            continue
        best_idx = -1
        best_ratio = 0.0
        for li, la in enumerate(layout_aabbs):
            inter = _intersection_area(wa, la)
            ratio = inter / wa_area if wa_area > 0 else 0.0
            if ratio > best_ratio:
                best_ratio = ratio
                best_idx = li
        if best_idx >= 0 and best_ratio >= min_overlap_ratio:
            assigned[wi] = best_idx
    return assigned

 

def get_layout_model():
    """LayoutDetection ëª¨ë¸ ì‹±ê¸€í†¤ ì´ˆê¸°í™”/ë°˜í™˜."""
    global LAYOUT_MODEL
    if LAYOUT_MODEL is None:
        with LAYOUT_MODEL_LOCK:
            if LAYOUT_MODEL is None:
                # ë””ë°”ì´ìŠ¤ ê°•ì œ ì„¤ì • (í´ë°± ì—†ìŒ)
                if LAYOUT_DEVICE == 'gpu':
                    if paddle is None or not paddle.is_compiled_with_cuda():
                        raise RuntimeError("[layout] PaddlePaddle CUDA ë¹Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤ (í˜„ì¬ GPU ìš”ì²­).")
                    if GPU_ID is not None:
                        os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)
                        try:
                            paddle.device.set_device(f"gpu:{int(GPU_ID)}")
                        except Exception:
                            paddle.device.set_device("gpu")
                    else:
                        paddle.device.set_device("gpu")
                else:
                    if paddle is not None:
                        paddle.device.set_device("cpu")
                print(f"[layout] init LayoutDetection(model={LAYOUT_MODEL_NAME}, device={LAYOUT_DEVICE}, gpu_id={GPU_ID})")
                # ì¸ì í˜¸í™˜ì„±: ë””ë°”ì´ìŠ¤ëŠ” paddle ì „ì—­ ì„¤ì •ìœ¼ë¡œ ê°•ì œ
                LAYOUT_MODEL = LayoutDetection(model_name=LAYOUT_MODEL_NAME)
                try:
                    if paddle is not None:
                        print(f"[layout] current device: {paddle.device.get_device()}")
                except Exception:
                    pass
    return LAYOUT_MODEL

TABLE_MODEL = None
TABLE_MODEL_LOCK = threading.Lock()

PUBLIC_ADMIN_SQLITE_PATH = None
PUBLIC_ADMIN_SQLITE_CONN = None

def _get_public_admin_sqlite_conn(sqlite_path):
    """ê³µê³µí–‰ì •ë¬¸ì„œ ì–´ë…¸í…Œì´ì…˜ ì„ì‹œ SQLite ì»¤ë„¥ì…˜(ì‹±ê¸€í†¤)"""
    global PUBLIC_ADMIN_SQLITE_CONN, PUBLIC_ADMIN_SQLITE_PATH
    if PUBLIC_ADMIN_SQLITE_CONN is not None and PUBLIC_ADMIN_SQLITE_PATH == sqlite_path:
        return PUBLIC_ADMIN_SQLITE_CONN
    # ìƒˆë¡œ ì—´ê¸°
    try:
        conn = sqlite3.connect(sqlite_path, check_same_thread=False)
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=OFF;")
        PUBLIC_ADMIN_SQLITE_CONN = conn
        PUBLIC_ADMIN_SQLITE_PATH = sqlite_path
        return conn
    except Exception:
        return None

def get_table_model():
    """TableCellsDetection ëª¨ë¸ ì‹±ê¸€í†¤ ì´ˆê¸°í™”/ë°˜í™˜."""
    global TABLE_MODEL
    if TableCellsDetection is None:
        print("[table] TableCellsDetection ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…€ ê²€ì¶œì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        return None
    if TABLE_MODEL is None:
        with TABLE_MODEL_LOCK:
            if TABLE_MODEL is None:
                # ë””ë°”ì´ìŠ¤ ê°•ì œ ì„¤ì • (í´ë°± ì—†ìŒ)
                if TABLE_DEVICE == 'gpu':
                    if paddle is None or not paddle.is_compiled_with_cuda():
                        raise RuntimeError("[table] PaddlePaddle CUDA ë¹Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤ (í˜„ì¬ GPU ìš”ì²­).")
                    if GPU_ID is not None:
                        os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)
                        try:
                            paddle.device.set_device(f"gpu:{int(GPU_ID)}")
                        except Exception:
                            paddle.device.set_device("gpu")
                    else:
                        paddle.device.set_device("gpu")
                else:
                    if paddle is not None:
                        paddle.device.set_device("cpu")
                print(f"[table] init TableCellsDetection(model=RT-DETR-L_wired_table_cell_det, device={TABLE_DEVICE}, gpu_id={GPU_ID})")
                # ì¸ì í˜¸í™˜ì„±: ë””ë°”ì´ìŠ¤ëŠ” paddle ì „ì—­ ì„¤ì •ìœ¼ë¡œ ê°•ì œ
                TABLE_MODEL = TableCellsDetection(model_name="RT-DETR-L_wired_table_cell_det")
                try:
                    if paddle is not None:
                        print(f"[table] current device: {paddle.device.get_device()}")
                except Exception:
                    pass
    return TABLE_MODEL

def _to_flat8_from_xyxy(x1, y1, x2, y2):
    """ì‚¬ê°í˜• [x1,y1,x2,y2]ë¥¼ 8ì¢Œí‘œ(flat8)ë¡œ ë³€í™˜."""
    return [float(x1), float(y1), float(x2), float(y1), float(x2), float(y2), float(x1), float(y2)]

def _aabb_from_flat8(bflat8):
    """flat8 bboxì—ì„œ AABB(minx,miny,maxx,maxy)ë¡œ ë³€í™˜."""
    xs = bflat8[0::2]
    ys = bflat8[1::2]
    return float(min(xs)), float(min(ys)), float(max(xs)), float(max(ys))

def _rotate_flat8_180(bflat8, img_w, img_h):
    """ì´ë¯¸ì§€ ì¤‘ì‹¬ ê¸°ì¤€ 180ë„ íšŒì „ëœ flat8 ì¢Œí‘œ ë°˜í™˜ (ì¶• ì •ë ¬ ë°•ìŠ¤ ê°€ì •)."""
    x1, y1, x2, y2 = _aabb_from_flat8(bflat8)
    rx1 = float(max(0.0, min(img_w, img_w - x2)))
    ry1 = float(max(0.0, min(img_h, img_h - y2)))
    rx2 = float(max(0.0, min(img_w, img_w - x1)))
    ry2 = float(max(0.0, min(img_h, img_h - y1)))
    return [rx1, ry1, rx2, ry1, rx2, ry2, rx1, ry2]

def _rotate_flat8_90_cw(bflat8, img_w, img_h):
    """ì´ë¯¸ì§€ ê¸°ì¤€ 90ë„ ì‹œê³„ íšŒì „ flat8 ë³€í™˜. ì…ë ¥ì€ ì›ë³¸(W,H), ì¶œë ¥ ì¢Œí‘œê³„ëŠ” (H,W)."""
    # ì ë“¤ ìƒì„±
    pts = [
        (bflat8[0], bflat8[1]),
        (bflat8[2], bflat8[3]),
        (bflat8[4], bflat8[5]),
        (bflat8[6], bflat8[7]),
    ]
    # (x', y') = (H - y, x)
    pts_r = [(float(img_h - y), float(x)) for (x, y) in pts]
    xs = [p[0] for p in pts_r]
    ys = [p[1] for p in pts_r]
    rx1, ry1, rx2, ry2 = min(xs), min(ys), max(xs), max(ys)
    # ì¶œë ¥ ì¢Œí‘œê³„ëŠ” (H, W)
    rx1 = max(0.0, min(img_h, rx1)); rx2 = max(0.0, min(img_h, rx2))
    ry1 = max(0.0, min(img_w, ry1)); ry2 = max(0.0, min(img_w, ry2))
    return [rx1, ry1, rx2, ry1, rx2, ry2, rx1, ry2]

def _rotate_flat8_270_cw(bflat8, img_w, img_h):
    """ì´ë¯¸ì§€ ê¸°ì¤€ 270ë„ ì‹œê³„(=90ë„ ë°˜ì‹œê³„) íšŒì „ flat8 ë³€í™˜. ì…ë ¥ì€ ì›ë³¸(W,H), ì¶œë ¥ ì¢Œí‘œê³„ëŠ” (H,W)."""
    # (x', y') = (y, W - x)
    pts = [
        (bflat8[0], bflat8[1]),
        (bflat8[2], bflat8[3]),
        (bflat8[4], bflat8[5]),
        (bflat8[6], bflat8[7]),
    ]
    pts_r = [(float(y), float(img_w - x)) for (x, y) in pts]
    xs = [p[0] for p in pts_r]
    ys = [p[1] for p in pts_r]
    rx1, ry1, rx2, ry2 = min(xs), min(ys), max(xs), max(ys)
    rx1 = max(0.0, min(img_h, rx1)); rx2 = max(0.0, min(img_h, rx2))
    ry1 = max(0.0, min(img_w, ry1)); ry2 = max(0.0, min(img_w, ry2))
    return [rx1, ry1, rx2, ry1, rx2, ry2, rx1, ry2]

def _intersection_area(a, b):
    """ë‘ AABBì˜ êµì°¨ ë©´ì ."""
    ax1, ay1, ax2, ay2 = a
    bx1, by1, bx2, by2 = b
    ix1 = max(ax1, bx1)
    iy1 = max(ay1, by1)
    ix2 = min(ax2, bx2)
    iy2 = min(ay2, by2)
    w = max(0.0, ix2 - ix1)
    h = max(0.0, iy2 - iy1)
    return w * h

def _area(aabb):
    """AABB ë©´ì ."""
    x1, y1, x2, y2 = aabb
    return max(0.0, x2 - x1) * max(0.0, y2 - y1)

def _extract_layout_boxes(res):
    """
    LayoutDetection ê²°ê³¼ ê°ì²´ì—ì„œ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í‘œì¤€í™”í•´ ì¶”ì¶œ.
    ì§€ì› í‚¤: boxes | result | preds | predictions | bbox/box/coordinate/points
    ë°˜í™˜: [{'label': str, 'coordinate': [x1,y1,x2,y2], 'score': float}, ...]
    """
    candidates = []
    try:
        # dict í˜•íƒœ
        if isinstance(res, dict):
            for k in ('boxes', 'result', 'preds', 'predictions'):
                v = res.get(k)
                if isinstance(v, (list, tuple)):
                    candidates = v
                    break
        else:
            # ê°ì²´ ì†ì„±
            for k in ('boxes', 'result', 'preds', 'predictions'):
                v = getattr(res, k, None)
                if isinstance(v, (list, tuple)):
                    candidates = v
                    break
    except Exception:
        candidates = []
    norm = []
    for b in candidates or []:
        try:
            if isinstance(b, dict):
                label = b.get('label')
                coord = b.get('coordinate')
                if coord is None:
                    coord = b.get('bbox') or b.get('box')
                if coord is None:
                    pts = b.get('points') or b.get('poly') or b.get('polygon')
                    if isinstance(pts, (list, tuple)) and len(pts) >= 4:
                        xs = [p[0] for p in pts[:4]]
                        ys = [p[1] for p in pts[:4]]
                        coord = [min(xs), min(ys), max(xs), max(ys)]
                score = b.get('score', b.get('confidence', 1.0))
            else:
                label = getattr(b, 'label', None)
                coord = getattr(b, 'coordinate', None)
                if coord is None:
                    coord = getattr(b, 'bbox', None) or getattr(b, 'box', None)
                score = getattr(b, 'score', getattr(b, 'confidence', 1.0))
            if isinstance(coord, (list, tuple)) and len(coord) >= 4:
                x1, y1, x2, y2 = float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])
                norm.append({'label': label, 'coordinate': [x1, y1, x2, y2], 'score': float(score if isinstance(score, (int, float)) else 1.0)})
        except Exception:
            continue
    return norm
def _extract_cell_boxes(res):
    """
    TableCellsDetection ê²°ê³¼ ê°ì²´ì—ì„œ 'cell' ë°•ìŠ¤ë§Œ í‘œì¤€í™”í•´ ì¶”ì¶œ.
    ì§€ì› í‚¤: boxes | result | preds | predictions | bbox/box/coordinate/points
    ë°˜í™˜: [{'label': 'cell', 'coordinate': [x1,y1,x2,y2], 'score': float}, ...]
    """
    candidates = []
    try:
        if isinstance(res, dict):
            for k in ('boxes', 'result', 'preds', 'predictions'):
                v = res.get(k)
                if isinstance(v, (list, tuple)):
                    candidates = v
                    break
        else:
            for k in ('boxes', 'result', 'preds', 'predictions'):
                v = getattr(res, k, None)
                if isinstance(v, (list, tuple)):
                    candidates = v
                    break
    except Exception:
        candidates = []
    norm = []
    labels_count = {}
    for b in candidates or []:
        try:
            if isinstance(b, dict):
                label = b.get('label')
                coord = b.get('coordinate') or b.get('bbox') or b.get('box')
                if coord is None:
                    pts = b.get('points') or b.get('poly') or b.get('polygon')
                    if isinstance(pts, (list, tuple)) and len(pts) >= 4:
                        xs = [p[0] for p in pts[:4]]
                        ys = [p[1] for p in pts[:4]]
                        coord = [min(xs), min(ys), max(xs), max(ys)]
                score = b.get('score', b.get('confidence', 1.0))
            else:
                label = getattr(b, 'label', None)
                coord = getattr(b, 'coordinate', None) or getattr(b, 'bbox', None) or getattr(b, 'box', None)
                score = getattr(b, 'score', getattr(b, 'confidence', 1.0))
            try:
                labels_count[label] = labels_count.get(label, 0) + 1
            except Exception:
                pass
            if isinstance(coord, (list, tuple)) and len(coord) >= 4:
                x1, y1, x2, y2 = float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])
                # ë¼ë²¨ì´ ë¬¸ìì—´ì´ê³  'cell' í¬í•¨ì‹œë§Œ ì‚¬ìš©. ë¼ë²¨ì´ None/ìˆ«ìì¸ ê²½ìš°ëŠ” ì œì™¸(ë””ë²„ê·¸ë¡œ í™•ì¸)
                if isinstance(label, str) and 'cell' in label.lower():
                    norm.append({'label': 'cell', 'coordinate': [x1, y1, x2, y2], 'score': float(score if isinstance(score, (int, float)) else 1.0)})
        except Exception:
            continue
    # ë¼ë²¨ ë¶„í¬ ë””ë²„ê¹…
    try:
        print(f"[debug] cell_extract: labels={labels_count} kept={len(norm)}")
    except Exception:
        pass
    return norm
def _sort_word_indices_by_reading_order(word_aabbs):
    """ì¢Œ->ìš°, ìƒ->í•˜ ê°„ë‹¨ ì •ë ¬ í‚¤ë¡œ ì¸ë±ìŠ¤ ë°˜í™˜."""
    # y ìš°ì„ (ìƒë‹¨), ë‹¤ìŒ x
    indices = list(range(len(word_aabbs)))
    indices.sort(key=lambda i: (word_aabbs[i][1], word_aabbs[i][0]))
    return indices

def _debug_inspect_layout_result(res, tag=""):
    """
    LayoutDetection ë‹¨ì¼ ê²°ê³¼ ê°ì²´ êµ¬ì¡° ê°„ë‹¨ ì ê²€ìš© ë””ë²„ê·¸.
    - íƒ€ì…
    - dict í‚¤ / ëŒ€í‘œ í‚¤ë“¤ì˜ ê¸¸ì´
    - ê°ì²´ ì†ì„± ì¡´ì¬ ì—¬ë¶€
    """
    try:
        info = {'tag': tag, 'type': type(res).__name__}
        if isinstance(res, dict):
            keys = list(res.keys())
            info['dict_keys'] = keys[:16]
            for k in ('boxes', 'result', 'preds', 'predictions'):
                v = res.get(k, None)
                info[f'len_{k}'] = (len(v) if isinstance(v, (list, tuple)) else (-1 if v is None else 1))
        else:
            for k in ('boxes', 'result', 'preds', 'predictions'):
                v = getattr(res, k, None)
                info[f'len_{k}'] = (len(v) if isinstance(v, (list, tuple)) else (-1 if v is None else 1))
        _log_verbose(f"[layout/debug] {info}")
    except Exception:
        pass

def run_layout_detection(img_path):
    """ì´ë¯¸ì§€ ê²½ë¡œë¡œ LayoutDetection ìˆ˜í–‰í•˜ê³ , í•„ìš”í•œ ë¼ë²¨ì˜ [x1,y1,x2,y2] ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    try:
        # ìºì‹œ ìš°ì„  ì‚¬ìš©
        try:
            with PRED_CACHE_LOCK:
                cached = PREDICTION_CACHE.get(img_path)
                if cached and 'layout' in cached:
                    return cached.get('layout') or []
        except Exception:
            pass
        model = get_layout_model()
        _log_verbose(f"[layout] call predict: path={img_path} thr={LAYOUT_THRESHOLD}")
        t0 = time.time()
        with LAYOUT_MODEL_LOCK:
            # ê²½ë¡œ ë¬¸ìì—´(ë˜ëŠ” ê²½ë¡œ ë¦¬ìŠ¤íŠ¸)ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
            output = model.predict(img_path, batch_size=8, layout_nms=True, threshold=LAYOUT_THRESHOLD)
        t1 = time.time()
        _log_verbose(f"[layout] predict(ms)={(t1-t0)*1000:.1f} path={os.path.basename(img_path)}")
        if not output:
            _log_verbose(f"[layout] result: 0 boxes")
            return []
        res = output[0]
        # ê²°ê³¼ íŒŒì‹±
        boxes = []
        try:
            raw_boxes = _extract_layout_boxes(res)
            labels_count = {}
            for b in raw_boxes:
                label = b.get('label')
                coord = b.get('coordinate')
                try:
                    labels_count[label] = labels_count.get(label, 0) + 1
                except Exception:
                    pass
                if label in LAYOUT_LABELS_TO_USE and isinstance(coord, (list, tuple)) and len(coord) == 4:
                    boxes.append({'label': label, 'coordinate': [float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])], 'score': float(b.get('score', 1.0))})
            _log_verbose(f"[layout] result: boxes_total={len(raw_boxes)} boxes_used={len(boxes)} labels={labels_count}")
            if boxes:
                _cache_update(img_path, layout=boxes)
                return boxes
        except Exception:
            pass
    except Exception:
        _log_verbose(f"[layout] exception in predict for {img_path}")
        return []

def run_layout_tables(img_path):
    """ì´ë¯¸ì§€ì—ì„œ layout ê²°ê³¼ ì¤‘ table ë¼ë²¨ë§Œ ë°˜í™˜."""
    try:
        print(f"[debug] run_layout_tables: path={img_path}")
        # ìºì‹œ ìš°ì„  ì‚¬ìš©
        try:
            with PRED_CACHE_LOCK:
                cached = PREDICTION_CACHE.get(img_path)
                if cached and 'tables' in cached:
                    # ìºì‹œê°€ ì¡´ì¬í•˜ë©´ ë¹„ì–´ ìˆì–´ë„ ì¬ì¶”ë¡ í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
                    tables_cached = cached.get('tables') or []
                    print(f"[debug] run_layout_tables: cached tables={len(tables_cached)}")
                    return tables_cached
        except Exception:
            pass
        model = get_layout_model()
        _log_verbose(f"[layout] call predict(for table): path={img_path} thr={TABLE_LAYOUT_THRESHOLD}")
        with LAYOUT_MODEL_LOCK:
            output = model.predict(img_path, batch_size=8, layout_nms=True, threshold=TABLE_LAYOUT_THRESHOLD)
        if not output or not output[0]:
            _log_verbose(f"[layout] table result: 0 boxes")
            print(f"[debug] run_layout_tables: model returned 0")
            return []
        res = output[0]
        tables = []
        try:
            raw_boxes = _extract_layout_boxes(res)
            print(f"[debug] run_layout_tables: raw_boxes={len(raw_boxes)}")
            labels_count = {}
            for b in raw_boxes:
                label = b.get('label')
                try:
                    labels_count[label] = labels_count.get(label, 0) + 1
                except Exception:
                    pass
                coord = b.get('coordinate')
                if isinstance(label, str) and label.lower() == 'table' and isinstance(coord, (list, tuple)) and len(coord) == 4:
                    tables.append({'label': label, 'coordinate': [float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])], 'score': float(b.get('score', 1.0))})
            _log_verbose(f"[layout] table result: total={len(raw_boxes)} tables={len(tables)} labels={labels_count}")
            print(f"[debug] run_layout_tables: tables={len(tables)} sample={tables[:3] if tables else []}")
            _cache_update(img_path, tables=tables)
            return tables
        except Exception:
            pass
        # JSON í´ë°± ì™„ì „ ì œê±°
    except Exception:
        _log_verbose(f"[layout] exception in predict for {img_path}")
        return []

def _flat8_to_crop_poly(flat8, crop_x1, crop_y1, crop_x2, crop_y2):
    """ë‹¨ì–´ flat8ì„ í¬ë¡­ ì¢Œí‘œê³„ í´ë¦¬ê³¤(np.int32)ë¡œ ë³€í™˜. í¬ë¡­ ë°–ì€ ì˜ë¦¼."""
    try:
        xs = [float(flat8[0]), float(flat8[2]), float(flat8[4]), float(flat8[6])]
        ys = [float(flat8[1]), float(flat8[3]), float(flat8[5]), float(flat8[7])]
        poly = []
        for i in range(4):
            xi = int(round(xs[i] - crop_x1))
            yi = int(round(ys[i] - crop_y1))
            poly.append([xi, yi])
        poly = np.array(poly, dtype=np.int32)
        Hc = max(0, int(crop_y2 - crop_y1))
        Wc = max(0, int(crop_x2 - crop_x1))
        if Hc <= 0 or Wc <= 0:
            return None
        poly[:, 0] = np.clip(poly[:, 0], 0, Wc)
        poly[:, 1] = np.clip(poly[:, 1], 0, Hc)
        return poly
    except Exception:
        return None

def merge_words_by_layout(bboxes_flat8, words, layout_boxes, word_ids=None, prefer_id_order=False, word_orients=None):
    """
    ë‹¨ì–´ ë‹¨ìœ„ ë¼ë²¨(bboxes_flat8/words)ì„ LayoutDetection ë°•ìŠ¤ë³„ ë¬¸ì¥ìœ¼ë¡œ ë³‘í•©.
    layout_boxes: [{'label': str, 'coordinate': [x1,y1,x2,y2], 'score': float}, ...]
    ë°˜í™˜: (merged_bboxes_flat8, merged_texts)
    """
    if not bboxes_flat8 or not words or not layout_boxes:
        return [], []
    word_aabbs = [_aabb_from_flat8(b) for b in bboxes_flat8]
    assigned = [-1] * len(words)  # word -> layout idx

    layout_aabbs = []
    for lb in layout_boxes:
        x1, y1, x2, y2 = lb['coordinate']
        layout_aabbs.append((float(x1), float(y1), float(x2), float(y2)))

    # ê° ë‹¨ì–´ë¥¼ ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” ë ˆì´ì•„ì›ƒ ë°•ìŠ¤ì— í• ë‹¹ (ë‹¨ì–´ ë©´ì  ëŒ€ë¹„ 0.5 ì´ìƒ ê²¹ì¹˜ë©´ í• ë‹¹)
    for wi, wa in enumerate(word_aabbs):
        best_idx = -1
        best_ratio = 0.0
        wa_area = _area(wa)
        if wa_area <= 0.0:
            continue
        for li, la in enumerate(layout_aabbs):
            inter = _intersection_area(wa, la)
            ratio = inter / wa_area if wa_area > 0 else 0.0
            if ratio > best_ratio:
                best_ratio = ratio
                best_idx = li
        if best_idx >= 0 and best_ratio >= 0.5:
            assigned[wi] = best_idx

    # ë ˆì´ì•„ì›ƒ ë°•ìŠ¤ë³„ë¡œ ë‹¨ì–´ ìˆ˜ì§‘ í›„ ì •ë ¬/ë³‘í•©
    merged_bboxes = []
    merged_texts = []
    def _id_key(i):
        if word_ids is None:
            return i
        try:
            return int(str(word_ids[i]))
        except Exception:
            return str(word_ids[i])
    def _compose_multiline_sentence(idxs):
        """ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°œí–‰ í¬í•¨ ë¬¸ì¥ìœ¼ë¡œ ê²°í•©."""
        if not idxs:
            return ""
        # word_ud/word_duê°€ í¬í•¨ë˜ì—ˆê±°ë‚˜, ë ˆì´ì•„ì›ƒì´ ì„¸ë¡œ ì§€ë°°ì (ë†’ì´/ë„ˆë¹„ ë¹„ â‰¥ 2)ì¸ ê²½ìš°: ì»¬ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§
        force_column = False
        has_orient = False
        if word_orients is not None:
            try:
                has_orient = any(((i < len(word_orients)) and (word_orients[i] in ('ud', 'du'))) for i in idxs)
            except Exception:
                has_orient = False
        if not has_orient:
            # ì„¸ë¡œ ì§€ë°°ë„ íœ´ë¦¬ìŠ¤í‹±
            try:
                xs = []; ys = []
                for j in idxs:
                    x1j, y1j, x2j, y2j = word_aabbs[j]
                    xs += [x1j, x2j]; ys += [y1j, y2j]
                span_x = max(xs) - min(xs) if xs else 0.0
                span_y = max(ys) - min(ys) if ys else 0.0
                force_column = (span_y > 0 and span_x > 0 and (span_y / max(1.0, span_x) >= 2.0))
            except Exception:
                force_column = False
        if has_orient or force_column:
            # xì¤‘ì‹¬/í­ ê¸°ë°˜ ì»¬ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§
            widths = []
            xcenters = {}
            ycenters = {}
            for j in idxs:
                x1j, y1j, x2j, y2j = word_aabbs[j]
                widths.append(max(1.0, abs(x2j - x1j)))
                xcenters[j] = (x1j + x2j) * 0.5
                ycenters[j] = (y1j + y2j) * 0.5
            median_w = float(np.median(widths)) if widths else 8.0
            x_thresh = max(10.0, 0.8 * median_w)
            # x ì •ë ¬ í›„ ì»¬ëŸ¼ ë¬¶ê¸°
            order_x = sorted(idxs, key=lambda i: xcenters[i])
            columns = []
            cur = []
            basex = None
            for j in order_x:
                cx = xcenters[j]
                if basex is None:
                    basex = cx; cur = [j]
                else:
                    if abs(cx - basex) > x_thresh:
                        if cur:
                            columns.append(cur)
                        basex = cx; cur = [j]
                    else:
                        cur.append(j)
            if cur:
                columns.append(cur)
            # ê° ì»¬ëŸ¼ ë‚´ë¶€ ë°©í–¥ì„± ê²°ì • ë° y ì •ë ¬
            col_strings = []
            for col in columns:
                if word_orients is not None:
                    cnt_ud_c = sum(1 for j in col if (j < len(word_orients) and word_orients[j] == 'ud'))
                    cnt_du_c = sum(1 for j in col if (j < len(word_orients) and word_orients[j] == 'du'))
                else:
                    cnt_ud_c = cnt_du_c = 0
                reverse_c = (cnt_ud_c > cnt_du_c)  # Trueë©´ ì•„ë˜â†’ìœ„
                col_sorted = sorted(col, key=lambda j: ycenters[j], reverse=reverse_c)
                col_str = " ".join(str(words[t]) if words[t] is not None else "" for t in col_sorted).strip()
                if col_str:
                    col_strings.append(col_str)
            return "\n".join(col_strings).strip()
        # ì—„ê²© ID ìˆœì„œ ëª¨ë“œ: ID ìˆœì„œë¥¼ ë”°ë¥´ë˜, yì¶• ë³€ë™ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ìœ¼ë©´ ê°œí–‰
        if prefer_id_order and STRICT_ID_ORDER:
            # 1) ìš°ì„  yì¶• ê¸°ì¤€ìœ¼ë¡œ ë¼ì¸ í´ëŸ¬ìŠ¤í„°ë§ â†’ ì¤„ ë‹¨ìœ„ ì•ˆì •í™”
            stats = []
            for j in idxs:
                x1i = word_aabbs[j][0]
                y1i = word_aabbs[j][1]
                x2i = word_aabbs[j][2]
                y2i = word_aabbs[j][3]
                cx = (x1i + x2i) / 2.0
                cy = (y1i + y2i) / 2.0
                h = max(1.0, y2i - y1i)
                w = max(1.0, x2i - x1i)
                stats.append((j, cx, cy, h, w))
            if not stats:
                return ""
            median_h = float(np.median([s[3] for s in stats])) if stats else 8.0
            median_w = float(np.median([s[4] for s in stats])) if stats else 12.0
            y_thresh = max(8.0, 0.6 * median_h)
            x_thresh = max(8.0, 0.6 * median_w)
            # y ì¤‘ì‹¬ìœ¼ë¡œ ì •ë ¬ í›„ ë¼ì¸ í´ëŸ¬ìŠ¤í„°
            stats.sort(key=lambda t: t[2])
            lines_idx = []
            cur = []
            cur_y = stats[0][2]
            for j, cx, cy, h, w in stats:
                if cur and abs(cy - cur_y) > y_thresh:
                    lines_idx.append(cur)
                    cur = [j]
                    cur_y = cy
                else:
                    cur.append(j)
                    # y ê¸°ì¤€ì„ ì€ ê³¼ë„í•˜ê²Œ ì´ë™í•˜ì§€ ì•Šê²Œ ì™„ë§Œíˆ ì—…ë°ì´íŠ¸
                    cur_y = 0.7 * cur_y + 0.3 * cy
            if cur:
                lines_idx.append(cur)
            # 2) ê° ë¼ì¸ ë‚´ì—ì„œëŠ” ì¢Œâ†’ìš° ìš°ì„ , IDëŠ” ë³´ì¡° í‚¤ë¡œ ì•ˆì •í™”
            out_lines = []
            for line in lines_idx:
                # ì™¼ìª½ìœ¼ë¡œ í° ë©ì–´ë¼ìš´ë“œê°€ ë°œìƒí•˜ë©´(AND ì¡°ê±´) ì„œë¸Œ ë¼ì¸ ë¶„í• 
                sub_lines = []
                sub = []
                # ID ìˆœìœ¼ë¡œ ë¨¼ì € í›‘ì–´ë³´ë©° ë©ì–´ë¼ìš´ë“œ ê°ì§€
                for j in sorted(line, key=lambda t: _id_key(t)):
                    cx = (word_aabbs[j][0] + word_aabbs[j][2]) / 2.0
                    cy = (word_aabbs[j][1] + word_aabbs[j][3]) / 2.0
                    if sub:
                        prev = sub[-1]
                        prev_cx = (word_aabbs[prev][0] + word_aabbs[prev][2]) / 2.0
                        basey = np.mean([(word_aabbs[k][1] + word_aabbs[k][3]) / 2.0 for k in sub])
                        large_y = abs(cy - basey) > y_thresh
                        large_x_wrap = (cx < prev_cx - x_thresh) and (abs(cy - basey) > 0.25 * median_h)
                        if large_y and large_x_wrap:
                            sub_lines.append(sub)
                            sub = [j]
                            continue
                    sub.append(j)
                if sub:
                    sub_lines.append(sub)
                # ìµœì¢…: ê° ì„œë¸Œë¼ì¸ì€ x ìš°ì„ , id ë³´ì¡°ë¡œ ì •ë ¬ í›„ í•©ì¹˜ê¸°
                for sline in sub_lines:
                    sline.sort(key=lambda t: (((word_aabbs[t][0] + word_aabbs[t][2]) / 2.0), _id_key(t)))
                    out_lines.append(" ".join(str(words[t]) if words[t] is not None else "" for t in sline).strip())
            return "\n".join([ln for ln in out_lines if ln])
        # ë‹¨ì–´ ì¤‘ì‹¬/ë†’ì´/ê°€ë¡œ ì¤‘ì‹¬ ê³„ì‚°
        stats = []
        for i in idxs:
            x1, y1, x2, y2 = word_aabbs[i][0], word_aabbs[i][1], word_aabbs[i][2], word_aabbs[i][3]
            cx = (x1 + x2) / 2.0
            cy = (y1 + y2) / 2.0
            h = max(1.0, y2 - y1)
            stats.append((i, cx, cy, h))
        if not stats:
            return ""
        median_h = float(np.median([s[3] for s in stats]))
        y_thresh = max(8.0, 0.6 * median_h)
        # ìš°ì„  yì¤‘ì‹¬ìœ¼ë¡œ ì •ë ¬ í›„ ë¼ì¸ í´ëŸ¬ìŠ¤í„°ë§
        stats.sort(key=lambda t: t[2])  # by cy
        lines_idx = []
        cur_line = []
        if stats:
            cur_y = stats[0][2]
        else:
            cur_y = 0.0
        for i, cx, cy, h in stats:
            if cur_line and abs(cy - cur_y) > y_thresh:
                lines_idx.append(cur_line)
                cur_line = [i]
                cur_y = cy
            else:
                cur_line.append(i)
                cur_y = 0.7 * cur_y + 0.3 * cy
        if cur_line:
            lines_idx.append(cur_line)
        # ë¼ì¸ ìˆœì„œ ì•ˆì •í™”: ë¼ì¸ì˜ í‰ê·  yë¡œ ì •ë ¬
        def line_key(line):
            ys = []
            for j in line:
                ys.append((word_aabbs[j][1] + word_aabbs[j][3]) / 2.0)
            return np.mean(ys) if ys else 0.0
        lines_idx.sort(key=line_key)
        # ê° ë¼ì¸ ë‚´ ì •ë ¬: ì¢Œâ†’ìš° ìš°ì„ , í•„ìš”ì‹œ idë¡œ ë³´ì¡° ì•ˆì •í™”
        out_lines = []
        for line in lines_idx:
            # ì¢Œ->ìš° ì •ë ¬
            line.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0))
            if prefer_id_order and word_ids is not None:
                # ê°™ì€ x ê·¼ì²˜ì¼ ë•Œ id ìˆœì„œë¥¼ ë³´ì¡°ë¡œ ì‚¬ìš© (ì•ˆì •í™”)
                line.sort(key=lambda j: (_id_key(j)))
                # ìµœì¢…ì ìœ¼ë¡œ x ìš°ì„ , id ë³´ì¡°ì˜ ì•ˆì •í™” ì •ë ¬
                line.sort(key=lambda j: (((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0), _id_key(j)))
            out_lines.append(" ".join(str(words[j]) if words[j] is not None else "" for j in line).strip())
        return "\n".join([ln for ln in out_lines if ln])
    for li, la in enumerate(layout_aabbs):
        word_indices = [i for i, a in enumerate(assigned) if a == li]
        if not word_indices:
            continue
        # ì •ë ¬: id ìš°ì„  ì˜µì…˜, ì—†ìœ¼ë©´ ì¢Œ->ìš° ìƒ->í•˜
        text = _compose_multiline_sentence(word_indices)
        if not text:
            continue
        x1, y1, x2, y2 = la
        flat8 = _to_flat8_from_xyxy(x1, y1, x2, y2)
        flat8 = normalize_ic15_clockwise_flat8(flat8)
        merged_bboxes.append(flat8)
        merged_texts.append(text)

    # ë ˆì´ì•„ì›ƒì— í• ë‹¹ë˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´
    for i, a in enumerate(assigned):
        if a == -1:
            merged_bboxes.append(bboxes_flat8[i])
            merged_texts.append(words[i])

    return merged_bboxes, merged_texts

def _order_points_clockwise(points: np.ndarray) -> np.ndarray:
    """ì‚¬ê°í˜• 4ì ì„ TL, TR, BR, BL ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ë ¬í•œë‹¤."""
    if points.shape != (4, 2):
        points = points.reshape(-1, 2)[:4]
    s = points.sum(axis=1)
    diff = np.diff(points, axis=1).reshape(-1)
    tl = points[np.argmin(s)]
    br = points[np.argmax(s)]
    tr = points[np.argmin(diff)]
    bl = points[np.argmax(diff)]
    return np.array([tl, tr, br, bl], dtype=np.float32)

def normalize_ic15_clockwise_flat8(bbox_flat8):
    """[x1,y1,x2,y2,x3,y3,x4,y4]ì„ IC15 í‘œì¤€ ìˆœì„œ(TL,TR,BR,BL)ë¡œ ì •ê·œí™”í•œë‹¤."""
    try:
        if not isinstance(bbox_flat8, (list, tuple)) or len(bbox_flat8) != 8:
            return bbox_flat8
        pts = np.array(bbox_flat8, dtype=np.float32).reshape(-1, 2)
        ordered = _order_points_clockwise(pts)
        return ordered.reshape(-1).astype(float).tolist()
    except Exception:
        return bbox_flat8

def load_optimized_lookup(dataset_name):
    """ìµœì í™”ëœ lookup ë”•ì…”ë„ˆë¦¬ë¥¼ pickleì—ì„œ ë¡œë“œ (5-10ë°° ë¹ ë¦„)"""
    try:
        if dataset_name in optimized_lookups:
            return optimized_lookups[dataset_name]
        
        # 1. ì••ì¶•ëœ pickle íŒŒì¼ ì‹œë„ (ìš°ì„ ìˆœìœ„)
        pkl_gz_file = f"FAST/lookup_{dataset_name}.pkl.gz"
        if os.path.exists(pkl_gz_file):
            print(f"  ğŸš€ ì••ì¶•ëœ pickle ë”•ì…”ë„ˆë¦¬ ë¡œë“œ: {pkl_gz_file}")
            import gzip
            with gzip.open(pkl_gz_file, 'rb') as f:
                lookup_dict = pickle.load(f)
            optimized_lookups[dataset_name] = lookup_dict
            return lookup_dict
        
        # 2. ì¼ë°˜ pickle íŒŒì¼ ì‹œë„
        pkl_file = f"FAST/lookup_{dataset_name}.pkl"
        if os.path.exists(pkl_file):
            print(f"  ğŸš€ pickle ë”•ì…”ë„ˆë¦¬ ë¡œë“œ: {pkl_file}")
            with open(pkl_file, 'rb') as f:
                lookup_dict = pickle.load(f)
            optimized_lookups[dataset_name] = lookup_dict
            return lookup_dict
        
        # 3. ê¸°ì¡´ Python ëª¨ë“ˆ ë°©ì‹ (fallback)
        module_name = f"optimized_lookup_{dataset_name}"
        if os.path.exists(f"FAST/{module_name}.py"):
            print(f"  ğŸŒ fallback Python í•¨ìˆ˜ ë¡œë“œ: {module_name}")
            module = __import__(module_name)
            lookup_func = getattr(module, f"lookup_{dataset_name}")
            optimized_lookups[dataset_name] = lookup_func
            return lookup_func
        
        print(f"  âš ï¸ ìµœì í™”ëœ lookup íŒŒì¼ ì—†ìŒ: {dataset_name} (fallback ì‚¬ìš©)")
        return None
            
    except Exception as e:
        print(f"  âš ï¸ ìµœì í™”ëœ lookup ë¡œë“œ ì‹¤íŒ¨: {e} (fallback ì‚¬ìš©)")
        return None

def scan_directory_recursive(directory, target_filename, extensions=('.jpg', '.png', '.jpeg')):
    """os.scandirì„ ì‚¬ìš©í•œ ì¬ê·€ì  íŒŒì¼ ê²€ìƒ‰ (os.walkë³´ë‹¤ ë¹ ë¦„)"""
    if not os.path.exists(directory):
        return None
    
    try:
        with os.scandir(directory) as entries:
            for entry in entries:
                if entry.is_file() and entry.name == target_filename:
                    return entry.path
                elif entry.is_dir() and not entry.name.startswith('.'):
                    # ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ë””ë ‰í† ë¦¬ ê²€ìƒ‰
                    result = scan_directory_recursive(entry.path, target_filename, extensions)
                    if result:
                        return result
    except (OSError, PermissionError):
        pass
    
    return None

def optimized_find_image_path(filename, base_path, dataset_name, fallback_cache=None):
    """ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸° (pickle ë”•ì…”ë„ˆë¦¬ ìš°ì„ , fallback ì§€ì›)"""
    # 1. ìµœì í™”ëœ lookup ë”•ì…”ë„ˆë¦¬/í•¨ìˆ˜ ì‹œë„
    lookup_obj = load_optimized_lookup(dataset_name)
    if lookup_obj:
        try:
            # pickle ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ìƒˆë¡œìš´ ë°©ì‹)
            if isinstance(lookup_obj, dict):
                # ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ (O(1), ì´ˆê³ ì†)
                if filename in lookup_obj:
                    result = lookup_obj[filename]
                    if result and os.path.exists(result):
                        return result
                
                # í™•ì¥ì ì¶”ê°€í•´ì„œ ì‹œë„
                for ext in ['.png', '.jpg', '.jpeg']:
                    filename_with_ext = f"{filename}{ext}"
                    if filename_with_ext in lookup_obj:
                        result = lookup_obj[filename_with_ext]
                        if result and os.path.exists(result):
                            return result
                    
                    # í™•ì¥ì ì œê±°í•´ì„œ ì‹œë„
                    filename_no_ext = filename.replace(ext, '')
                    if filename_no_ext in lookup_obj:
                        result = lookup_obj[filename_no_ext]
                        if result and os.path.exists(result):
                            return result
            
            # ê¸°ì¡´ í•¨ìˆ˜ì¸ ê²½ìš° (fallback)
            elif callable(lookup_obj):
                result = lookup_obj(filename, base_path)
                if result and os.path.exists(result):
                    return result
                    
        except Exception as e:
            print(f"  âš ï¸ ìµœì í™”ëœ lookup ì‹¤íŒ¨: {e}")
    
    # 2. Fallback ìºì‹œ ì‚¬ìš©
    if fallback_cache and filename in fallback_cache:
        return fallback_cache[filename]
    
    # 3. ë§ˆì§€ë§‰ fallback: os.scandir ì¬ê·€ ê²€ìƒ‰ (os.walkë³´ë‹¤ ë¹ ë¦„)
    print(f"  ğŸš€ Fallback os.scandir ì‚¬ìš©: {filename}")
    return scan_directory_recursive(base_path, filename)
    
    return None

# FTP ë§ˆìš´íŠ¸ëœ ë°ì´í„°ì…‹ ê¸°ë³¸ ê²½ë¡œ
FTP_BASE_PATH = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\\ocr_dataset"
# ë¡œì»¬ LMDB ìƒì„± ê²½ë¡œ
LOCAL_OUTPUT_PATH = "/mnt/nas/ocr_dataset"
# í•©ì³ì§„ JSON íŒŒì¼ ê²½ë¡œ
MERGED_JSON_PATH = "/home/mango/ocr_test/FAST/json_merged"

def scan_images_with_scandir(image_dir, extensions=('.jpg', '.jpeg', '.png', '.bmp')):
    """scandirì„ ì‚¬ìš©í•œ ë¹ ë¥¸ ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰"""
    image_files = {}
    
    try:
        with os.scandir(image_dir) as entries:
            for entry in entries:
                if entry.is_file() and entry.name.lower().endswith(extensions):
                    image_files[entry.name] = entry.path
    except Exception as e:
        print(f"âš ï¸ scandir ì‹¤íŒ¨: {e}")
    
    return image_files

def scan_images_recursive_with_scandir(base_dir, extensions=('.jpg', '.jpeg', '.png', '.bmp')):
    """os.scandirì„ ì‚¬ìš©í•œ ì¬ê·€ì  ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰ (os.walk ëŒ€ì²´)"""
    image_files = {}
    
    def _scan_recursive(directory):
        try:
            with os.scandir(directory) as entries:
                for entry in entries:
                    if entry.is_file() and entry.name.lower().endswith(extensions):
                        image_files[entry.name] = entry.path
                    elif entry.is_dir() and not entry.name.startswith('.'):
                        _scan_recursive(entry.path)
        except (OSError, PermissionError) as e:
            print(f"âš ï¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨ {directory}: {e}")
    
    if os.path.exists(base_dir):
        _scan_recursive(base_dir)
    
    return image_files

def build_image_cache_parallel(base_path, dataset_type):
    """ë³‘ë ¬ë¡œ ì´ë¯¸ì§€ ê²½ë¡œ ìºì‹œ êµ¬ì¶•"""
    print(f"ğŸ”„ ë³‘ë ¬ ì´ë¯¸ì§€ ê²½ë¡œ ìºì‹œ êµ¬ì¶• ì¤‘... ({dataset_type})")
    cache = {}
    
    def scan_directory(directory):
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº” í•¨ìˆ˜"""
        local_cache = {}
        if os.path.exists(directory):
            try:
                with os.scandir(directory) as entries:
                    for entry in entries:
                        if entry.is_file() and entry.name.lower().endswith(('.jpg', '.png', '.jpeg')):
                            local_cache[entry.name] = entry.path
            except Exception as e:
                print(f"âš ï¸ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹¤íŒ¨: {directory} - {e}")
        return local_cache
    
    # ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ ëª©ë¡
    scan_dirs = []
    
    if dataset_type == "ocr_public":
        for split in ['Training', 'Validation']:
            scan_dirs.append(f"{base_path}/{split}/01.ì›ì²œë°ì´í„°")
    
    elif dataset_type == "finance_logistics":
        for split in ['Training', 'Validation']:
            scan_dirs.append(f"{base_path}/{split}/01.ì›ì²œë°ì´í„°")
    
    elif dataset_type == "handwriting":
        for split in ['1.Training', '2.Validation']:
            scan_dirs.append(f"{base_path}/{split}/ì›ì²œë°ì´í„°")
    
    elif dataset_type == "public_admin":
        for train_num in [1, 2, 3]:
            scan_dirs.append(f"{base_path}/Training/[ì›ì²œ]train{train_num}/02.ì›ì²œë°ì´í„°(jpg)")
        scan_dirs.append(f"{base_path}/Validation/[ì›ì²œ]validation/02.ì›ì²œë°ì´í„°(Jpg)")
    
    # ë³‘ë ¬ ìŠ¤ìº” ì‹¤í–‰
    max_workers = min(mp.cpu_count(), 16)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_dir = {executor.submit(scan_directory, dir_path): dir_path for dir_path in scan_dirs}
        
        for future in tqdm(as_completed(future_to_dir), total=len(scan_dirs), desc="ë””ë ‰í† ë¦¬ ìŠ¤ìº”"):
            local_cache = future.result()
            cache.update(local_cache)
    
    print(f"  âœ… ìºì‹œ ì™„ë£Œ: {len(cache)}ê°œ íŒŒì¼")
    return cache

def cleanup_memory():
    """ê°•ë ¥í•œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    # 1. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    collected = gc.collect()
    
    # 2. CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except:
        pass
    
    # 3. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    
    print(f"  ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬: GC {collected}ê°œ í•´ì œ, í˜„ì¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")

def is_ftp_mounted():
    """gvfs FTPê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    gvfs_path = "/run/user/0/gvfs/ftp:host=172.30.1.226/Y:\ocr_dataset"
    return os.path.exists(gvfs_path)

def load_json_with_orjson(json_path):
    """JSON íŒŒì¼ì„ orjsonìœ¼ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ê³ ì†)"""
    print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(json_path)
    file_size_gb = file_size / (1024**3)
    print(f"  ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_gb:.2f} GB")
    
    try:
        # orjsonìœ¼ë¡œ ë¡œë“œ (ê³ ì†)
        print("  ğŸš€ orjsonìœ¼ë¡œ ë¡œë“œ ì¤‘...")
        with open(json_path, 'rb') as f:
            data = orjson.loads(f.read())
        print("  âœ… orjson ë¡œë“œ ì„±ê³µ")
        return data, None  # (data, file_handle)
        
    except MemoryError:
        print("  âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± - ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„...")
        cleanup_memory()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¬ì‹œë„
        with open(json_path, 'rb') as f:
            data = orjson.loads(f.read())
        print("  âœ… orjson ë¡œë“œ ì„±ê³µ (ì¬ì‹œë„)")
        return data, None
        
    except Exception as e:
        print(f"  âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

 

def safe_close_file(file_handle):
    """íŒŒì¼ í•¸ë“¤ì„ ì•ˆì „í•˜ê²Œ ë‹«ëŠ” í•¨ìˆ˜"""
    if file_handle:
        try:
            file_handle.close()
        except:
            pass

# ============================================================================
# Text in the Wild ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_text_in_wild_train_valid(max_samples=500):
    """Text in the wild train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª Text in the wild train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/13.í•œêµ­ì–´ê¸€ìì²´/04. Text in the wild_230209_add"
    json_path = f"{MERGED_JSON_PATH}/textinthewild_data_info.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/text_in_wild_train_layout.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/text_in_wild_valid_layout.lmdb"
    
    if os.path.exists(json_path):
        create_lmdb_text_in_wild_split(base_path, json_path, train_output_path, valid_output_path, 
                                     train_ratio=0.9, max_samples=max_samples, random_seed=42)
        
        test_fast_model_input(train_output_path)
        test_fast_model_input(valid_output_path)
        cleanup_memory()
    else:
        print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")

def create_lmdb_text_in_wild_split(base_path, json_path, train_output_path, valid_output_path, train_ratio=0.9, max_samples=None, random_seed=42):
    """Text in the wild LMDB ìƒì„± (í•©ì³ì§„ JSONì—ì„œ train/valid ë¶„í• )"""
    print(f"ğŸ§ª Text in the wild LMDB ìƒì„± ì¤‘... (train/valid {train_ratio}:{1-train_ratio} ë¶„í• )")
    
    random.seed(random_seed)
    os.makedirs(os.path.dirname(train_output_path), exist_ok=True)
    os.makedirs(os.path.dirname(valid_output_path), exist_ok=True)
    
    # Text in the WildëŠ” ì‘ì€ íŒŒì¼ì´ë¯€ë¡œ orjsonìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
    print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    
    # orjsonì„ ì‚¬ìš©í•œ ì „ì²´ JSON ë¡œë“œ (ë¹ ë¥¸ ì²˜ë¦¬)
    with open(json_path, 'rb') as f:
        data = orjson.loads(f.read())
    
    # imagesì™€ annotations ì²˜ë¦¬ (ë¹ ë¥¸ Python ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
    images_info = {img['id']: img for img in data.get('images', [])}
    image_annotations = {}
    for ann in data.get('annotations', []):
        img_id = ann['image_id']
        if img_id not in image_annotations:
            image_annotations[img_id] = []
        image_annotations[img_id].append(ann)
    
    # JSON ë°ì´í„° ì¦‰ì‹œ í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    del data
    gc.collect()
    print(f"  ğŸ—‘ï¸ JSON ì›ë³¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ì–´ì„œ train/valid ë¶„í• 
    img_ids = list(images_info.keys())
    
    if max_samples and len(img_ids) > max_samples:
        img_ids = img_ids[:max_samples]
        print(f"ğŸ“Š {max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ")
    elif max_samples is None:
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ì²˜ë¦¬: {len(img_ids)}ê°œ ì´ë¯¸ì§€")
    
    random.shuffle(img_ids)
    train_size = int(len(img_ids) * train_ratio)
    train_img_ids = img_ids[:train_size]
    valid_img_ids = img_ids[train_size:]
    
    print(f"ğŸ“Š ì´ {len(img_ids)}ê°œ ì´ë¯¸ì§€ -> Train: {len(train_img_ids)}ê°œ, Valid: {len(valid_img_ids)}ê°œ")
    
    # Training LMDB ìƒì„±
    create_lmdb_text_in_wild_from_ids(base_path, images_info, image_annotations, train_img_ids, train_output_path, "Training")
    
    # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
    del train_img_ids
    gc.collect()
    print(f"ğŸ—‘ï¸ Training ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # Validation LMDB ìƒì„±
    create_lmdb_text_in_wild_from_ids(base_path, images_info, image_annotations, valid_img_ids, valid_output_path, "Validation")
    
    # ëª¨ë“  ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ
    del valid_img_ids
    del images_info
    del image_annotations
    gc.collect()
    print(f"ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

# ============================================================================
# ê³µí†µ ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================

def process_single_text_wild_image(args):
    """Text in Wild ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    img_id, img_info, annotations, base_path, lookup_dict = args
    
    try:
        # ë¼ë²¨ í‚¤ ëŒ€ì†Œë¬¸ì/ë¬¸ìì—´/ë¶ˆë¦¬ì–¸ í˜¼ì¬ ëŒ€ë¹„
        def _as_bool(v):
            if isinstance(v, bool):
                return v
            if isinstance(v, (int, float)):
                return v != 0
            if isinstance(v, str):
                return v.strip().lower() in ('1','true','t','y','yes')
            return False
        meta = {}
        try:
            # í‰íƒ„í™” ì—†ì´ í‚¤ë§Œ ì†Œë¬¸ìí™”í•˜ì—¬ ì¡°íšŒ
            for k, v in (img_info or {}).items():
                if isinstance(k, str):
                    meta[k.lower()] = v
        except Exception:
            pass
        usd_flag = _as_bool(meta.get('usd', False))  # 180ë„
        ud_flag  = _as_bool(meta.get('ud', False))   # 270ë„(ë°˜ì‹œê³„)
        du_flag  = _as_bool(meta.get('du', False))   # 90ë„(ì‹œê³„)
        try:
            img_w = int(img_info.get('width') or 0)
            img_h = int(img_info.get('height') or 0)
        except Exception:
            img_w = 0
            img_h = 0
        word_ids = []
        char_bboxes = []
        # íŒŒì¼ëª…ì— í™•ì¥ì ì¶”ê°€ (.jpg)
        img_file_name = img_info['file_name']
        if not img_file_name.endswith('.jpg'):
            img_file_name = f"{img_file_name}.jpg"
        
        # ğŸš€ ìµœì í™”ëœ ê²½ë¡œ ì°¾ê¸° (ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ì ‘ê·¼)
        img_path = None
        if lookup_dict and isinstance(lookup_dict, dict):
            if img_file_name in lookup_dict:
                img_path = lookup_dict[img_file_name]
            else:
                # í™•ì¥ì ë³€í˜• ì‹œë„
                for ext in ['.png', '.jpeg']:
                    alt_name = img_file_name.replace('.jpg', ext)
                    if alt_name in lookup_dict:
                        img_path = lookup_dict[alt_name]
                        break
        
        # fallback: íƒ€ì…ë³„ ê²½ë¡œ ë¡œì§
        if not img_path:
            img_type = img_info.get('type', 'book')
            if img_type == "book":
                image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
            elif img_type == "sign":
                image_dir = f"{base_path}/01_textinthewild_signboard_images_new/01_textinthewild_signboard_images_new/Signboard"
            elif img_type == "traffic sign":
                image_dir = f"{base_path}/01_textinthewild_traffic_sign_images_new/01_textinthewild_traffic_sign_images_new/Traffic_Sign"
            elif img_type == "product":
                image_dir = f"{base_path}/01_textinthewild_goods_images_new/01_textinthewild_goods_images_new/Goods"
            else:
                image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
            
            img_path = os.path.join(image_dir, img_file_name)
        
        if not img_path or not os.path.exists(img_path):
            _log_verbose(f"[ocr_public] early-exit: image path not found {img_file_name}")
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ í›„ íšŒì „ í”Œë˜ê·¸ì— ë”°ë¼ ì¦‰ì‹œ íšŒì „ ì ìš© (ë ˆì´ì•„ì›ƒ ì´ì „ ë‹¨ê³„)
        with open(img_path, 'rb') as f:
            img_data = f.read()
        layout_img_path = img_path
        tmp_rot_path = None
        if usd_flag or ud_flag or du_flag:
            try:
                img_cv = _decode_image_bytes(img_data)
                if img_cv is not None and img_cv.size > 0:
                    # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°(íšŒì „ ì „) í™•ë³´: JSON width/heightê°€ 0ì´ê±°ë‚˜ ëˆ„ë½ëœ ê²½ìš° ë³´ì •
                    try:
                        oh, ow = img_cv.shape[:2]
                        if (img_w or 0) <= 0 or (img_h or 0) <= 0:
                            img_w, img_h = int(ow), int(oh)
                    except Exception:
                        pass
                    if usd_flag:
                        img_cv = cv2.rotate(img_cv, cv2.ROTATE_180)
                    elif ud_flag:
                        # ud: 90ë„ ë°˜ì‹œê³„(= 270ë„ ì‹œê³„ì™€ ë™ì¼)
                        img_cv = cv2.rotate(img_cv, cv2.ROTATE_90_COUNTERCLOCKWISE)
                    elif du_flag:
                        # du: 90ë„ ì‹œê³„
                        img_cv = cv2.rotate(img_cv, cv2.ROTATE_90_CLOCKWISE)
                    ok, buf = fast_encode_jpg(img_cv)
                    if ok:
                        img_data = bytes(buf)
                        # ë ˆì´ì•„ì›ƒì€ ê²½ë¡œ ê¸°ë°˜ì´ë¯€ë¡œ ì„ì‹œ íŒŒì¼ë¡œ ì „ë‹¬
                        import tempfile, os as _os
                        fd, tmp_rot_path = tempfile.mkstemp(prefix="tiw_rot_", suffix=".jpg")
                        _os.write(fd, img_data)
                        _os.close(fd)
                        layout_img_path = tmp_rot_path
                        try:
                            ang = 180 if usd_flag else (270 if ud_flag else 90)  # í‘œì‹œìš©: usd=180, ud=270(CCW), du=90(CW)
                        except Exception:
                            ang = -1
                        # íšŒì „ ì ìš©ì€ ê¸°ë³¸ ë¡œê·¸ í† ê¸€ê³¼ ë¬´ê´€í•˜ê²Œ 1ì¤„ì€ ë‚¨ê²¨ í™•ì¸ ìš©ì´í•˜ê²Œ í•¨
                        print(f"[tiw][rotate] applied angle={ang} file={os.path.basename(img_path)} tmp={os.path.basename(layout_img_path)}")
            except Exception:
                pass
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
        bboxes = []
        words = []
        word_ids = []
        # ë‹¨ì–´ ë‹¨ìœ„ ë°©í–¥ì„± í”Œë˜ê·¸(ud/du) ê¸°ë¡
        word_orients = []
        
        for ann in annotations:
            attrs = ann.get('attributes', {})
            # character ë°•ìŠ¤ëŠ” ë§ˆìŠ¤í‚¹ìš©ìœ¼ë¡œë§Œ ìˆ˜ì§‘
            try:
                if isinstance(attrs, dict) and str(attrs.get('class', '')).lower() == 'character':
                    cx, cy, cw, ch = ann['bbox']
                    cx1, cy1, cx2, cy2 = cx, cy, cx + cw, cy + ch
                    cflat = [cx1, cy1, cx2, cy1, cx2, cy2, cx1, cy2]
                    if img_w > 0 and img_h > 0:
                        if usd_flag:
                            cflat = _rotate_flat8_180(cflat, img_w, img_h)
                        elif ud_flag:
                            # ud: 90ë„ ë°˜ì‹œê³„(=270ë„ ì‹œê³„)
                            cflat = _rotate_flat8_270_cw(cflat, img_w, img_h)
                        elif du_flag:
                            # du: 90ë„ ì‹œê³„
                            cflat = _rotate_flat8_90_cw(cflat, img_w, img_h)
                    char_bboxes.append(cflat)
            except Exception:
                pass
            # attributes.classê°€ 'word'ì¸ ê²½ìš°ë§Œ ë¼ë²¨ ì²˜ë¦¬
            attrs = ann.get('attributes', {})
            cls_value = None
            if isinstance(attrs, dict):
                cls_value = str(attrs.get('class', '')).lower()
            if cls_value != 'word':
                continue
            # bbox: [x, y, width, height] -> [x1, y1, x2, y1, x2, y2, x1, y2]
            x, y, w, h = ann['bbox']
            x1, y1, x2, y2 = x, y, x + w, y + h
            
            # ì›ë³¸ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ë¦¬í•‘ ì—†ìŒ)
            pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
            if img_w > 0 and img_h > 0:
                if usd_flag:
                    pixel_coords = _rotate_flat8_180(pixel_coords, img_w, img_h)
                elif ud_flag:
                    # ud: 270ë„(ë°˜ì‹œê³„)
                    pixel_coords = _rotate_flat8_270_cw(pixel_coords, img_w, img_h)
                elif du_flag:
                    # du: 90ë„(ì‹œê³„)
                    pixel_coords = _rotate_flat8_90_cw(pixel_coords, img_w, img_h)
            
            # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
            if not bbox_debug_flags['text_in_wild']:
                print(f"ğŸ“‹ Text in Wild bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                bbox_debug_flags['text_in_wild'] = True
            
            bboxes.append(pixel_coords)
            words.append(ann['text'])
            # id ìˆ˜ì§‘
            word_ids.append(ann.get('id'))
            # annotation ë‹¨ìœ„ ë°©í–¥ì„±(word_ud/word_du) í”Œë˜ê·¸ ìˆ˜ì§‘
            try:
                attrs_lc = {}
                if isinstance(attrs, dict):
                    for k, v in attrs.items():
                        if isinstance(k, str):
                            attrs_lc[k.lower()] = v
                w_ud = False
                w_du = False
                if attrs_lc:
                    # ë¬¸ìì—´/ë¶ˆë¦¬ì–¸/ìˆ«ì í˜¼ìš© ëŒ€ì‘
                    def _as_bool2(v):
                        if isinstance(v, bool): return v
                        if isinstance(v, (int, float)): return v != 0
                        if isinstance(v, str): return v.strip().lower() in ('1','true','t','y','yes')
                        return False
                    w_ud = _as_bool2(attrs_lc.get('word_ud', False))
                    w_du = _as_bool2(attrs_lc.get('word_du', False))
                if w_ud:
                    word_orients.append('ud')
                elif w_du:
                    word_orients.append('du')
                else:
                    word_orients.append(None)
            except Exception:
                word_orients.append(None)
        
        # ğŸ“¦ LayoutDetection ê¸°ë°˜ ë¬¸ì¥ ë³‘í•©
        orig_bboxes = list(bboxes)
        orig_words = list(words)
        orig_ids = list(word_ids)
        try:
            layout_boxes = run_layout_detection(layout_img_path)
            multi_samples = []
            if layout_boxes:
                # ë³‘í•©ëœ ì „ì²´ ë¬¸ì¥(ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€)ì€ ë”ì´ìƒ LMDBì— ì €ì¥í•˜ì§€ ì•ŠìŒ.
                # ëŒ€ì‹  ë ˆì´ì•„ì›ƒë³„ ì¸í˜ì¸íŠ¸ í¬ë¡­ì„ ê°œë³„ ìƒ˜í”Œë¡œ ì €ì¥.
                # ë ˆì´ì•„ì›ƒ-ë‹¨ì–´ ë§¤í•‘ ì¤€ë¹„
                img_cv_full = _decode_image_bytes(img_data)
                if img_cv_full is None:
                    layout_boxes = []
                else:
                    # img_cv_fullì€ ì´ë¯¸ usd/ud/du í”Œë˜ê·¸ ë°˜ì˜ë¨(ìœ„ì—ì„œ íšŒì „ ì ìš©)
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    layout_aabbs = []
                    for lb in layout_boxes:
                        x1, y1, x2, y2 = lb['coordinate']
                        layout_aabbs.append((max(0, int(x1)), max(0, int(y1)), min(W, int(x2)), min(H, int(y2))))
                    assigned = _assign_words_to_layout(word_aabbs, layout_aabbs, min_overlap_ratio=0.15)
                    # character ë§¤í•‘
                    char_aabbs = []
                    if char_bboxes:
                        try:
                            char_aabbs = [_aabb_from_flat8(b) for b in char_bboxes]
                        except Exception:
                            char_aabbs = []
                    char_assigned = []
                    if char_aabbs:
                        char_assigned = _assign_words_to_layout(char_aabbs, layout_aabbs, min_overlap_ratio=0.3)
                    # ë ˆì´ì•„ì›ƒë³„ë¡œ ìƒ˜í”Œ ìƒì„±
                    for li, la in enumerate(layout_aabbs):
                        # ì¤‘ì‹¬ì  í¬í•¨ ê¸°ì¤€ìœ¼ë¡œ ë” ì—„ê²©í•˜ê²Œ í•„í„°ë§(ê²¹ì¹¨ í˜¼ì„  ë°©ì§€)
                        lx1, ly1, lx2, ly2 = la
                        idxs = []
                        for i, a in enumerate(assigned):
                            if a != li:
                                continue
                            wx1, wy1, wx2, wy2 = word_aabbs[i]
                            cx = (wx1 + wx2) * 0.5
                            cy = (wy1 + wy2) * 0.5
                            # ë ˆì´ì•„ì›ƒ ì¤‘ì‹¬ í¬í•¨ + ë‹¨ì–´ ë°•ìŠ¤ì˜ ëŒ€ë¶€ë¶„ì´ ë ˆì´ì•„ì›ƒì— í¬í•¨ë˜ì–´ì•¼ í•¨(>=90%)
                            if (lx1 <= cx <= lx2) and (ly1 <= cy <= ly2):
                                wa = (float(wx1), float(wy1), float(wx2), float(wy2))
                                la_aabb = (float(lx1), float(ly1), float(lx2), float(ly2))
                                w_area = _area(wa)
                                inter = _intersection_area(wa, la_aabb)
                                coverage = (inter / w_area) if w_area > 0 else 0.0
                                if coverage >= 0.6:
                                    idxs.append(i)
                        if not idxs:
                            continue
                        # ud/du ë‹¨ì–´ê°€ í¬í•¨ëœ ë ˆì´ì•„ì›ƒì´ë©´: ê°œí–‰ ì—†ì´ ë°©í–¥ì„± ë°˜ì˜ ë‹¨ì¼ ë¼ì¸ ìƒì„±
                        has_ud = False
                        has_du = False
                        for i2 in idxs:
                            try:
                                if word_orients[i2] == 'ud': has_ud = True
                                elif word_orients[i2] == 'du': has_du = True
                            except Exception:
                                pass
                        if has_ud or has_du:
                            # ì—´(ì„¸ë¡œ ì»¬ëŸ¼) í´ëŸ¬ìŠ¤í„°ë§ â†’ ê° ì»¬ëŸ¼ ë‚´ë¶€ y ì •ë ¬(ud: ì•„ë˜â†’ìœ„, du: ìœ„â†’ì•„ë˜) â†’ ì»¬ëŸ¼ ê°„ ", "ë¡œ ê²°í•©
                            # 1) xì¤‘ì‹¬/í­ ê¸°ë°˜ ì»¬ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§ ì„ê³„ì¹˜ ê³„ì‚°
                            widths = []
                            xcenters = {}
                            ycenters = {}
                            for i2 in idxs:
                                wx1, wy1, wx2, wy2 = word_aabbs[i2]
                                widths.append(max(1.0, abs(wx2 - wx1)))
                                xcenters[i2] = (wx1 + wx2) * 0.5
                                ycenters[i2] = (wy1 + wy2) * 0.5
                            median_w = float(np.median(widths)) if widths else 8.0
                            x_thresh = max(8.0, 0.8 * median_w)
                            # 2) xì¤‘ì‹¬ìœ¼ë¡œ ì •ë ¬ í›„, ê°€ê¹Œìš´ ê²ƒë¼ë¦¬ ì»¬ëŸ¼ìœ¼ë¡œ ë¬¶ê¸°
                            order_x = sorted(idxs, key=lambda i: xcenters[i])
                            columns = []
                            cur = []
                            basex = None
                            for i2 in order_x:
                                cx = xcenters[i2]
                                if basex is None:
                                    basex = cx; cur = [i2]
                                else:
                                    if abs(cx - basex) > x_thresh:
                                        if cur:
                                            columns.append(cur)
                                        basex = cx; cur = [i2]
                                    else:
                                        cur.append(i2)
                            if cur:
                                columns.append(cur)
                            # 3) ê° ì»¬ëŸ¼ ë‚´ë¶€ ë°©í–¥ì„± ê²°ì • ë° yì •ë ¬
                            col_strings = []
                            for col in columns:
                                cnt_ud_c = sum(1 for i2 in col if (i2 < len(word_orients) and word_orients[i2] == 'ud'))
                                cnt_du_c = sum(1 for i2 in col if (i2 < len(word_orients) and word_orients[i2] == 'du'))
                                reverse_c = (cnt_ud_c > cnt_du_c)  # Trueë©´ ì•„ë˜â†’ìœ„
                                col_sorted = sorted(col, key=lambda i: ycenters[i], reverse=reverse_c)
                                col_str = " ".join(str(orig_words[j]) if orig_words[j] is not None else "" for j in col_sorted).strip()
                                if col_str:
                                    col_strings.append(col_str)
                            sentence = "\n".join(col_strings).strip()
                        else:
                            # ê¸°ì¡´ ë¡œì§: ë¬¸ì¥ ê²°í•©(ê°œí–‰ í¬í•¨): y-í´ëŸ¬ìŠ¤í„° â†’ ë¼ì¸ ë‚´ x1 ì •ë ¬
                            # 1) ë‹¨ì–´ ë†’ì´ ê¸°ë°˜ ì„ê³„ì¹˜
                            heights = []
                            for i2 in idxs:
                                y1i = word_aabbs[i2][1]; y2i = word_aabbs[i2][3]
                                heights.append(max(1.0, y2i - y1i))
                            median_h = float(np.median(heights)) if heights else 8.0
                            y_thresh = max(8.0, 0.6 * median_h)
                            # 2) yì¤‘ì‹¬ìœ¼ë¡œ ì •ë ¬ í›„ ë¼ì¸ í´ëŸ¬ìŠ¤í„°
                            order_y = sorted(idxs, key=lambda i: ((word_aabbs[i][1] + word_aabbs[i][3]) * 0.5))
                            lines_idx = []
                            cur = []
                            basey = None
                            for i2 in order_y:
                                cy = (word_aabbs[i2][1] + word_aabbs[i2][3]) * 0.5
                                if basey is None:
                                    basey = cy; cur = [i2]
                                else:
                                    if abs(cy - basey) > y_thresh:
                                        lines_idx.append(cur)
                                        basey = cy; cur = [i2]
                                    else:
                                        cur.append(i2)
                            if cur:
                                lines_idx.append(cur)
                            # 3) ê° ë¼ì¸ ë‚´ x1(ì¢Œì¸¡) ì •ë ¬, id ë™ì  ë³´ì¡°
                            def _id_fallback(i):
                                try:
                                    return int(str(orig_ids[i]))
                                except Exception:
                                    return 10**9  # idê°€ ì—†ìœ¼ë©´ ë’¤ë¡œ
                            line_strings = []
                            for arr in lines_idx:
                                arr_sorted = sorted(
                                    arr,
                                    key=lambda i: (min(word_aabbs[i][0], word_aabbs[i][2]), _id_fallback(i))
                                )
                                line_str = " ".join(str(orig_words[j]) if orig_words[j] is not None else "" for j in arr_sorted).strip()
                                if line_str:
                                    line_strings.append(line_str)
                            sentence = "\n".join(line_strings)
                        if not sentence:
                            continue
                        # í¬ë¡­ ì˜ì—­: ì›ë˜ ë ˆì´ì•„ì›ƒ AABB ëŒ€ì‹ , ì„ íƒëœ ë‹¨ì–´ë“¤ì˜ í•©ì§‘í•© AABBë¡œ í™•ì¥í•˜ì—¬
                        # ë‹¨ì–´ê°€ ë¶€ë¶„ ì ˆë‹¨ë˜ì§€ ì•Šë„ë¡ ë³´ì¥
                        if not idxs:
                            continue
                        ux1 = min(word_aabbs[i][0] for i in idxs)
                        uy1 = min(word_aabbs[i][1] for i in idxs)
                        ux2 = max(word_aabbs[i][2] for i in idxs)
                        uy2 = max(word_aabbs[i][3] for i in idxs)
                        x1 = max(0, int(np.floor(ux1)))
                        y1 = max(0, int(np.floor(uy1)))
                        x2 = min(W, int(np.ceil(ux2)))
                        y2 = min(H, int(np.ceil(uy2)))
                        if x2<=x1 or y2<=y1:
                            continue
                        sentence_list = merge_words_by_layout(
                            [orig_bboxes[k] for k in idxs],
                            [orig_words[k] for k in idxs],
                            [{'label': 'layout', 'coordinate': [x1, y1, x2, y2], 'score': 1.0}],
                            word_ids=[orig_ids[k] for k in idxs],
                            prefer_id_order=True,
                            word_orients=[word_orients[k] for k in idxs] if word_orients else None
                        )[1]
                        sentence = sentence_list[0] if sentence_list else ""
                        # ì´ë¯¸ì§€ ë‹¨ìœ„ ud/du ë¼ë²¨ë§Œ ê°œí–‰ ì œê±°, ë‹¨ì–´ ë‹¨ìœ„ ë°©í–¥ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                        has_word_orient_local = any((word_orients[k] in ('ud','du')) for k in idxs) if word_orients else False
                        if (ud_flag or du_flag) and sentence and not has_word_orient_local:
                            try:
                                sentence = sentence.replace("\n", " ")
                            except Exception:
                                pass
                        if not sentence:
                            continue                        
                        crop = img_cv_full[y1:y2, x1:x2].copy()
                        # character ì¸í˜ì¸íŠ¸ + ë¬¸ì ë³´ì¡´ í•©ì„±
                        if char_aabbs and Inpaint is not None:
                            try:
                                # ë ˆì´ì•„ì›ƒ í• ë‹¹ ëŒ€ì‹ , ë‹¨ì–´ í•©ì§‘í•© AABB(ux1..uy2) ë‚´ë¶€ì˜ ë¬¸ì ì„ íƒ
                                ch_idxs = []
                                for ci, b in enumerate(char_bboxes):
                                    try:
                                        cx = (b[0] + b[2] + b[4] + b[6]) * 0.25
                                        cy = (b[1] + b[3] + b[5] + b[7]) * 0.25
                                        if (x1 <= cx <= x2) and (y1 <= cy <= y2):
                                            ch_idxs.append(ci)
                                    except Exception:
                                        continue
                                if ch_idxs:
                                    _log_verbose(f"[text_in_wild] char-mask used: {len(ch_idxs)} chars for layout {li}")
                                    Hc, Wc = (y2-y1), (x2-x1)
                                    mask = np.ones((Hc, Wc), dtype=np.uint8) * 255
                                    preserve = np.zeros((Hc, Wc), dtype=np.uint8)
                                    polys=[]; char_heights=[]
                                    for ci in ch_idxs:
                                        b = char_bboxes[ci]
                                        poly = np.array([
                                            [max(0, min(Wc, int(round(b[0]-x1)))), max(0, min(Hc, int(round(b[1]-y1))))],
                                            [max(0, min(Wc, int(round(b[2]-x1)))), max(0, min(Hc, int(round(b[3]-y1))))],
                                            [max(0, min(Wc, int(round(b[4]-x1)))), max(0, min(Hc, int(round(b[5]-y1))))],
                                            [max(0, min(Wc, int(round(b[6]-x1)))), max(0, min(Hc, int(round(b[7]-y1))))],
                                        ], dtype=np.int32)
                                        polys.append(poly)
                                        by1 = max(0, min(poly[:,1])); by2 = min(Hc, max(poly[:,1]))
                                        char_heights.append(max(1, by2-by1))
                                    try:
                                        # ë‹¨ì–´ ROIë¥¼ ë§Œë“¤ê³  char ë§ˆìŠ¤í¬ë¥¼ í•­ìƒ ê·¸ ë‚´ë¶€ë¡œ í´ë¦¬í•‘
                                        word_roi = np.zeros((Hc, Wc), dtype=np.uint8)
                                        for i2 in idxs:
                                            b = orig_bboxes[i2]
                                            wpoly = np.array([
                                                [max(0, min(Wc, int(round(b[0]-x1)))), max(0, min(Hc, int(round(b[1]-y1))))],
                                                [max(0, min(Wc, int(round(b[2]-x1)))), max(0, min(Hc, int(round(b[3]-y1))))],
                                                [max(0, min(Wc, int(round(b[4]-x1)))), max(0, min(Hc, int(round(b[5]-y1))))],
                                                [max(0, min(Wc, int(round(b[6]-x1)))), max(0, min(Hc, int(round(b[7]-y1))))],
                                            ], dtype=np.int32)
                                            cv2.fillPoly(word_roi, [wpoly], 1)
                                        cv2.fillPoly(mask, polys, 0)
                                        cv2.fillPoly(preserve, polys, 1)
                                        # preserveë¥¼ ë‹¨ì–´ ROIë¡œ ì œí•œ
                                        preserve = (preserve & word_roi).astype(np.uint8)
                                    except Exception:
                                        pass
                                    try:
                                        median_h2 = float(np.median(char_heights)) if char_heights else 8.0
                                        # íŒ½ì°½ ê°•ë„ ì¶”ê°€ ì™„í™”(ì•„í‹°íŒ©íŠ¸ ê°ì†Œ)
                                        dilate_px = int(max(1, round(0.04*median_h2)))
                                        ksz = max(1, dilate_px*2+1)
                                        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksz, ksz))
                                        preserve_dil = cv2.dilate(preserve, kernel, iterations=1)
                                        # ê°€ë“œ ë°´ë“œ(ë¬¸ì ê°€ì¥ìë¦¬ ì£¼ë³€ì€ ì¸í˜ì¸íŠ¸ ê¸ˆì§€) ì¶”ê°€
                                        guard_px = int(max(1, round(0.02*median_h2)))
                                        gsz = max(1, guard_px*2+1)
                                        gkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (gsz, gsz))
                                        guard = cv2.dilate(preserve, gkernel, iterations=1)
                                        mask = np.where(preserve_dil>0, 0, 255).astype(np.uint8)
                                        mask[guard>0] = 0
                                    except Exception:
                                        pass
                                    crop_orig = crop.copy()
                                    bg_restored = None
                                    try:
                                        inp = Inpaint(crop, mask)
                                        out1 = inp()
                                        if out1 is not None and out1.shape == crop.shape:
                                            bg_restored = out1
                                    except Exception:
                                        bg_restored = None
                                    if bg_restored is None:
                                        try:
                                            mask_cv = (mask>0).astype(np.uint8)*255
                                            # Navier-Stokesë¡œ ë³€ê²½í•˜ì—¬ ê²½ê³„ ìƒ‰ ë²ˆì§ ì™„í™”
                                            bg_restored = cv2.inpaint(crop, mask_cv, 3, cv2.INPAINT_NS)
                                        except Exception:
                                            bg_restored = None
                                    if bg_restored is not None:
                                        m = (mask==0).astype(np.float32)
                                        try:
                                            feather = max(1, int(round(0.04*(median_h2 if 'median_h2' in locals() else 8.0))))
                                            kf = max(1, feather*2+1)
                                            m = cv2.GaussianBlur(m, (kf, kf), 0)
                                        except Exception:
                                            pass
                                        if len(crop.shape)==3:
                                            m3 = np.repeat(m[:, :, None], 3, axis=2)
                                        else:
                                            m3 = m
                                        crop = (crop_orig.astype(np.float32)*m3 + bg_restored.astype(np.float32)*(1.0-m3)).clip(0,255).astype(crop_orig.dtype)
                            except Exception:
                                pass
                        # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                        crop = _apply_rotation_if_needed(crop, img_path)
                        ok, buf = fast_encode_jpg(crop)
                        if not ok:
                            continue
                        img_bytes_li = bytes(buf)
                        # GT: í¬ë¡­ ì „ì²´ ë°•ìŠ¤ 1ê°œ + ë¬¸ì¥ 1ê°œ (íšŒì „ í›„ ì‹¤ì œ í¬ê¸° ê¸°ì¤€)
                        h, w = crop.shape[:2]
                        flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                        gt_li = {
                            'bboxes': [flat8],
                            'words': [sentence],
                            'filename': f"{img_info['file_name']}_layout_{li:02d}.jpg"
                        }
                        multi_samples.append((f"{img_id}_{li}", img_bytes_li, gt_li))
            # multi_samplesê°€ ìˆìœ¼ë©´ ë°˜í™˜
            if layout_boxes and multi_samples:
                return multi_samples
            # ë ˆì´ì•„ì›ƒì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ, ê¸°ì¡´ ë‹¨ì–´ ë‹¨ìœ„ ë°˜í™˜
            if layout_boxes:
                merged_bboxes, merged_words = merge_words_by_layout(bboxes, words, layout_boxes, word_ids=word_ids, prefer_id_order=True, word_orients=word_orients)
                if merged_bboxes and merged_words:
                    # ì´ë¯¸ì§€ ë‹¨ìœ„ ud/du ë¼ë²¨ë§Œ ê°œí–‰ ì œê±° (ë‹¨ì–´ ë‹¨ìœ„ ë°©í–¥ì´ ìˆìœ¼ë©´ ìœ ì§€)
                    if (ud_flag or du_flag):
                        try:
                            merged_words = [w.replace("\n", " ") if isinstance(w, str) else w for w in merged_words]
                        except Exception:
                            pass
                    bboxes, words = merged_bboxes, merged_words
        except Exception as e:
            _log_verbose(f"[text_in_wild] exception before detection: {img_file_name} err={type(e).__name__}")
            pass

        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_info['file_name']
        }
        # ë ˆì´ì•„ì›ƒ ë‹¨ìœ„ ë””ë²„ê·¸ ì €ì¥ ë¡œì§ ì œê±°ë¨
        return None
        
    except Exception as e:
        return None
    finally:
        try:
            if 'tmp_rot_path' in locals() and tmp_rot_path and os.path.exists(tmp_rot_path):
                os.remove(tmp_rot_path)
        except Exception:
            pass

def process_single_public_admin_image(args):
    """ê³µê³µí–‰ì •ë¬¸ì„œ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    img_info, annotations, base_path, lookup_dict, dataset_lookup_name, image_path_cache = args
    
    try:
        word_ids = []
        # íŒŒì¼ëª… ì¶”ì¶œ
        img_file_name = img_info.get('image.file.name', '')
        image_category = img_info.get('image.category', '')
        image_make_code = img_info.get('image.make.code', '')
        image_make_year = img_info.get('image.make.year', '')
        
        if not img_file_name:
            return None
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸°
        img_path = optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
        if not img_path or not os.path.exists(img_path):
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
        bboxes = []
        words = []
        img_id = img_info.get('id')
        word_ids = []
        word_ids = []
        
        # ì–´ë…¸í…Œì´ì…˜ ë¡œë”© (ë©”ëª¨ë¦¬ ì ˆì•½: SQLiteì—ì„œ ì§€ì—° ë¡œë“œ ê°€ëŠ¥)
        if isinstance(annotations, dict) and annotations.get('sqlite') and annotations.get('image_id') is not None:
            sqlite_path = annotations['sqlite']
            target_img_id = annotations['image_id']
            anns_loaded = []
            try:
                conn = _get_public_admin_sqlite_conn(sqlite_path)
                if conn is not None:
                    cur = conn.cursor()
                    for row in cur.execute("SELECT ann FROM a WHERE image_id=?", (int(target_img_id),)):
                        try:
                            anns_loaded.append(orjson.loads(row[0]))
                        except Exception:
                            continue
                    cur.close()
            except Exception:
                anns_loaded = []
        else:
            anns_loaded = annotations or []

        for ann in anns_loaded:
            # annotation.bbox: [x, y, width, height] -> [x1, y1, x2, y1, x2, y2, x1, y2]
            x, y, w, h = ann['annotation.bbox']
            x1, y1, x2, y2 = x, y, x + w, y + h
            
            # ì›ë³¸ í”½ì…€ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (8ê°œ ì¢Œí‘œ í˜•íƒœë¡œ í†µì¼)
            pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
            
            # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
            if not bbox_debug_flags['public_admin']:
                print(f"ğŸ“‹ Public Admin bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                bbox_debug_flags['public_admin'] = True
            
            bboxes.append(pixel_coords)
            words.append(ann['annotation.text'])
            ann_id = ann.get('id') or ann.get('annotation.id')
            if ann_id is None:
                ann_id = len(word_ids)
            word_ids.append(ann_id)
    
        # ğŸ“¦ LayoutDetection ê¸°ë°˜ ë¬¸ì¥ ë³‘í•©
        orig_bboxes = list(bboxes)
        orig_words = list(words)
        try:
            layout_boxes = run_layout_detection(img_path)
            # ë ˆì´ì•„ì›ƒ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ í…Œì´ë¸” ì…€ íƒì§€ ì‹œë„
            if not layout_boxes:
                tables = run_layout_tables(img_path)
                if tables:
                    # í…Œì´ë¸” ì…€ íƒì§€
                    table_model = get_table_model()
                    if table_model is None:
                        return None
                    img_cv_full = _decode_image_bytes(img_data)
                    if img_cv_full is None:
                        return None
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    results = []
                    # ìºì‹œëœ í…Œì´ë¸” ì…€ ìš°ì„  ì‚¬ìš©
                    cached_cells = None
                    try:
                        with PRED_CACHE_LOCK:
                            cached = PREDICTION_CACHE.get(img_path) or {}
                            cached_cells = cached.get('table_cells')
                    except Exception:
                        cached_cells = None
                    if cached_cells:
                        # cached_cells: [(tx1,ty1,tx2,ty2,[(cx1,cy1,cx2,cy2), ...]), ...]
                        for (tx1, ty1, tx2, ty2, cells) in cached_cells or []:
                            for (cx1, cy1, cx2, cy2) in cells or []:
                                cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                                idxs = []
                                for wi, wa in enumerate(word_aabbs):
                                    inter = _intersection_area(wa, cell_aabb)
                                    wa_area = _area(wa)
                                    if wa_area > 0 and (inter / wa_area) >= 0.2:
                                        idxs.append(wi)
                                if not idxs:
                                    continue
                                def _id_key(i):
                                    try:
                                        return int(str(word_ids[i]))
                                    except Exception:
                                        return str(word_ids[i])
                                idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                                sentence = merge_words_by_layout(
                                    [orig_bboxes[k] for k in idxs],
                                    [orig_words[k] for k in idxs],
                                    [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                    word_ids=[word_ids[k] for k in idxs],
                                    prefer_id_order=True
                                )[1][0] if idxs else ""
                                if not sentence:
                                    continue
                                crop = img_cv_full[cy1:cy2, cx1:cx2]
                                try:
                                    preserve_polys = []
                                    for k in idxs:
                                        poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                        if poly is not None and poly.size > 0:
                                            preserve_polys.append(poly)
                                    if preserve_polys:
                                        crop = _inpaint_preserve_regions(crop, preserve_polys)
                                except Exception:
                                    pass
                                crop = _apply_rotation_if_needed(crop, img_path)
                                ok, buf = fast_encode_jpg(crop)
                                if not ok:
                                    continue
                                img_bytes_li = bytes(buf)
                                h, w = crop.shape[:2]
                                flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                                gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                                results.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                    else:
                        # ë°°ì¹˜ë¡œ í‘œ ë‚´ë¶€ ì…€ ë°•ìŠ¤ ì˜ˆì¸¡
                        batch_cells = _predict_table_cells_batch(img_cv_full, tables, img_id)
                        for tx1, ty1, tx2, ty2, cells in (batch_cells or []):
                            for (cx1, cy1, cx2, cy2) in cells:
                                cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                                idxs = []
                                for wi, wa in enumerate(word_aabbs):
                                    inter = _intersection_area(wa, cell_aabb)
                                    wa_area = _area(wa)
                                    if wa_area > 0 and (inter / wa_area) >= 0.2:
                                        idxs.append(wi)
                                if not idxs:
                                    continue
                                def _id_key(i):
                                    try:
                                        return int(str(word_ids[i]))
                                    except Exception:
                                        return str(word_ids[i])
                                idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                                sentence = merge_words_by_layout(
                                    [orig_bboxes[k] for k in idxs],
                                    [orig_words[k] for k in idxs],
                                    [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                    word_ids=[word_ids[k] for k in idxs],
                                    prefer_id_order=True
                                )[1][0] if idxs else ""
                                if not sentence:
                                    continue
                                crop = img_cv_full[cy1:cy2, cx1:cx2]
                                try:
                                    preserve_polys = []
                                    for k in idxs:
                                        poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                        if poly is not None and poly.size > 0:
                                            preserve_polys.append(poly)
                                    if preserve_polys:
                                        crop = _inpaint_preserve_regions(crop, preserve_polys)
                                except Exception:
                                    pass
                                crop = _apply_rotation_if_needed(crop, img_path)
                                ok, buf = fast_encode_jpg(crop)
                                if not ok:
                                    continue
                                img_bytes_li = bytes(buf)
                                h, w = crop.shape[:2]
                                flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                                gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                                results.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                            if not sentence:
                                continue
                            # ì…€ í¬ë¡­ ì €ì¥
                            crop = img_cv_full[cy1:cy2, cx1:cx2]
                            # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´, ë‚˜ë¨¸ì§€ ë°°ê²½ ë³µì›
                            try:
                                preserve_polys = []
                                for k in idxs:
                                    poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                    if poly is not None and poly.size > 0:
                                        preserve_polys.append(poly)
                                if preserve_polys:
                                    crop = _inpaint_preserve_regions(crop, preserve_polys)
                            except Exception:
                                pass
                            # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                            crop = _apply_rotation_if_needed(crop, img_path)
                            ok, buf = fast_encode_jpg(crop)
                            if not ok:
                                continue
                            img_bytes_li = bytes(buf)
                            h, w = crop.shape[:2]
                            flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                            gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                            results.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                    if results:
                        return results
                # tableë„ ì—†ìœ¼ë©´ ì œì™¸
                return None
            multi_samples = []
            if layout_boxes:
                img_cv_full = _decode_image_bytes(img_data)
                if img_cv_full is not None:
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    layout_aabbs = []
                    for lb in layout_boxes:
                        x1, y1, x2, y2 = lb['coordinate']
                        layout_aabbs.append((max(0, int(x1)), max(0, int(y1)), min(W, int(x2)), min(H, int(y2))))
                    assigned = _assign_words_to_layout(word_aabbs, layout_aabbs, min_overlap_ratio=0.3)
                    for li, la in enumerate(layout_aabbs):
                        idxs = [i for i, a in enumerate(assigned) if a == li]
                        if not idxs:
                            continue
                        x1, y1, x2, y2 = la
                        # ID ìš°ì„ (STRICT_ID_ORDER) ë³‘í•©ê¸°ë¡œ ë¬¸ì¥ êµ¬ì„±
                        sentence_list = merge_words_by_layout(
                            [orig_bboxes[k] for k in idxs],
                            [orig_words[k] for k in idxs],
                            [{'label': 'layout', 'coordinate': [x1, y1, x2, y2], 'score': 1.0}],
                            word_ids=[word_ids[k] for k in idxs],
                            prefer_id_order=True
                        )[1]
                        sentence = sentence_list[0] if sentence_list else ""
                        if not sentence:
                            continue
                        x1, y1, x2, y2 = la
                        if x2<=x1 or y2<=y1:
                            continue
                        crop = img_cv_full[y1:y2, x1:x2].copy()
                        # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                        try:
                            preserve_polys = []
                            for k in idxs:
                                poly = _flat8_to_crop_poly(orig_bboxes[k], x1, y1, x2, y2)
                                if poly is not None and poly.size > 0:
                                    preserve_polys.append(poly)
                            if preserve_polys:
                                crop = _inpaint_preserve_regions(crop, preserve_polys)
                        except Exception:
                            pass
                        # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                        crop = _apply_rotation_if_needed(crop, img_path)
                        ok, buf = fast_encode_jpg(crop)
                        if not ok:
                            continue
                        img_bytes_li = bytes(buf)
                        h, w = crop.shape[:2]
                        flat8 = [0.0,0.0,float(w),0.0,float(w),float(h),0.0,float(h)]
                        gt_li = {'bboxes':[flat8], 'words':[sentence], 'filename': f"{img_file_name}_layout_{li:02d}.jpg"}
                        multi_samples.append((f"{img_id}_{li}", img_bytes_li, gt_li))
            # í…Œì´ë¸” ì…€ ìƒ˜í”Œë„ í•­ìƒ ì¶”ê°€ ì‹œë„ (public_adminì—ì„œë„ ë ˆì´ì•„ì›ƒ ìœ ë¬´ì™€ ë¬´ê´€)
            try:
                tables = run_layout_tables(img_path)
                print(f"[debug] public_admin(always): tables_found={len(tables) if tables else 0}")
                cell_samples = []
                if tables:
                    table_model = get_table_model()
                    img_cv_full = _decode_image_bytes(img_data)
                    if table_model is not None and img_cv_full is not None:
                        H, W = img_cv_full.shape[:2]
                        word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                        print(f"[debug] public_admin(always): call batch tables={len(tables)}")
                        batch_cells = _predict_table_cells_batch(img_cv_full, tables, img_id)
                        print(f"[debug] public_admin(always): batch_cells groups={len(batch_cells)}")
                        for (tx1, ty1, tx2, ty2, cells) in (batch_cells or []):
                            for (cx1, cy1, cx2, cy2) in (cells or []):
                                cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                                idxs = []
                                for wi, wa in enumerate(word_aabbs):
                                    inter = _intersection_area(wa, cell_aabb)
                                    wa_area = _area(wa)
                                    if wa_area > 0 and (inter / wa_area) >= 0.5:
                                        idxs.append(wi)
                                if not idxs:
                                    continue
                                def _id_key(i):
                                    try:
                                        return int(str(word_ids[i]))
                                    except Exception:
                                        return str(word_ids[i])
                                idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                                sentence = merge_words_by_layout(
                                    [orig_bboxes[k] for k in idxs],
                                    [orig_words[k] for k in idxs],
                                    [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                    word_ids=[word_ids[k] for k in idxs],
                                    prefer_id_order=True
                                )[1][0] if idxs else ""
                                if not sentence:
                                    continue
                                crop = img_cv_full[cy1:cy2, cx1:cx2]
                                # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                                crop = _apply_rotation_if_needed(crop, img_path)
                                ok, buf = fast_encode_jpg(crop)
                                if not ok:
                                    continue
                                img_bytes_li = bytes(buf)
                                h, w = crop.shape[:2]
                                flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                                gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                                cell_samples.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                # ë°°ì¹˜ë¡œ í‘œ ë‚´ë¶€ ì…€ ë°•ìŠ¤ ì˜ˆì¸¡
                batch_cells = _predict_table_cells_batch(img_cv_full, tables, img_id)
                for tx1, ty1, tx2, ty2, cells in (batch_cells or []):
                    for (cx1, cy1, cx2, cy2) in cells:
                        cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                        idxs = []
                        for wi, wa in enumerate(word_aabbs):
                            inter = _intersection_area(wa, cell_aabb)
                            wa_area = _area(wa)
                            if wa_area > 0 and (inter / wa_area) >= 0.2:
                                idxs.append(wi)
                        if not idxs:
                            continue
                        def _id_key(i):
                            try:
                                return int(str(word_ids[i]))
                            except Exception:
                                return str(word_ids[i])
                        idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                        sentence = merge_words_by_layout(
                            [orig_bboxes[k] for k in idxs],
                            [orig_words[k] for k in idxs],
                            [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                            word_ids=[word_ids[k] for k in idxs],
                            prefer_id_order=True
                        )[1][0] if idxs else ""
                        if not sentence:
                            continue
                        crop = img_cv_full[cy1:cy2, cx1:cx2]
                        try:
                            preserve_polys = []
                            for k in idxs:
                                poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                if poly is not None and poly.size > 0:
                                    preserve_polys.append(poly)
                            if preserve_polys:
                                crop = _inpaint_preserve_regions(crop, preserve_polys)
                        except Exception:
                            pass
                        crop = _apply_rotation_if_needed(crop, img_path)
                        ok, buf = fast_encode_jpg(crop)
                        if not ok:
                            continue
                        img_bytes_li = bytes(buf)
                        h, w = crop.shape[:2]
                        flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                        gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                        cell_samples.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
            except Exception:
                pass
            # í…Œì´ë¸” ì…€ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ë°˜í™˜ (ë ˆì´ì•„ì›ƒ ê²°ê³¼ëŠ” í…Œì´ë¸”ê³¼ ì¤‘ë³µë  ìˆ˜ ìˆì–´ ì œì™¸)
            if 'cell_samples' in locals() and cell_samples:
                return cell_samples
            if multi_samples:
                return multi_samples
            if layout_boxes:
                merged_bboxes, merged_words = merge_words_by_layout(bboxes, words, layout_boxes, word_ids=word_ids, prefer_id_order=True)
                if merged_bboxes and merged_words:
                    bboxes, words = merged_bboxes, merged_words
        except Exception:
            pass
        
        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_file_name
        }
        # ë ˆì´ì•„ì›ƒ ë‹¨ìœ„ ë””ë²„ê·¸ ì €ì¥ ë¡œì§ ì œê±°ë¨
        
        return None
        
    except Exception as e:
        return None

def process_single_ocr_public_image(args):
    """OCR ê³µê³µ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    img_info, annotations, base_path, dataset_lookup_name, image_path_cache = args
    
    try:
        # í•„ìˆ˜ ìƒíƒœ ì´ˆê¸°í™”
        word_ids = []
        layout_boxes = None
        img_file_name = img_info.get('file_name', '')
        
        # í™•ì¥ì í™•ì¸
        if not img_file_name.endswith(('.jpg', '.png', '.jpeg')):
            img_file_name = f"{img_file_name}.jpg"
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ì°¾ê¸°
        img_path = optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
        if not img_path or not os.path.exists(img_path):
            return None
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬
        bboxes = []
        words = []
        img_id = img_info.get('id')
        
        for ann in annotations:
            bbox_coords = ann['bbox']
            try:
                # ì›ë³¸ bboxê°€ [x1, x2, x3, x4, y1, y2, y3, y4] í˜•íƒœì¸ì§€ í™•ì¸
                if len(bbox_coords) == 8:
                    # x, y ì¢Œí‘œ ë¶„ë¦¬
                    x_coords = bbox_coords[:4]  # [x1, x2, x3, x4]
                    y_coords = bbox_coords[4:]  # [y1, y2, y3, y4]
                    
                    # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¬êµ¬ì„±: [x1, y1, x2, y2, x3, y3, x4, y4]
                    pixel_coords = []
                    for i in range(4):
                        pixel_coords.extend([x_coords[i], y_coords[i]])
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['ocr_public']:
                        print(f"ğŸ“‹ OCR Public bbox í˜•íƒœ: ì›ë³¸ [x1={x_coords[0]}, x2={x_coords[1]}, x3={x_coords[2]}, x4={x_coords[3]}, y1={y_coords[0]}, y2={y_coords[1]}, y3={y_coords[2]}, y4={y_coords[3]}] -> ìˆ˜ì • [x1={pixel_coords[0]}, y1={pixel_coords[1]}, x2={pixel_coords[2]}, y2={pixel_coords[3]}, x3={pixel_coords[4]}, y3={pixel_coords[5]}, x4={pixel_coords[6]}, y4={pixel_coords[7]}]")
                        bbox_debug_flags['ocr_public'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann['text'])
                    ann_id = ann.get('id')
                    if ann_id is None:
                        ann_id = len(word_ids)
                    word_ids.append(ann_id)
                else:
                    # ê¸°ì¡´ ë¡œì§ (8ê°œê°€ ì•„ë‹Œ ê²½ìš°)
                    x_coords = [bbox_coords[0], bbox_coords[2], bbox_coords[4], bbox_coords[6]]
                    y_coords = [bbox_coords[1], bbox_coords[3], bbox_coords[5], bbox_coords[7]]
                    
                    # ì›ë³¸ í”½ì…€ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    pixel_coords = [
                        x_coords[0], y_coords[0],
                        x_coords[1], y_coords[1],
                        x_coords[2], y_coords[2],
                        x_coords[3], y_coords[3]
                    ]
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    bboxes.append(pixel_coords)
                    words.append(ann['text'])
                    ann_id = ann.get('id')
                    if ann_id is None:
                        ann_id = len(word_ids)
                    word_ids.append(ann_id)
            except (IndexError, TypeError):
                try:
                    # 4ê°œ ì¢Œí‘œì¸ì§€ í™•ì¸ (x, y, w, h)
                    x, y, w, h = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                    x1, y1, x2, y2 = x, y, x + w, y + h
                    
                    # ì›ë³¸ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ë¦¬í•‘ ì—†ìŒ)
                    pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['ocr_public']:
                        print(f"ğŸ“‹ OCR Public bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                        bbox_debug_flags['ocr_public'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann['text'])
                    ann_id = ann.get('id')
                    if ann_id is None:
                        ann_id = len(word_ids)
                    word_ids.append(ann_id)
                except (IndexError, TypeError):
                    continue
        
        # ğŸ“¦ LayoutDetection ê¸°ë°˜ ë¬¸ì¥ ë³‘í•©
        orig_bboxes = list(bboxes)
        orig_words = list(words)
        try:
            layout_boxes = run_layout_detection(img_path)
            layout_cnt = len(layout_boxes) if layout_boxes else 0
            results = []
            # 1) ë ˆì´ì•„ì›ƒ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ìƒì„±
            if layout_boxes:
                img_cv_full = _decode_image_bytes(img_data)
                if img_cv_full is not None:
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    layout_aabbs = []
                    for lb in layout_boxes:
                        x1, y1, x2, y2 = lb['coordinate']
                        layout_aabbs.append((max(0, int(x1)), max(0, int(y1)), min(W, int(x2)), min(H, int(y2))))
                    assigned = _assign_words_to_layout(word_aabbs, layout_aabbs, min_overlap_ratio=0.3)
                    for li, la in enumerate(layout_aabbs):
                        idxs = [i for i, a in enumerate(assigned) if a == li]
                        if not idxs:
                            continue
                        x1, y1, x2, y2 = la
                        sentence_list = merge_words_by_layout(
                            [orig_bboxes[k] for k in idxs],
                            [orig_words[k] for k in idxs],
                            [{'label': 'layout', 'coordinate': [x1, y1, x2, y2], 'score': 1.0}],
                            word_ids=[word_ids[k] for k in idxs],
                            prefer_id_order=True
                        )[1]
                        sentence = sentence_list[0] if sentence_list else ""
                        if not sentence:
                            continue
                        x1, y1, x2, y2 = la
                        if x2<=x1 or y2<=y1:
                            continue
                        crop = img_cv_full[y1:y2, x1:x2].copy()
                        # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                        try:
                            preserve_polys = []
                            for k in idxs:
                                poly = _flat8_to_crop_poly(orig_bboxes[k], x1, y1, x2, y2)
                                if poly is not None and poly.size > 0:
                                    preserve_polys.append(poly)
                            if preserve_polys:
                                crop = _inpaint_preserve_regions(crop, preserve_polys)
                        except Exception:
                            pass
                        # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                        crop = _apply_rotation_if_needed(crop, img_path)
                        ok, buf = fast_encode_jpg(crop)
                        if not ok:
                            continue
                        img_bytes_li = bytes(buf)
                        h, w = crop.shape[:2]
                        flat8 = [0.0,0.0,float(w),0.0,float(w),float(h),0.0,float(h)]
                        gt_li = {'bboxes':[flat8], 'words':[sentence], 'filename': f"{img_file_name}_layout_{li:02d}.jpg"}
                        results.append((f"{img_id}_{li}", img_bytes_li, gt_li))
            # 2) í…Œì´ë¸” ì…€ ìƒ˜í”Œ ì¶”ê°€
            tables = run_layout_tables(img_path)
            table_cnt = len(tables) if tables else 0
            cell_results = []
            if tables:
                table_model = get_table_model()
                img_cv_full = _decode_image_bytes(img_data)
                if table_model is not None and img_cv_full is not None:
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    # ìºì‹œëœ table_cells ìš°ì„  ì‚¬ìš©
                    cached_cells = None
                    try:
                        with PRED_CACHE_LOCK:
                            cached = PREDICTION_CACHE.get(img_path) or {}
                            cached_cells = cached.get('table_cells')
                    except Exception:
                        cached_cells = None
                    if cached_cells:
                        for (tx1, ty1, tx2, ty2, cells) in cached_cells or []:
                            for (cx1, cy1, cx2, cy2) in cells or []:
                                cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                                idxs = []
                                for wi, wa in enumerate(word_aabbs):
                                    inter = _intersection_area(wa, cell_aabb)
                                    wa_area = _area(wa)
                                    # ì…€ ë‚´ë¶€ í¬í•¨ ë¹„ìœ¨ ì™„í™”: 50% -> 20%
                                    if wa_area > 0 and (inter / wa_area) >= 0.2:
                                        idxs.append(wi)
                                if not idxs:
                                    continue
                                def _id_key(i):
                                    try:
                                        return int(str(word_ids[i]))
                                    except Exception:
                                        return str(word_ids[i])
                                idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                                sentence = merge_words_by_layout(
                                    [orig_bboxes[k] for k in idxs],
                                    [orig_words[k] for k in idxs],
                                    [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                    word_ids=[word_ids[k] for k in idxs],
                                    prefer_id_order=True
                                )[1][0] if idxs else ""
                                if not sentence:
                                    continue
                                crop = img_cv_full[cy1:cy2, cx1:cx2]
                                try:
                                    preserve_polys = []
                                    for k in idxs:
                                        poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                        if poly is not None and poly.size > 0:
                                            preserve_polys.append(poly)
                                    if preserve_polys:
                                        crop = _inpaint_preserve_regions(crop, preserve_polys)
                                except Exception:
                                    pass
                                crop = _apply_rotation_if_needed(crop, img_path)
                                ok, buf = fast_encode_jpg(crop)
                                if not ok:
                                    continue
                                img_bytes_li = bytes(buf)
                                h, w = crop.shape[:2]
                                flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                                gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                                cell_results.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                    else:
                        print(f"[cells] use BATCH tables={len(tables)} id={img_id}")
                        batch_cells = _predict_table_cells_batch(img_cv_full, tables, img_id)
                        for tx1, ty1, tx2, ty2, cells in (batch_cells or []):
                            for (cx1, cy1, cx2, cy2) in cells:
                                cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                                idxs = []
                                for wi, wa in enumerate(word_aabbs):
                                    inter = _intersection_area(wa, cell_aabb)
                                    wa_area = _area(wa)
                                    # ì…€ ë‚´ë¶€ í¬í•¨ ë¹„ìœ¨ ì™„í™”: 50% -> 20%
                                    if wa_area > 0 and (inter / wa_area) >= 0.2:
                                        idxs.append(wi)
                                if not idxs:
                                    continue
                                def _id_key(i):
                                    try:
                                        return int(str(word_ids[i]))
                                    except Exception:
                                        return str(word_ids[i])
                                idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                                sentence = merge_words_by_layout(
                                    [orig_bboxes[k] for k in idxs],
                                    [orig_words[k] for k in idxs],
                                    [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                    word_ids=[word_ids[k] for k in idxs],
                                    prefer_id_order=True
                                )[1][0] if idxs else ""
                                if not sentence:
                                    continue
                                crop = img_cv_full[cy1:cy2, cx1:cx2]
                                # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                                try:
                                    preserve_polys = []
                                    for k in idxs:
                                        poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                        if poly is not None and poly.size > 0:
                                            preserve_polys.append(poly)
                                    if preserve_polys:
                                        crop = _inpaint_preserve_regions(crop, preserve_polys)
                                except Exception:
                                    pass
                                # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                                crop = _apply_rotation_if_needed(crop, img_path)
                                ok, buf = fast_encode_jpg(crop)
                                if not ok:
                                    continue
                                img_bytes_li = bytes(buf)
                                h, w = crop.shape[:2]
                                flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                                gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                                cell_results.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
            # ì…€ì´ ìˆìœ¼ë©´ ì…€ë§Œ ë°˜í™˜ (ë ˆì´ì•„ì›ƒ ì¤‘ë³µ ì œê±°)
            if cell_results:
                _log_verbose(f"[ocr_public] keep cells: {img_file_name} layouts={layout_cnt} tables={table_cnt} cells={len(cell_results)}")
                return cell_results
            if results:
                _log_verbose(f"[ocr_public] keep layouts: {img_file_name} layouts={layout_cnt} kept={len(results)}")
                return results
            if layout_boxes:
                merged_bboxes, merged_words = merge_words_by_layout(bboxes, words, layout_boxes, word_ids=word_ids, prefer_id_order=True)
                if merged_bboxes and merged_words:
                    bboxes, words = merged_bboxes, merged_words
            _log_verbose(f"[ocr_public] exclude: {img_file_name} layouts={layout_cnt} tables={table_cnt} cells=0 (no detections)")
            return None
        except Exception:
            return None
        
    except Exception as e:
        return None

def process_single_finance_logistics_image(args):
    """ê¸ˆìœµë¬¼ë¥˜ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    sub_dataset, img_info_data, annotations_for_dataset = args
    
    if not annotations_for_dataset:
        return None
        
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_info_data['file_path'], 'rb') as f:
            img_data = f.read()
        # ì•ˆì „í•œ ì´ë¯¸ì§€ ID (sub_dataset ë˜ëŠ” íŒŒì¼ëª… ê¸°ë°˜)
        try:
            safe_img_id = str(img_info_data.get('id') or sub_dataset or os.path.splitext(os.path.basename(img_info_data['file_path']))[0])
        except Exception:
            safe_img_id = os.path.splitext(os.path.basename(img_info_data['file_path']))[0]
        
        # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ)
        bboxes = []
        words = []
        img_w = img_info_data['width']
        img_h = img_info_data['height']
        word_ids = []
        
        for ann in annotations_for_dataset:
            bbox_coords = ann.get('bbox', [])
            
            try:
                # ğŸš€ bigjson Arrayë¥¼ ì•ˆì „í•˜ê²Œ Python listë¡œ ë³€í™˜
                if hasattr(bbox_coords, '__getitem__') and not isinstance(bbox_coords, list):
                    # bigjson Arrayì¸ ê²½ìš° Python listë¡œ ë³€í™˜
                    bbox_list = []
                    try:
                        for i in range(8):  # ìµœëŒ€ 8ê°œê¹Œì§€ ì‹œë„
                            bbox_list.append(bbox_coords[i])
                    except (IndexError, TypeError):
                        # 8ê°œë³´ë‹¤ ì ìœ¼ë©´ 4ê°œ ì‹œë„
                        try:
                            bbox_list = []
                            for i in range(4):
                                bbox_list.append(bbox_coords[i])
                        except (IndexError, TypeError):
                            continue
                    bbox_coords = bbox_list
                
                # 8ê°œ ì¢Œí‘œì¸ì§€ í™•ì¸ (4ê°œ ì ì˜ x,y)
                if len(bbox_coords) >= 8:
                    # merged JSONì—ì„œ [x1,x2,x3,x4,y1,y2,y3,y4] í˜•íƒœë¥¼ [x1,y1,x2,y2,x3,y3,x4,y4]ë¡œ ë³€í™˜
                    x_coords = bbox_coords[:4]  # [x1, x2, x3, x4]
                    y_coords = bbox_coords[4:]  # [y1, y2, y3, y4]
                    
                    # ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¬êµ¬ì„±: [x1, y1, x2, y2, x3, y3, x4, y4]
                    pixel_coords = []
                    for i in range(4):
                        pixel_coords.extend([x_coords[i], y_coords[i]])
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['finance_logistics']:
                        print(f"ğŸ“‹ Finance Logistics bbox í˜•íƒœ: ì›ë³¸ [x1={x_coords[0]}, x2={x_coords[1]}, x3={x_coords[2]}, x4={x_coords[3]}, y1={y_coords[0]}, y2={y_coords[1]}, y3={y_coords[2]}, y4={y_coords[3]}] -> ìˆ˜ì • [x1={pixel_coords[0]}, y1={pixel_coords[1]}, x2={pixel_coords[2]}, y2={pixel_coords[3]}, x3={pixel_coords[4]}, y3={pixel_coords[5]}, x4={pixel_coords[6]}, y4={pixel_coords[7]}]")
                        bbox_debug_flags['finance_logistics'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann.get('text', ''))
                    ann_id = ann.get('id')
                    if ann_id is None:
                        ann_id = len(word_ids)
                    word_ids.append(ann_id)
                elif len(bbox_coords) >= 4:
                    # 4ê°œ ì¢Œí‘œì¸ì§€ í™•ì¸ (x, y, w, h)
                    x, y, w, h = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                    x1, y1, x2, y2 = x, y, x + w, y + h
                    
                    # ì›ë³¸ ì¢Œí‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í´ë¦¬í•‘ ì—†ìŒ)
                    pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                    # IC15 í‘œì¤€ ì‹œê³„ë°©í–¥ìœ¼ë¡œ ì •ê·œí™”
                    pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                    
                    # bbox í˜•íƒœ í•œ ë²ˆë§Œ ì¶œë ¥
                    if not bbox_debug_flags['finance_logistics']:
                        print(f"ğŸ“‹ Finance Logistics bbox í˜•íƒœ: ì›ë³¸ [x={x}, y={y}, w={w}, h={h}] -> í†µì¼ [x1={x1}, y1={y1}, x2={x2}, y1={y1}, x2={x2}, y2={y2}, x1={x1}, y2={y2}]")
                        bbox_debug_flags['finance_logistics'] = True
                    
                    bboxes.append(pixel_coords)
                    words.append(ann.get('text', ''))
                    ann_id = ann.get('id')
                    if ann_id is None:
                        ann_id = len(word_ids)
                    word_ids.append(ann_id)
            except (IndexError, TypeError, ValueError):
                # bboxê°€ ì˜ëª»ëœ í˜•ì‹ì´ë©´ ê±´ë„ˆë›°ê¸°
                continue
        
        # ğŸ“¦ LayoutDetection ê¸°ë°˜ ë¬¸ì¥ ë³‘í•©
        orig_bboxes = list(bboxes)
        orig_words = list(words)
        try:
            layout_boxes = run_layout_detection(img_info_data['file_path'])
            layout_cnt = len(layout_boxes) if layout_boxes else 0
            results = []
            # 1) í…Œì´ë¸” ì…€ ìƒ˜í”Œ
            tables = run_layout_tables(img_info_data['file_path'])
            table_cnt = len(tables) if tables else 0
            cell_results = []
            table_model = get_table_model()
            img_cv_full = _decode_image_bytes(img_data)
            if table_model is not None and img_cv_full is not None and tables:
                    # ë°°ì¹˜ ì˜ˆì¸¡ ê²½ë¡œ (í…Œì´ë¸” í¬ë¡­ë“¤ì„ í•œ ë²ˆì— ì˜ˆì¸¡) - ìºì‹œ ìš°ì„ 
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    # ìºì‹œëœ table_cells ë¨¼ì € í™•ì¸
                    batch_cells = None
                    try:
                        with PRED_CACHE_LOCK:
                            cached = PREDICTION_CACHE.get(img_info_data['file_path']) or {}
                            batch_cells = cached.get('table_cells')
                    except Exception:
                        batch_cells = None
                    if not batch_cells:
                        batch_cells = _predict_table_cells_batch(img_cv_full, tables, safe_img_id)
                    for tx1, ty1, tx2, ty2, cells in (batch_cells or []):
                        for (cx1, cy1, cx2, cy2) in cells:
                            cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                            idxs = []
                            for wi, wa in enumerate(word_aabbs):
                                inter = _intersection_area(wa, cell_aabb)
                                wa_area = _area(wa)
                                # ì…€ ë‚´ë¶€ í¬í•¨ ë¹„ìœ¨ ì™„í™”: 50% -> 20%
                                if wa_area > 0 and (inter / wa_area) >= 0.2:
                                    idxs.append(wi)
                            if not idxs:
                                continue
                            def _id_key(i):
                                try:
                                    return int(str(word_ids[i]))
                                except Exception:
                                    return str(word_ids[i])
                            idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                            sentence = merge_words_by_layout(
                                [orig_bboxes[k] for k in idxs],
                                [orig_words[k] for k in idxs],
                                [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                word_ids=[word_ids[k] for k in idxs],
                                prefer_id_order=True
                            )[1][0] if idxs else ""
                            if not sentence:
                                continue
                            crop = img_cv_full[cy1:cy2, cx1:cx2]
                            # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                            try:
                                preserve_polys = []
                                for k in idxs:
                                    poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                    if poly is not None and poly.size > 0:
                                        preserve_polys.append(poly)
                                if preserve_polys:
                                    crop = _inpaint_preserve_regions(crop, preserve_polys)
                            except Exception:
                                pass
                            # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                            crop = _apply_rotation_if_needed(crop, img_info_data['file_path'])
                            ok, buf = fast_encode_jpg(crop)
                            if not ok:
                                continue
                            img_bytes_li = bytes(buf)
                            h, w = crop.shape[:2]
                            flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                            gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{os.path.basename(img_info_data['file_path'])}_cell_{cx1}_{cy1}.jpg"}
                            cell_results.append((f"{safe_img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
            elif table_model is not None and img_cv_full is not None and (not tables) and int(os.environ.get("FAST_PAGE_LEVEL_CELL","0")) == 1:
                # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ í˜ì´ì§€ ì „ì²´ì—ì„œ ì…€ íƒì§€
                H, W = img_cv_full.shape[:2]
                cells = []
                try:
                    with TABLE_MODEL_LOCK:
                        _log_verbose(f"[cells] page-level predict: {os.path.basename(img_info_data['file_path'])} thr={TABLE_THRESHOLD}")
                        cell_out = table_model.predict(img_info_data['file_path'], threshold=TABLE_THRESHOLD, batch_size=1)
                    first = cell_out[0] if isinstance(cell_out, (list, tuple)) and cell_out else cell_out
                    boxes_list = []
                    if isinstance(first, dict):
                        for k in ('boxes', 'result', 'preds', 'predictions'):
                            if k in first and isinstance(first[k], (list, tuple)):
                                boxes_list = first[k]
                                break
                    else:
                        for k in ('boxes', 'result', 'preds', 'predictions'):
                            v = getattr(first, k, None)
                            if isinstance(v, (list, tuple)):
                                boxes_list = v
                                break
                    for b in boxes_list or []:
                        try:
                            _lbl = b.get('label') if isinstance(b, dict) else getattr(b, 'label', None)
                            if isinstance(_lbl, str) and ('cell' in _lbl):
                                coord = b.get('coordinate') if isinstance(b, dict) else getattr(b, 'coordinate', None)
                                if coord is None and isinstance(b, dict):
                                    coord = b.get('bbox') or b.get('box')
                                if isinstance(coord, (list, tuple)) and len(coord) >= 4:
                                    cx1, cy1, cx2, cy2 = coord[:4]
                                    cx1 = int(max(0, min(W, cx1))); cy1 = int(max(0, min(H, cy1)))
                                    cx2 = int(max(0, min(W, cx2))); cy2 = int(max(0, min(H, cy2)))
                                    if cx2 > cx1 and cy2 > cy1:
                                        cells.append((cx1, cy1, cx2, cy2))
                        except Exception:
                            continue
                except Exception:
                    cells = []
                word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                for (cx1, cy1, cx2, cy2) in cells:
                    cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                    idxs = []
                    for wi, wa in enumerate(word_aabbs):
                        inter = _intersection_area(wa, cell_aabb)
                        wa_area = _area(wa)
                        if wa_area > 0 and (inter / wa_area) >= 0.2:
                            idxs.append(wi)
                    if not idxs:
                        continue
                    def _id_key(i):
                        try:
                            return int(str(word_ids[i]))
                        except Exception:
                            return str(word_ids[i])
                    idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                    sentence = merge_words_by_layout(
                        [orig_bboxes[k] for k in idxs],
                        [orig_words[k] for k in idxs],
                        [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                        word_ids=[word_ids[k] for k in idxs],
                        prefer_id_order=True
                    )[1][0] if idxs else ""
                    if not sentence:
                        continue
                    crop = img_cv_full[cy1:cy2, cx1:cx2]
                    try:
                        preserve_polys = []
                        for k in idxs:
                            poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                            if poly is not None and poly.size > 0:
                                preserve_polys.append(poly)
                        if preserve_polys:
                            crop = _inpaint_preserve_regions(crop, preserve_polys)
                    except Exception:
                        pass
                    crop = _apply_rotation_if_needed(crop, img_info_data['file_path'])
                    ok, buf = fast_encode_jpg(crop)
                    if not ok:
                        continue
                    img_bytes_li = bytes(buf)
                    h, w = crop.shape[:2]
                    flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                    gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{os.path.basename(img_info_data['file_path'])}_cell_{cx1}_{cy1}.jpg"}
                    cell_results.append((f"{safe_img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
            elif img_cv_full is not None and tables:
                    # Fallback: TableCellsDetection ì‚¬ìš© ë¶ˆê°€ ì‹œ, í…Œì´ë¸” ë‚´ë¶€ ê° ë‹¨ì–´ë¥¼ "ì…€"ë¡œ ì·¨ê¸‰í•˜ì—¬ í¬ë¡­ ìƒì„±
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    processed_words = set()
                    # ë°°ì¹˜ ê¸°ë°˜ ì…€ ê²€ì¶œë¡œ ì „í™˜ (í‘œ í¬ë¡­ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬)
                    batch_cells = _predict_table_cells_batch(img_cv_full, tables, safe_img_id)
                    for tx1, ty1, tx2, ty2, cells in (batch_cells or []):
                        for (cx1, cy1, cx2, cy2) in (cells or []):
                            cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                            idxs = []
                            for wi, wa in enumerate(word_aabbs):
                                inter = _intersection_area(wa, cell_aabb)
                                wa_area = _area(wa)
                                # ì…€ ë‚´ë¶€ í¬í•¨ ë¹„ìœ¨ ì™„í™”: 50% -> 20%
                                if wa_area > 0 and (inter / wa_area) >= 0.2:
                                    idxs.append(wi)
                            if not idxs:
                                continue
                            def _id_key(i):
                                try:
                                    return int(str(word_ids[i]))
                                except Exception:
                                    return str(word_ids[i])
                            idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                            sentence = merge_words_by_layout(
                                [orig_bboxes[k] for k in idxs],
                                [orig_words[k] for k in idxs],
                                [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                word_ids=[word_ids[k] for k in idxs],
                                prefer_id_order=True
                            )[1][0] if idxs else ""
                            if not sentence:
                                continue
                            crop = img_cv_full[cy1:cy2, cx1:cx2]
                            # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                            try:
                                preserve_polys = []
                                for k in idxs:
                                    poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                    if poly is not None and poly.size > 0:
                                        preserve_polys.append(poly)
                                if preserve_polys:
                                    crop = _inpaint_preserve_regions(crop, preserve_polys)
                            except Exception:
                                pass
                            crop = _apply_rotation_if_needed(crop, img_path)
                            ok, buf = fast_encode_jpg(crop)
                            if not ok:
                                continue
                            img_bytes_li = bytes(buf)
                            h, w = crop.shape[:2]
                            flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                            gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                            cell_results.append((f"{img_id}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                    # ë ˆê±°ì‹œ ë‹¨ê±´ ì²˜ë¦¬ ë¹„í™œì„±í™”
                    for _ in []:
                        tx1, ty1, tx2, ty2 = map(int, tb['coordinate'])
                        tx1 = max(0, min(W, tx1)); tx2 = max(0, min(W, tx2))
                        ty1 = max(0, min(H, ty1)); ty2 = max(0, min(H, ty2))
                        if tx2 <= tx1 or ty2 <= ty1:
                            continue
                        table_aabb = (float(tx1), float(ty1), float(tx2), float(ty2))
                        for wi, wa in enumerate(word_aabbs):
                            if wi in processed_words:
                                continue
                            inter = _intersection_area(wa, table_aabb)
                            wa_area = _area(wa)
                            # í…Œì´ë¸” ë‚´ë¶€ì— 20% ì´ìƒ í¬í•¨ëœ ë‹¨ì–´ë§Œ ì‚¬ìš©
                            if wa_area <= 0 or (inter / wa_area) < 0.2:
                                continue
                            x1, y1, x2, y2 = int(wa[0]), int(wa[1]), int(wa[2]), int(wa[3])
                            # ì•ˆì „ íŒ¨ë”©
                            pad = 2
                            cx1 = max(0, x1 - pad); cy1 = max(0, y1 - pad)
                            cx2 = min(W, x2 + pad); cy2 = min(H, y2 + pad)
                            if cx2 <= cx1 or cy2 <= cy1:
                                continue
                            crop = img_cv_full[cy1:cy2, cx1:cx2]
                            # ì¸í˜ì¸íŠ¸: í•´ë‹¹ ë‹¨ì–´ë§Œ ë³´ì¡´ (ì„ íƒì )
                            try:
                                poly = _flat8_to_crop_poly(orig_bboxes[wi], cx1, cy1, cx2, cy2)
                                if poly is not None and poly.size > 0:
                                    crop = _inpaint_preserve_regions(crop, [poly])
                            except Exception:
                                pass
                            crop = _apply_rotation_if_needed(crop, img_info_data['file_path'])
                            ok, buf = fast_encode_jpg(crop)
                            if not ok:
                                continue
                            img_bytes_li = bytes(buf)
                            h, w = crop.shape[:2]
                            flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                            sentence = str(orig_words[wi]) if wi < len(orig_words) else ""
                            if not sentence:
                                continue
                            gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{os.path.basename(img_info_data['file_path'])}_wordcell_{cx1}_{cy1}.jpg"}
                            cell_results.append((f"{safe_img_id}_wordcell_{cx1}_{cy1}", img_bytes_li, gt_li))
                            processed_words.add(wi)
            # ì…€ ê²°ê³¼ë§Œ ì‚¬ìš© (ë ˆì´ì•„ì›ƒ/í´ë°± ì œê±°)
            if cell_results:
                _log_verbose(f"[finance_logistics] keep cells: {os.path.basename(img_info_data['file_path'])} layouts={layout_cnt} tables={table_cnt} cells={len(cell_results)}")
                return cell_results
            _log_verbose(f"[finance_logistics] exclude(no cells): {os.path.basename(img_info_data['file_path'])} layouts={layout_cnt} tables={table_cnt}")
            return None
        except Exception:
            pass
        
        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_info_data['filename']
        }
        # ë ˆì´ì•„ì›ƒ ë‹¨ìœ„ ë””ë²„ê·¸ ì €ì¥ ë¡œì§ ì œê±°ë¨
        
        return None
        
    except Exception as e:
        return None

def process_single_handwriting_image(args):
    """ì†ê¸€ì”¨ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    args í˜•íƒœ:
      - (img_file_name, img_info_data) (ì´ì „ í˜¸í™˜)
      - (img_file_name, img_info_data, annotations_for_image) (ì‹ ê·œ)
    """
    if len(args) == 3:
        img_file_name, img_info_data, annotations_for_image = args
    else:
        img_file_name, img_info_data = args
        annotations_for_image = []

    try:
        img_path = img_info_data['file_path']
        if not os.path.exists(img_path):
            return None

        # ì´ë¯¸ì§€ ë¡œë“œ
        with open(img_path, 'rb') as f:
            img_data = f.read()

        bboxes = []
        words = []
        word_ids = []

        # 1) ìš°ì„  merged JSONì˜ annotations ì‚¬ìš© (ìˆì„ ê²½ìš°)
        if annotations_for_image:
            for ann in annotations_for_image:
                bbox_coords = ann.get('bbox', [])
                try:
                    if isinstance(bbox_coords, list) and len(bbox_coords) >= 8:
                        # [x1,x2,x3,x4,y1,y2,y3,y4] -> interleave -> normalize
                        x_coords = bbox_coords[:4]
                        y_coords = bbox_coords[4:]
                        pixel_coords = []
                        for i in range(4):
                            pixel_coords.extend([x_coords[i], y_coords[i]])
                        pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                        if not bbox_debug_flags['handwriting']:
                            print(f"ğŸ“‹ Handwriting bbox(merged) í˜•íƒœ: x={x_coords}, y={y_coords} -> {pixel_coords}")
                            bbox_debug_flags['handwriting'] = True
                        bboxes.append(pixel_coords)
                        words.append(ann.get('text', ''))
                        ann_id = ann.get('id')
                        if ann_id is None:
                            ann_id = len(word_ids)
                        word_ids.append(ann_id)
                    elif isinstance(bbox_coords, list) and len(bbox_coords) >= 4:
                        # [x,y,w,h]
                        x, y, w, h = bbox_coords[0], bbox_coords[1], bbox_coords[2], bbox_coords[3]
                        x1, y1, x2, y2 = x, y, x + w, y + h
                        pixel_coords = [x1, y1, x2, y1, x2, y2, x1, y2]
                        pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                        bboxes.append(pixel_coords)
                        words.append(ann.get('text', ''))
                        ann_id = ann.get('id')
                        if ann_id is None:
                            ann_id = len(word_ids)
                        word_ids.append(ann_id)
                except Exception:
                    continue

        # 2) fallback: original_json_pathì—ì„œ ì§ì ‘ ì½ê¸°
        if not bboxes:
            original_json_path = img_info_data.get("original_json_path", "")
            # ê²½ë¡œê°€ ì ˆëŒ€ ê²½ë¡œê°€ ì•„ë‹ˆë©´ base_pathì™€ í•©ì¹˜ëŠ” ì²˜ë¦¬ëŠ” ìƒìœ„ì—ì„œ ë³´ì¥
            if original_json_path and os.path.exists(original_json_path):
                try:
                    json_data, json_file_handle = load_json_with_orjson(original_json_path)
                    try:
                        if 'bbox' in json_data:
                            for bbox_info in json_data['bbox']:
                                x_coords = bbox_info.get('x')
                                y_coords = bbox_info.get('y')
                                if x_coords is None or y_coords is None:
                                    continue
                                pixel_coords = []
                                for i in range(4):
                                    pixel_coords.extend([x_coords[i], y_coords[i]])
                                pixel_coords = normalize_ic15_clockwise_flat8(pixel_coords)
                                if not bbox_debug_flags['handwriting']:
                                    print(f"ğŸ“‹ Handwriting bbox(orig) í˜•íƒœ: x={x_coords}, y={y_coords} -> {pixel_coords}")
                                    bbox_debug_flags['handwriting'] = True
                                bboxes.append(pixel_coords)
                                words.append(bbox_info.get('data', ''))
                                word_ids.append(len(word_ids))
                    finally:
                        safe_close_file(json_file_handle)
                except Exception:
                    pass

        # ğŸ“¦ LayoutDetection ê¸°ë°˜ ë¬¸ì¥ ë³‘í•©
        orig_bboxes = list(bboxes)
        orig_words = list(words)
        try:
            layout_boxes = run_layout_detection(img_path)
            layout_cnt = len(layout_boxes) if layout_boxes else 0
            results = []
            # 1) í…Œì´ë¸” ì…€ ìƒ˜í”Œ
            tables = run_layout_tables(img_path)
            table_cnt = len(tables) if tables else 0
            cell_results = []
            if tables:
                table_model = get_table_model()
                img_cv_full = _decode_image_bytes(img_data)
                if table_model is not None and img_cv_full is not None:
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    # ë°°ì¹˜ ê¸°ë°˜ ì…€ ê²€ì¶œë¡œ ì „í™˜ (í‘œ í¬ë¡­ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬)
                    batch_cells = _predict_table_cells_batch(img_cv_full, tables, img_file_name)
                    for tx1, ty1, tx2, ty2, cells in (batch_cells or []):
                        for (cx1, cy1, cx2, cy2) in (cells or []):
                            cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                            idxs = []
                            for wi, wa in enumerate(word_aabbs):
                                inter = _intersection_area(wa, cell_aabb)
                                wa_area = _area(wa)
                                if wa_area > 0 and (inter / wa_area) >= 0.5:
                                    idxs.append(wi)
                            if not idxs:
                                continue
                            def _id_key(i):
                                try:
                                    return int(str(word_ids[i]))
                                except Exception:
                                    return str(word_ids[i])
                            idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                            sentence = merge_words_by_layout(
                                [orig_bboxes[k] for k in idxs],
                                [orig_words[k] for k in idxs],
                                [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                word_ids=[word_ids[k] for k in idxs],
                                prefer_id_order=True
                            )[1][0] if idxs else ""
                            if not sentence:
                                continue
                            crop = img_cv_full[cy1:cy2, cx1:cx2]
                            # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´ (ì„ íƒ)
                            try:
                                preserve_polys = []
                                for k in idxs:
                                    poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                    if poly is not None and poly.size > 0:
                                        preserve_polys.append(poly)
                                if preserve_polys:
                                    crop = _inpaint_preserve_regions(crop, preserve_polys)
                            except Exception:
                                pass
                            # ì¸ì½”ë”©
                            crop = _apply_rotation_if_needed(crop, img_path)
                            ok, buf = fast_encode_jpg(crop)
                            if not ok:
                                continue
                            img_bytes_li = bytes(buf)
                            h, w = crop.shape [:2]
                            flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                            gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                            cell_results.append((f"{img_file_name}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
                    # ë ˆê±°ì‹œ ë‹¨ê±´ ì²˜ë¦¬ ë¹„í™œì„±í™”
                    for _ in []:
                        tx1, ty1, tx2, ty2 = map(int, tb['coordinate'])
                        tx1 = max(0, min(W, tx1)); tx2 = max(0, min(W, tx2))
                        ty1 = max(0, min(H, ty1)); ty2 = max(0, min(H, ty2))
                        if tx2 <= tx1 or ty2 <= ty1:
                            continue
                        crop_path = f"/tmp/ti_table_{os.getpid()}_{img_file_name}_{tx1}_{ty1}.jpg"
                        try:
                            cv2.imwrite(crop_path, img_cv_full[ty1:ty2, tx1:tx2])
                        except Exception:
                            continue
                        with TABLE_MODEL_LOCK:
                            cell_out = table_model.predict(crop_path, threshold=TABLE_THRESHOLD, batch_size=16)
                        try:
                            os.remove(crop_path)
                        except Exception:
                            pass
                        if not cell_out:
                            continue
                        cells = []
                        try:
                            for b in getattr(cell_out[0], 'boxes', []):
                                if b.get('label') == 'cell':
                                    cx1, cy1, cx2, cy2 = b.get('coordinate', [0,0,0,0])
                                    cells.append((int(tx1 + cx1), int(ty1 + cy1), int(tx1 + cx2), int(ty1 + cy2)))
                        except Exception:
                            pass
                        for (cx1, cy1, cx2, cy2) in cells:
                            cell_aabb = (float(cx1), float(cy1), float(cx2), float(cy2))
                            idxs = []
                            for wi, wa in enumerate(word_aabbs):
                                inter = _intersection_area(wa, cell_aabb)
                                wa_area = _area(wa)
                                if wa_area > 0 and (inter / wa_area) >= 0.5:
                                    idxs.append(wi)
                            if not idxs:
                                continue
                            def _id_key(i):
                                try:
                                    return int(str(word_ids[i]))
                                except Exception:
                                    return str(word_ids[i])
                            idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                            sentence = merge_words_by_layout(
                                [orig_bboxes[k] for k in idxs],
                                [orig_words[k] for k in idxs],
                                [{'label': 'cell', 'coordinate': [cx1, cy1, cx2, cy2], 'score': 1.0}],
                                word_ids=[word_ids[k] for k in idxs],
                                prefer_id_order=True
                            )[1][0] if idxs else ""
                            if not sentence:
                                continue
                            crop = img_cv_full[cy1:cy2, cx1:cx2]
                            # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                            try:
                                preserve_polys = []
                                for k in idxs:
                                    poly = _flat8_to_crop_poly(orig_bboxes[k], cx1, cy1, cx2, cy2)
                                    if poly is not None and poly.size > 0:
                                        preserve_polys.append(poly)
                                if preserve_polys:
                                    crop = _inpaint_preserve_regions(crop, preserve_polys)
                            except Exception:
                                pass
                            # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                            crop = _apply_rotation_if_needed(crop, img_path)
                            ok, buf = fast_encode_jpg(crop)
                            if not ok:
                                continue
                            img_bytes_li = bytes(buf)
                            h, w = crop.shape[:2]
                            flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                            gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_cell_{cx1}_{cy1}.jpg"}
                            cell_results.append((f"{img_file_name}_cell_{cx1}_{cy1}", img_bytes_li, gt_li))
            # 2) ë ˆì´ì•„ì›ƒ í¬ë¡­ ìƒ˜í”Œ
            if layout_boxes:
                img_cv_full = _decode_image_bytes(img_data)
                if img_cv_full is not None:
                    H, W = img_cv_full.shape[:2]
                    word_aabbs = [_aabb_from_flat8(b) for b in orig_bboxes]
                    layout_aabbs = []
                    for lb in layout_boxes:
                        x1, y1, x2, y2 = lb['coordinate']
                        layout_aabbs.append((max(0, int(x1)), max(0, int(y1)), min(W, int(x2)), min(H, int(y2))))
                    assigned = _assign_words_to_layout(word_aabbs, layout_aabbs, min_overlap_ratio=0.3)
                    for li, la in enumerate(layout_aabbs):
                        idxs = [i for i, a in enumerate(assigned) if a == li]
                        if not idxs:
                            continue
                        x1, y1, x2, y2 = la
                        if x2 <= x1 or y2 <= y1:
                            continue
                        # ë ˆì´ì•„ì›ƒ ë°•ìŠ¤ ë‚´ ë‹¨ì–´ë“¤ì„ ë¬¸ì¥ìœ¼ë¡œ ë³‘í•©
                        def _id_key(i):
                            try:
                                return int(str(word_ids[i]))
                            except Exception:
                                return str(word_ids[i])
                        # ì¢Œ->ìš° + id ë³´ì¡° ì •ë ¬
                        idxs.sort(key=lambda j: ((word_aabbs[j][0] + word_aabbs[j][2]) / 2.0, _id_key(j)))
                        sentence = merge_words_by_layout(
                            [orig_bboxes[k] for k in idxs],
                            [orig_words[k] for k in idxs],
                            [{'label': 'layout', 'coordinate': [x1, y1, x2, y2], 'score': 1.0}],
                            word_ids=[word_ids[k] for k in idxs],
                            prefer_id_order=True
                        )[1][0] if idxs else ""
                        if not sentence:
                            continue
                        crop = img_cv_full[y1:y2, x1:x2].copy()
                        # inpaint: ë¼ë²¨ ë‹¨ì–´ë§Œ ë³´ì¡´
                        try:
                            preserve_polys = []
                            for k in idxs:
                                poly = _flat8_to_crop_poly(orig_bboxes[k], x1, y1, x2, y2)
                                if poly is not None and poly.size > 0:
                                    preserve_polys.append(poly)
                            if preserve_polys:
                                crop = _inpaint_preserve_regions(crop, preserve_polys)
                        except Exception:
                            pass
                        # ì¸í˜ì¸íŠ¸/ì¼ë°˜ í¬ë¡­ì„ JPEGë¡œ ì¸ì½”ë”© (íšŒì „ ë³´ì • í›„)
                        crop = _apply_rotation_if_needed(crop, img_path)
                        ok, buf = fast_encode_jpg(crop)
                        if not ok:
                            continue
                        img_bytes_li = bytes(buf)
                        h, w = crop.shape[:2]
                        flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                        gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_layout_{li:02d}.jpg"}
                        results.append((f"{img_file_name}_{li}", img_bytes_li, gt_li))
            # ì…€ì´ ìˆìœ¼ë©´ ì…€ë§Œ ë°˜í™˜
            if cell_results:
                _log_verbose(f"[handwriting] keep cells: {img_file_name} layouts={layout_cnt} tables={table_cnt} cells={len(cell_results)}")
                return cell_results
            if results:
                _log_verbose(f"[handwriting] keep layouts: {img_file_name} layouts={layout_cnt} kept={len(results)}")
                return results
            # 3) ë§ˆì§€ë§‰ í´ë°±: ë ˆì´ì•„ì›ƒ/ì…€ ëª¨ë‘ ì—†ìœ¼ë©´ ë‹¨ì–´ ë‹¨ìœ„ í¬ë¡­ ìƒì„±
            if orig_bboxes and orig_words:
                img_cv_full = _decode_image_bytes(img_data)
                if img_cv_full is not None:
                    H, W = img_cv_full.shape[:2]
                    word_samples = []
                    for wi, b in enumerate(orig_bboxes):
                        x1, y1, x2, y2 = map(int, _aabb_from_flat8(b))
                        # ì•ˆì „ íŒ¨ë”©
                        pad = 2
                        cx1 = max(0, x1 - pad); cy1 = max(0, y1 - pad)
                        cx2 = min(W, x2 + pad); cy2 = min(H, y2 + pad)
                        if cx2 <= cx1 or cy2 <= cy1:
                            continue
                        crop = img_cv_full[cy1:cy2, cx1:cx2]
                        # ì„ íƒì  ì¸í˜ì¸íŠ¸: í•´ë‹¹ ë‹¨ì–´ë§Œ ë³´ì¡´
                        try:
                            poly = _flat8_to_crop_poly(b, cx1, cy1, cx2, cy2)
                            if poly is not None and poly.size > 0:
                                crop = _inpaint_preserve_regions(crop, [poly])
                        except Exception:
                            pass
                        crop = _apply_rotation_if_needed(crop, img_path)
                        ok, buf = fast_encode_jpg(crop)
                        if not ok:
                            continue
                        img_bytes_li = bytes(buf)
                        h, w = crop.shape[:2]
                        flat8 = [0.0, 0.0, float(w), 0.0, float(w), float(h), 0.0, float(h)]
                        sentence = str(orig_words[wi]) if wi < len(orig_words) else ""
                        if not sentence:
                            continue
                        gt_li = {'bboxes': [flat8], 'words': [sentence], 'filename': f"{img_file_name}_word_{cx1}_{cy1}.jpg"}
                        word_samples.append((f"{img_file_name}_word_{cx1}_{cy1}", img_bytes_li, gt_li))
                    if word_samples:
                        _log_verbose(f"[handwriting] keep words(fallback): {img_file_name} word_samples={len(word_samples)}")
                        return word_samples
            # ì œì™¸
            _log_verbose(f"[handwriting] exclude: {img_file_name} layouts={layout_cnt} tables={table_cnt} cells=0 words=0")
            return None
        except Exception:
            pass

        gt_info = {
            'bboxes': bboxes,
            'words': words,
            'filename': img_info_data['filename']
        }
        # ë ˆì´ì•„ì›ƒ ë‹¨ìœ„ ë””ë²„ê·¸ ì €ì¥ ë¡œì§ ì œê±°ë¨
        return None
    except Exception:
        return None

def create_parallel_lmdb_from_args(process_args, output_path, split_name, process_func, max_workers=None, path_extractor=None, gpu_prefetch_batch_size=None):
    """ê³µí†µ ë³‘ë ¬ LMDB ìƒì„± í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½í˜•)
    path_extractor: ê° argì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë½‘ëŠ” í•¨ìˆ˜(ë°°ì¹˜ GPU ì„ ê³„ì‚°ìš©)
    gpu_prefetch_batch_size: ì„ ê³„ì‚° ë°°ì¹˜ í¬ê¸°(ì„¤ì • ì‹œ ì„ ê³„ì‚° í™œì„±í™”)
    """
    print(f"ğŸš€ {split_name} ë³‘ë ¬ LMDB ìƒì„± ì¤‘... ({len(process_args)}ê°œ ìƒ˜í”Œ)")
    
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìµœì  ì›Œì»¤ ìˆ˜
    if max_workers is None:
        # CPU ì½”ì–´ ìˆ˜ ê¸°ì¤€ ìƒí•œ 16
        _cpus = mp.cpu_count() or 16
        max_workers = min(_cpus, 16)
    print(f"  ğŸ”§ ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ")
    
    # LMDB í™˜ê²½ ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •)
    # lmdb.open(subdir=True ê¸°ë³¸)ì—ì„œëŠ” output_path(ë””ë ‰í† ë¦¬)ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•´ì•¼ í•œë‹¤.
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    os.makedirs(output_path, exist_ok=True)
    env = lmdb.open(output_path, 
                    map_size=1099511627776,  # 1TB
                    writemap=True,  # ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”
                    meminit=False,  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë¹„í™œì„±í™”
                    map_async=True)  # ë¹„ë™ê¸° ë§µí•‘
    
    # ìŠ¤í‚¤ë§ˆ/ë©”íƒ€ ëª…ì‹œ ê¸°ë¡
    try:
        txn_meta = env.begin(write=True)
        txn_meta.put('scheme'.encode(), 'det'.encode())
        txn_meta.put('format_version'.encode(), '1'.encode())
        txn_meta.put('image_ext'.encode(), 'jpg'.encode())
        txn_meta.put('serializer'.encode(), 'pickle'.encode())
        txn_meta.put('bboxes_type'.encode(), 'ic15_flat8'.encode())
        txn_meta.commit()
    except Exception:
        pass
    
    print(f"  ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ + ì¦‰ì‹œ ì €ì¥ ì‹œì‘...")
    
    idx = 0
    start_time = time.time()
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    chunk_size = 1000  # 10000ê°œì”© ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    
    # ì „ì—­ GPU í”„ë¦¬í˜ì¹˜ ì›Œì»¤ ì‹œì‘(ë¹„í™œì„±í™” ê³ ì •: ë©”ëª¨ë¦¬ ê¸‰ì¦ ë°©ì§€)
    _use_prefetch = False
    if _use_prefetch:
        try:
            _start_gpu_prefetch_worker(gpu_prefetch_batch_size)
        except Exception:
            _use_prefetch = False

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # process_argsë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ìˆœíšŒ
        for chunk_start in tqdm(range(0, len(process_args), chunk_size), desc=f"{split_name} ì²­í¬ ì²˜ë¦¬"):
            chunk_end = min(chunk_start + chunk_size, len(process_args))
            chunk_args = process_args[chunk_start:chunk_end]

            # ë™ê¸° ë ˆì´ì•„ì›ƒ ë°°ì¹˜ ì„ ì˜ˆì¸¡(ë°°ì—´ ì…ë ¥)ìœ¼ë¡œ ìºì‹œ ì±„ìš°ê¸°
            if path_extractor is not None:
                try:
                    paths = []
                    for arg in chunk_args:
                        try:
                            pth = path_extractor(arg)
                        except Exception:
                            pth = None
                        if pth and os.path.exists(pth):
                            paths.append(pth)
                    if paths:
                        paths = list(dict.fromkeys(paths))
                        _layout_predict_batch_numpy(paths, threshold=LAYOUT_THRESHOLD)
                except Exception:
                    pass

            # GPU ì„ ê³„ì‚° í íˆ¬ì…(ì˜µì…˜, ì¤‘ë³µ ì œê±°)
            if _use_prefetch:
                try:
                    paths = []
                    for arg in chunk_args:
                        try:
                            pth = path_extractor(arg)
                        except Exception:
                            pth = None
                        if pth and os.path.exists(pth):
                            paths.append(pth)
                    if paths:
                        # ì…ë ¥ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
                        paths = list(dict.fromkeys(paths))
                        _gpu_prefetch_enqueue(paths)
                except Exception:
                    pass
            
            # í˜„ì¬ ì²­í¬ì˜ futureë§Œ ìƒì„±
            futures = {executor.submit(process_func, arg) for arg in chunk_args}
            
            # ë” ì‘ì€ íŠ¸ëœì­ì…˜ ë‹¨ìœ„ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
            txn_batch_size = int(os.environ.get('FAST_TXN_BATCH', '1000'))  # í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì ˆ
            batch_count = 0
            txn = None
            
            # í˜„ì¬ ì²­í¬ì˜ ì‘ì—…ë§Œ ì²˜ë¦¬
            for future in as_completed(futures):
                result = future.result()
                
                if result is not None:
                    # ê²°ê³¼ê°€ ë‹¨ì¼ ìƒ˜í”Œ(tuple) ë˜ëŠ” ë‹¤ì¤‘ ìƒ˜í”Œ(list[tuple]) ëª¨ë‘ ì²˜ë¦¬
                    results_iter = result if isinstance(result, list) else [result]
                    for item in results_iter:
                        if item is None:
                            continue
                        try:
                            img_id, img_data, gt_info = item
                        except Exception:
                            continue
                        
                        # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘ (ë°°ì¹˜ ë‹¨ìœ„)
                        if batch_count % txn_batch_size == 0:
                            if txn is not None:
                                txn.commit()  # ì´ì „ íŠ¸ëœì­ì…˜ ì»¤ë°‹
                            txn = env.begin(write=True)  # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘
                        
                        # LMDBì— ì¦‰ì‹œ ì €ì¥
                        img_key = f'image-{idx:09d}'.encode()
                        gt_key = f'gt-{idx:09d}'.encode()
                        
                        txn.put(img_key, img_data)
                        txn.put(gt_key, pickle.dumps(gt_info))
                        idx += 1
                        batch_count += 1
                        
                        # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
                        del img_data
                        del gt_info
            
            # ë§ˆì§€ë§‰ íŠ¸ëœì­ì…˜ ì»¤ë°‹
            if txn is not None:
                txn.commit()
            del chunk_args, futures
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            print(f"  ğŸ—‘ï¸ ì²­í¬ {chunk_start//chunk_size + 1} ì™„ë£Œ: {idx}ê°œ ì²˜ë¦¬, GC {collected}ê°œ í•´ì œ")
        
        # ë§ˆì§€ë§‰ì— ìƒ˜í”Œ ìˆ˜ ì €ì¥
        txn = env.begin(write=True)
        txn.put('num-samples'.encode(), str(idx).encode())
        txn.commit()
    
    env.close()
    # GPU í”„ë¦¬í˜ì¹˜ ì›Œì»¤ ì •ë¦¬
    if _use_prefetch:
        try:
            _stop_gpu_prefetch_worker()
        except Exception:
            pass
    
    # ìµœì¢… ë©”ëª¨ë¦¬ í•´ì œ
    del process_args
    gc.collect()
    
    total_time = time.time() - start_time
    speed = idx / total_time if total_time > 0 else 0
    print(f"âœ… {split_name} ë³‘ë ¬ LMDB ìƒì„± ì™„ë£Œ: {idx}ê°œ ìƒ˜í”Œ")
    print(f"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {speed:.1f} samples/sec")
    print(f"ğŸ—‘ï¸ {split_name} ëª¨ë“  ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

def create_lmdb_text_in_wild_from_ids(base_path, images_info, image_annotations, img_ids, output_path, split_name):
    """Text in the wild ì´ë¯¸ì§€ ID ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° LMDB ìƒì„± (thread_map ë³‘ë ¬ì²˜ë¦¬ ë²„ì „)"""
    print(f"ğŸš€ {split_name} ë³‘ë ¬ LMDB ìƒì„± ì¤‘... ({len(img_ids)}ê°œ ìƒ˜í”Œ)")
    
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìµœì  ì›Œì»¤ ìˆ˜
    max_workers = min(mp.cpu_count(), 16)
    print(f"  ğŸ”§ ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ")
    
    # ğŸš€ lookup ë”•ì…”ë„ˆë¦¬ ì‚¬ì „ ë¡œë“œ
    dataset_lookup_name = "text_in_wild"
    lookup_dict = load_optimized_lookup(dataset_lookup_name)
    
    # ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„
    process_args = []
    for img_id in img_ids:
        if img_id not in images_info:
            continue
        img_info = images_info[img_id]
        annotations = image_annotations.get(img_id, [])
        process_args.append((img_id, img_info, annotations, base_path, lookup_dict))
    
    print(f"  ğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°: {len(process_args)}ê°œ")
    
    # JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ (ê°€ì¥ í° ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶€ë¶„)
    del images_info
    del image_annotations
    del img_ids
    gc.collect()
    print(f"  ğŸ—‘ï¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ + ì¦‰ì‹œ LMDB ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
    start_time = time.time()
    
    # LMDB í™˜ê²½ ìƒì„± (ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •)
    # lmdb.open(subdir=True ê¸°ë³¸)ì—ì„œëŠ” output_path(ë””ë ‰í† ë¦¬)ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•´ì•¼ í•œë‹¤.
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    os.makedirs(output_path, exist_ok=True)
    env = lmdb.open(output_path, 
                    map_size=1099511627776,  # 1TB
                    writemap=True,  # ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”
                    meminit=False,  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë¹„í™œì„±í™”
                    map_async=True)  # ë¹„ë™ê¸° ë§µí•‘
    
    # ìŠ¤í‚¤ë§ˆ/ë©”íƒ€ ëª…ì‹œ ê¸°ë¡
    try:
        txn_meta = env.begin(write=True)
        txn_meta.put('scheme'.encode(), 'det'.encode())
        txn_meta.put('format_version'.encode(), '1'.encode())
        txn_meta.put('image_ext'.encode(), 'jpg'.encode())
        txn_meta.put('serializer'.encode(), 'pickle'.encode())
        txn_meta.put('bboxes_type'.encode(), 'ic15_flat8'.encode())
        txn_meta.commit()
    except Exception:
        pass
    
    print(f"  ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ + ì¦‰ì‹œ ì €ì¥ ì‹œì‘...")
    
    idx = 0
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    chunk_size = 10000  # 10000ê°œì”© ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # process_argsë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ìˆœíšŒ
        for chunk_start in tqdm(range(0, len(process_args), chunk_size), desc=f"{split_name} ì²­í¬ ì²˜ë¦¬"):
            chunk_end = min(chunk_start + chunk_size, len(process_args))
            chunk_args = process_args[chunk_start:chunk_end]
            
            # í˜„ì¬ ì²­í¬ì˜ futureë§Œ ìƒì„±
            futures = {executor.submit(process_single_text_wild_image, arg) for arg in chunk_args}
            
            # ë” ì‘ì€ íŠ¸ëœì­ì…˜ ë‹¨ìœ„ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
            txn_batch_size = 500  # 500ê°œì”© íŠ¸ëœì­ì…˜ ë¶„í•  (ë” ì‘ê²Œ)
            batch_count = 0
            txn = None
            
            # í˜„ì¬ ì²­í¬ì˜ ì‘ì—…ë§Œ ì²˜ë¦¬
            for future in as_completed(futures):
                result = future.result()
                
                if result is not None:
                    # Text in Wildì˜ ê²½ìš° resultê°€ ë‹¤ì¤‘ ìƒ˜í”Œ(list)ì¼ ìˆ˜ ìˆìŒ
                    results_iter = result if isinstance(result, list) else [result]
                    for item in results_iter:
                        if item is None:
                            continue
                        # item í˜•íƒœ: (img_id, img_bytes, gt_info)
                        try:
                            img_id_item, img_data_item, gt_info_item = item
                        except Exception:
                            continue
                        # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘ (ë°°ì¹˜ ë‹¨ìœ„)
                        if batch_count % txn_batch_size == 0:
                            if txn is not None:
                                txn.commit()  # ì´ì „ íŠ¸ëœì­ì…˜ ì»¤ë°‹
                            txn = env.begin(write=True)  # ìƒˆ íŠ¸ëœì­ì…˜ ì‹œì‘
                        
                        # LMDBì— ì¦‰ì‹œ ì €ì¥
                        img_key = f'image-{idx:09d}'.encode()
                        gt_key = f'gt-{idx:09d}'.encode()
                        
                        txn.put(img_key, img_data_item)
                        txn.put(gt_key, pickle.dumps(gt_info_item))
                        idx += 1
                        batch_count += 1
                        
                        # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
                        del img_data_item
                        del gt_info_item
            
            # ë§ˆì§€ë§‰ íŠ¸ëœì­ì…˜ ì»¤ë°‹
            if txn is not None:
                txn.commit()
            del chunk_args, futures
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            collected = gc.collect()
            print(f"  ğŸ—‘ï¸ ì²­í¬ {chunk_start//chunk_size + 1} ì™„ë£Œ: {idx}ê°œ ì²˜ë¦¬, GC {collected}ê°œ í•´ì œ")
        
        # ë§ˆì§€ë§‰ ì»¤ë°‹ (ìƒˆ íŠ¸ëœì­ì…˜ìœ¼ë¡œ)
        txn = env.begin(write=True)
        txn.put('num-samples'.encode(), str(idx).encode())
        txn.commit()
    
    env.close()
    
    # ìµœì¢… ë©”ëª¨ë¦¬ í•´ì œ
    del process_args
    del lookup_dict
    gc.collect()
    
    total_time = time.time() - start_time
    speed = idx / total_time if total_time > 0 else 0
    print(f"âœ… {split_name} ë³‘ë ¬ LMDB ìƒì„± ì™„ë£Œ: {idx}ê°œ ìƒ˜í”Œ")
    print(f"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {speed:.1f} samples/sec")
    print(f"ğŸ—‘ï¸ {split_name} ëª¨ë“  ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

# ============================================================================
# ê³µê³µí–‰ì •ë¬¸ì„œ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_public_admin_train_valid(max_samples=500):
    """ê³µê³µí–‰ì •ë¬¸ì„œ OCR train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª ê³µê³µí–‰ì •ë¬¸ì„œ OCR train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR"
    train_json_path = f"{MERGED_JSON_PATH}/public_admin_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/public_admin_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/public_admin_train_layout.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/public_admin_valid_layout.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_public_admin_from_json(base_path, train_json_path, train_output_path, "ê³µê³µí–‰ì •ë¬¸ì„œ Train", max_samples)
        test_fast_model_input(train_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_public_admin_from_json(base_path, valid_json_path, valid_output_path, "ê³µê³µí–‰ì •ë¬¸ì„œ Valid", max_samples)
        test_fast_model_input(valid_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_public_admin_train_partly(max_samples=500):
    """ê³µê³µí–‰ì •ë¬¸ì„œ OCR train_partly LMDB ìƒì„± (í•™ìŠµ ë°ì´í„°ì…‹)"""
    print("=" * 60)
    print("ğŸ§ª ê³µê³µí–‰ì •ë¬¸ì„œ OCR train_partly LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/ê³µê³µí–‰ì •ë¬¸ì„œ OCR"
    train_json_path = f"{MERGED_JSON_PATH}/public_admin_train_partly_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/public_admin_train_partly_layout.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_public_admin_from_json(base_path, train_json_path, train_output_path, "ê³µê³µí–‰ì •ë¬¸ì„œ Train Partly", max_samples)
        test_fast_model_input(train_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")

def create_lmdb_public_admin_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    """ê³µê³µí–‰ì •ë¬¸ì„œ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„±"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    
    # JSON íŒŒì¼ ë¡œë“œ (orjson ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # imagesì™€ annotations ì²˜ë¦¬ (orjsonìœ¼ë¡œ ë¡œë“œëœ Python ë¦¬ìŠ¤íŠ¸)
        images = data.get('images', [])
        anns = data.get('annotations', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: orjson Python ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œì„ ìœ„í•´ ì¸ë±ìŠ¤ ê¸°ë°˜ ì²˜ë¦¬
        total_images = 0
        for _ in images:
            total_images += 1
        
        if max_samples and total_images > max_samples:
            print(f"ğŸ“Š {max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì´ {total_images}ê°œ ì¤‘)")
            # ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„± í›„ ì„ê¸°
            indices = list(range(total_images))
            random.seed(42)
            random.shuffle(indices)
            indices = indices[:max_samples]
        else:
            indices = list(range(total_images))
        
        # ë©”ëª¨ë¦¬ ì ˆì•½: ì´ë¯¸ì§€ë³„ ì–´ë…¸í…Œì´ì…˜ì„ ë©”ëª¨ë¦¬ dictë¡œ ë§Œë“¤ì§€ ì•Šê³  SQLite ì„ì‹œ DBì— ì €ì¥
        tmp_sqlite = f"/tmp/public_admin_anns_{os.getpid()}.db"
        try:
            if os.path.exists(tmp_sqlite):
                os.remove(tmp_sqlite)
        except Exception:
            pass
        print(f"  ğŸ’¾ ì„ì‹œ SQLite ìƒì„±: {tmp_sqlite}")
        conn = sqlite3.connect(tmp_sqlite)
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=OFF;")
        conn.execute("CREATE TABLE a(image_id INTEGER, ann TEXT)")
        # ëŒ€ìš©ëŸ‰ ì¸ì„œíŠ¸
        batch = []
        count = 0
        for ann in anns:
            try:
                img_id = ann.get('image_id', ann.get('id'))
                batch.append((int(img_id), orjson.dumps(ann).decode('utf-8')))
                if len(batch) >= 10000:
                    conn.executemany("INSERT INTO a(image_id, ann) VALUES (?,?)", batch)
                    conn.commit()
                    count += len(batch)
                    print(f"    ğŸ§± SQLite ì ì¬: {count}ê°œ")
                    batch = []
            except Exception:
                continue
        if batch:
            conn.executemany("INSERT INTO a(image_id, ann) VALUES (?,?)", batch)
            conn.commit()
            count += len(batch)
        print(f"  âœ… SQLite ì ì¬ ì™„ë£Œ: ì´ {count}ê°œ")
        # ì¦‰ì‹œ ì›ë³¸ JSON ë©”ëª¨ë¦¬ í•´ì œ
        del data
        del anns
        gc.collect()
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        print("  ğŸ”„ ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì¤‘...")
        # dataset_nameì— ë”°ë¼ ì •í™•í•œ lookup ì´ë¦„ ê²°ì •
        if 'train_partly' in dataset_name.lower() or ('train' in dataset_name.lower() and 'partly' in dataset_name.lower()):
            dataset_lookup_name = "public_admin_train_partly"
        elif 'train' in dataset_name.lower() and 'partly' not in dataset_name.lower():
            dataset_lookup_name = "public_admin_train"
        else:
            dataset_lookup_name = "public_admin_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìºì‹œ (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        image_path_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìºì‹œ ìƒì„± ì¤‘...")
            # Training í´ë”ë“¤ ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            for train_num in [1, 2, 3]:
                image_dir = f"{base_path}/Training/[ì›ì²œ]train{train_num}/02.ì›ì²œë°ì´í„°(jpg)"
                if os.path.exists(image_dir):
                    scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=('.jpg',))
                    image_path_cache.update(scanned_files)
            
            # Validation í´ë” ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            image_dir = f"{base_path}/Validation/[ì›ì²œ]validation/02.ì›ì²œë°ì´í„°(Jpg)"
            if os.path.exists(image_dir):
                scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=('.jpg',))
                image_path_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(image_path_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ (ì£¼ì„: annotationsëŠ” SQLite ê²½ë¡œ/ì´ë¯¸ì§€IDë§Œ ì „ë‹¬)
        process_args = []
        for i, img_idx in enumerate(indices):
            img_info = images[img_idx]  # orjson Python ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì ‘ê·¼
            img_id = img_info.get('id', i)
            ann_ref = {'sqlite': tmp_sqlite, 'image_id': img_id}
            process_args.append((img_info, ann_ref, base_path, lookup_func, dataset_lookup_name, image_path_cache))
        print(f"  ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(process_args)}ê°œ")
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë¦¬ìŠ¤íŠ¸ í•´ì œ
        del images
        del indices
        gc.collect()
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë¦¬ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ LMDB ìƒì„±
        gpu_bs = int(os.environ.get('FAST_LAYOUT_BATCH', '8'))
        try:
            # ì „ì—­ ê²½ë¡œ ì„¤ì • (ì›Œì»¤ì—ì„œ ì‚¬ìš©)
            global PUBLIC_ADMIN_SQLITE_PATH
            PUBLIC_ADMIN_SQLITE_PATH = tmp_sqlite
            create_parallel_lmdb_from_args(
                process_args, output_path, dataset_name, process_single_public_admin_image,
                path_extractor=_extract_path_public_admin, gpu_prefetch_batch_size=gpu_bs
            )
        finally:
            try:
                conn.close()
            except Exception:
                pass
            # ë³‘ë ¬ ì²˜ë¦¬ ì¢…ë£Œ í›„ ì„ì‹œ DB ì‚­ì œ ì‹œë„ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ìƒˆë¡œ ìƒì„±)
            try:
                if os.path.exists(tmp_sqlite):
                    os.remove(tmp_sqlite)
            except Exception:
                pass
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

# ============================================================================
# OCR ê³µê³µ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_ocr_public_train_valid(max_samples=500):
    """023.OCR ë°ì´í„°(ê³µê³µ) train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª 023.OCR ë°ì´í„°(ê³µê³µ) train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/023.OCR ë°ì´í„°(ê³µê³µ)/01-1.ì •ì‹ê°œë°©ë°ì´í„°"
    train_json_path = f"{MERGED_JSON_PATH}/ocr_public_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/ocr_public_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/ocr_public_train_layout.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/ocr_public_valid_layout.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_ocr_public_from_json(base_path, train_json_path, train_output_path, "OCR ê³µê³µ Train", max_samples)
        test_fast_model_input(train_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_ocr_public_from_json(base_path, valid_json_path, valid_output_path, "OCR ê³µê³µ Valid", max_samples)
        test_fast_model_input(valid_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_lmdb_ocr_public_from_json(base_path, json_path, output_path, dataset_name, max_samples=None, use_groups=False):
    """OCR ê³µê³µ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„±"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    
    if use_groups:
        # ê·¸ë£¹ë³„ ì²˜ë¦¬
        def process_group(group_data, original_path):
            # ê·¸ë£¹ë³„ ì²˜ë¦¬ ë¡œì§
            print(f"  ğŸ“ ê·¸ë£¹ ë°ì´í„° ì²˜ë¦¬: {len(group_data['images'])}ê°œ ì´ë¯¸ì§€")
            return len(group_data['images'])
        
        total_processed = process_json_by_groups(json_path, process_group, max_samples)
        print(f"âœ… ê·¸ë£¹ë³„ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_processed}ê°œ ì²˜ë¦¬ë¨")
        return
    
    # ê¸°ì¡´ ë°©ì‹ (ì „ì²´ JSON ë¡œë“œ)
    data, file_handle = load_json_with_orjson(json_path)
    try:
        images = data.get('images', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        if max_samples and len(images) > max_samples:
            print(f"ğŸ“Š {max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì´ {len(images)}ê°œ ì¤‘)")
            random.seed(42)
            random.shuffle(images)
            images = images[:max_samples]
        # ì´ë¯¸ì§€ë³„ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”
        image_annotations = {}
        annotations = data.get('annotations', [])
        print("  ğŸ”„ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì¤‘...")
        for ann in tqdm(annotations, desc="ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”"):
            img_id = ann.get('image_id', ann.get('id'))
            if img_id not in image_annotations:
                image_annotations[img_id] = []
            image_annotations[img_id].append(ann)
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì™„ë£Œ: {len(image_annotations)}ê°œ ì´ë¯¸ì§€")
        del data
        del annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    finally:
        safe_close_file(file_handle)
        file_handle = None
    # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
    print("  ğŸ”„ ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì¤‘...")
    dataset_lookup_name = "ocr_public_train" if 'train' in dataset_name.lower() else "ocr_public_valid"
    lookup_func = load_optimized_lookup(dataset_lookup_name)
    
    # Fallbackìš© ìºì‹œ (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
    image_path_cache = {}
    if not lookup_func:
        print("  ğŸ”„ Fallback ì´ë¯¸ì§€ ê²½ë¡œ ìºì‹œ êµ¬ì¶• ì¤‘...")
        # Training/Validation êµ¬ë¶„
        if 'train' in dataset_name.lower():
            image_dir = f"{base_path}/Training/01.ì›ì²œë°ì´í„°"
        else:
            image_dir = f"{base_path}/Validation/01.ì›ì²œë°ì´í„°"
        
        # ì‹¤ì œ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” (os.scandir ì‚¬ìš©)
        if os.path.exists(image_dir):
            scanned_files = scan_images_recursive_with_scandir(image_dir, extensions=('.jpg', '.png', '.jpeg'))
            image_path_cache.update(scanned_files)
    
    print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(image_path_cache)}ê°œ fallback ìºì‹œ'}")
    
    # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„
    process_args = []
    for img_info in images:
        img_id = img_info.get('id')
        annotations = image_annotations.get(img_id, [])
        process_args.append((img_info, annotations, base_path, dataset_lookup_name, image_path_cache))
    
    print(f"  ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(process_args)}ê°œ")
    
    # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
    del images
    del image_annotations
    print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    # ğŸš€ ë³‘ë ¬ LMDB ìƒì„±
    gpu_bs = int(os.environ.get('FAST_LAYOUT_BATCH', '8'))
    create_parallel_lmdb_from_args(
        process_args, output_path, dataset_name, process_single_ocr_public_image,
        path_extractor=_extract_path_ocr_public, gpu_prefetch_batch_size=gpu_bs
    )
    # (ì„ì‹œ ì¸ë±ìŠ¤ ì‚¬ìš© ì•ˆ í•¨)

# ============================================================================
# ê¸ˆìœµë¬¼ë¥˜ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_finance_logistics_train_valid(max_samples=None):
    """025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜) train/valid LMDB ìƒì„± (ì „ì²´ ë°ì´í„°)"""
    print("=" * 60)
    print("ğŸ§ª 025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜) train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜)/01-1.ì •ì‹ê°œë°©ë°ì´í„°"
    train_json_path = f"{MERGED_JSON_PATH}/finance_logistics_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/finance_logistics_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/finance_logistics_train_layout.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/finance_logistics_valid_layout.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_finance_logistics_from_json(base_path, train_json_path, train_output_path, "ê¸ˆìœµë¬¼ë¥˜ Train", max_samples)
        test_fast_model_input(train_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_finance_logistics_from_json(base_path, valid_json_path, valid_output_path, "ê¸ˆìœµë¬¼ë¥˜ Valid", max_samples)
        test_fast_model_input(valid_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_lmdb_finance_logistics_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    """ê¸ˆìœµë¬¼ë¥˜ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„± (ì´ˆê³ ì† ë²„ì „: orjson ì§ì ‘ ì‚¬ìš©)"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    
    # JSON íŒŒì¼ ë¡œë“œ (orjson ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # ğŸš€ ìµœì í™” 1: bigjson Array ì§ì ‘ ì‚¬ìš© (ë³€í™˜ ì—†ìŒ)
        images = data.get('images', [])
        annotations = data.get('annotations', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ - bigjson Array ì§ì ‘ ì‚¬ìš©")
        
        # ğŸš€ ìµœì í™” 2: ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        print("  ğŸ”„ ìµœì í™”ëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì¤‘...")
        dataset_lookup_name = "finance_logistics_train" if 'train' in dataset_name.lower() else "finance_logistics_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìŠ¤ìº” (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        fallback_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            # Training/Validation êµ¬ë¶„
            if 'train' in dataset_name.lower():
                scan_dirs = [f"{base_path}/Training/01.ì›ì²œë°ì´í„°"]
            else:
                scan_dirs = [f"{base_path}/Validation/01.ì›ì²œë°ì´í„°"]
            
            for scan_dir in scan_dirs:
                if os.path.exists(scan_dir):
                    scanned_files = scan_images_recursive_with_scandir(scan_dir, extensions=('.png',))
                    fallback_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(fallback_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ìµœì í™” 3: bigjson ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (500ê°œë§Œ ì„ íƒ, ë¹ ë¥´ê²Œ)
        print("  ğŸ”„ ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì¤‘...")
        image_info = {}  # sub_dataset â†’ image_info
        
        # ğŸš€ ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ (max_samplesê°€ ìˆìœ¼ë©´ ì œí•œ)
        target_count = max_samples if max_samples else None  # ì „ì²´ ë°ì´í„° ì²˜ë¦¬
        if target_count:
            print(f"  ğŸ“Š ëª©í‘œ ì´ë¯¸ì§€ ìˆ˜: {target_count}ê°œ (ì œí•œ)")
        else:
            print(f"  ğŸ“Š ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)")
        
        i = 0
        matched_count = 0
        while True:
            try:
                img = images[i]
                sub_dataset = img.get('sub_dataset', '')
                filename = f"{sub_dataset}.png"
                
                # ğŸš€ ìµœì í™”ëœ ê²½ë¡œ ì°¾ê¸°
                img_path = optimized_find_image_path(filename, base_path, dataset_lookup_name, fallback_cache)
                if img_path:
                    image_info[sub_dataset] = {
                        'file_path': img_path,
                        'width': img.get('width', 1000),
                        'height': img.get('height', 1000),
                        'filename': filename
                    }
                    matched_count += 1
                
                i += 1
                if i % 10000 == 0:
                    if target_count:
                        print(f"    ğŸ“Š ë§¤í•‘ ì§„í–‰: {i}ê°œ ì²˜ë¦¬, {matched_count}ê°œ ë§¤ì¹­ (ëª©í‘œ: {target_count}ê°œ)")
                    else:
                        print(f"    ğŸ“Š ë§¤í•‘ ì§„í–‰: {i}ê°œ ì²˜ë¦¬, {matched_count}ê°œ ë§¤ì¹­ (ì „ì²´ ì²˜ë¦¬)")
                
                # ëª©í‘œ ë‹¬ì„±ì‹œ ì¡°ê¸° ì¢…ë£Œ ğŸ¯ (target_countê°€ ì„¤ì •ëœ ê²½ìš°ë§Œ)
                if target_count and matched_count >= target_count:
                    print(f"    ğŸ¯ ëª©í‘œ ë‹¬ì„±: {matched_count}ê°œ ì´ë¯¸ì§€ ì„ íƒ ì™„ë£Œ!")
                    break
                    
            except IndexError:
                break
        
        print(f"  âœ… ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì™„ë£Œ: {len(image_info)}ê°œ")
        
        # ğŸš€ ìµœì í™” 4: ë‹¨ìˆœ ìˆœì°¨ ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ (sub_dataset ê¸°ë°˜)
        print("  ğŸ”„ ìˆœì°¨ ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬...")
        
        all_annotations = {}
        total_found = 0
        
        print(f"  ğŸš€ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘ (Iterator ë°©ì‹)")
        
        # ğŸš€ bigjson Arrayë¥¼ Iteratorë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        i = 0
        for ann in annotations:
            try:
                # ğŸš€ None ì²´í¬ë¡œ ë ê°ì§€
                if ann is None:
                    print(f"    ğŸ ì–´ë…¸í…Œì´ì…˜ ë ê°ì§€ (None) - ì´ {total_found:,}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
                    break
                
                # annì´ ë¹ˆ ê°’ì´ê±°ë‚˜ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš° ì²´í¬
                if not ann or not hasattr(ann, 'get'):
                    i += 1
                    continue
                
                sub_dataset = ann.get('sub_dataset', '')
                
                if sub_dataset in image_info:
                    if sub_dataset not in all_annotations:
                        all_annotations[sub_dataset] = []
                    
                    # ğŸš€ bigjson Array bboxë¥¼ ì•ˆì „í•˜ê²Œ Python listë¡œ ë³€í™˜
                    bbox_data = ann.get('bbox', [])
                    safe_bbox = []
                    
                    if bbox_data:
                        try:
                            # bigjson Arrayì¸ ê²½ìš° ì•ˆì „í•˜ê²Œ ë³€í™˜
                            if hasattr(bbox_data, '__getitem__') and not isinstance(bbox_data, list):
                                # ìµœëŒ€ 8ê°œê¹Œì§€ ì‹œë„
                                for j in range(8):
                                    try:
                                        safe_bbox.append(bbox_data[j])
                                    except (IndexError, TypeError):
                                        break
                            else:
                                safe_bbox = bbox_data
                        except Exception:
                            safe_bbox = []
                    
                    all_annotations[sub_dataset].append({
                        'bbox': safe_bbox,
                        'text': ann.get('text', ''),
                        'sub_dataset': sub_dataset
                    })
                    total_found += 1
                
                i += 1
                if i % 100000 == 0:
                    print(f"    ğŸ“Š ì²˜ë¦¬ ì§„í–‰: {i:,}ê°œ, ë°œê²¬: {total_found:,}ê°œ")
                    
            except Exception as e:
                if i % 100000 == 0:
                    print(f"    âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                i += 1
                continue
        
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ ì™„ë£Œ: {len(all_annotations)}ê°œ ì´ë¯¸ì§€, {total_found:,}ê°œ ì–´ë…¸í…Œì´ì…˜")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ JSON ë°ì´í„° í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        del data
        del annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ìµœì í™” 5: ë³‘í–‰ì²˜ë¦¬ LMDB ìƒì„± (ê°„ë‹¨í•œ ThreadPoolExecutor)
        print("  ğŸ”„ ë³‘í–‰ì²˜ë¦¬ LMDB ìƒì„± ì¤‘...")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ (all_annotationsë¥¼ í¬í•¨)
        process_args = []
        for sub_dataset, img_info_data in image_info.items():
            annotations_for_dataset = all_annotations.get(sub_dataset, [])
            process_args.append((sub_dataset, img_info_data, annotations_for_dataset))
        
        print(f"  ğŸš€ ë³‘í–‰ì²˜ë¦¬ ì‹œì‘: {len(process_args)}ê°œ ì´ë¯¸ì§€, 16ê°œ ì›Œì»¤")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
        del image_info
        del all_annotations
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        gpu_bs = int(os.environ.get('FAST_LAYOUT_BATCH', '8'))
        create_parallel_lmdb_from_args(
            process_args, output_path, dataset_name, process_single_finance_logistics_image,
            max_workers=16, path_extractor=_extract_path_finance_logistics, gpu_prefetch_batch_size=gpu_bs
        )
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

# ============================================================================
# ì†ê¸€ì”¨ ë°ì´í„°ì…‹ ì „ìš© í•¨ìˆ˜
# ============================================================================

def create_handwriting_train_valid(max_samples=500):
    """053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR train/valid LMDB ìƒì„±"""
    print("=" * 60)
    print("ğŸ§ª 053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR train/valid LMDB ìƒì„±")
    print("=" * 60)
    
    base_path = f"{FTP_BASE_PATH}/053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR ë°ì´í„°/01.ë°ì´í„°"
    train_json_path = f"{MERGED_JSON_PATH}/handwriting_train_merged.json"
    valid_json_path = f"{MERGED_JSON_PATH}/handwriting_valid_merged.json"
    train_output_path = f"{LOCAL_OUTPUT_PATH}/handwriting_train_layout.lmdb"
    valid_output_path = f"{LOCAL_OUTPUT_PATH}/handwriting_valid_layout.lmdb"
    
    # Training LMDB ìƒì„±
    if os.path.exists(train_json_path):
        print(f"ğŸ“Š Training JSON íŒŒì¼ ë°œê²¬: {train_json_path}")
        create_lmdb_handwriting_from_json(base_path, train_json_path, train_output_path, "ì†ê¸€ì”¨ Train", max_samples)
        test_fast_model_input(train_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Training JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_json_path}")
    
    # Validation LMDB ìƒì„±
    if os.path.exists(valid_json_path):
        print(f"ğŸ“Š Validation JSON íŒŒì¼ ë°œê²¬: {valid_json_path}")
        create_lmdb_handwriting_from_json(base_path, valid_json_path, valid_output_path, "ì†ê¸€ì”¨ Valid", max_samples)
        test_fast_model_input(valid_output_path)
        cleanup_memory()
    else:
        print(f"âŒ Validation JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {valid_json_path}")

def create_lmdb_handwriting_from_json(base_path, json_path, output_path, dataset_name, max_samples=None):
    """ì†ê¸€ì”¨ JSON íŒŒì¼ë¡œë¶€í„° LMDB ìƒì„± (orjson ìµœì í™” ë²„ì „)"""
    print(f"ğŸ§ª {dataset_name} LMDB ìƒì„± ì¤‘...")
    print(f"ğŸ“‹ bbox í˜•íƒœ: [x1, y1, x2, y1, x2, y2, x3, y3] -> [x1, y1, x2, y1, x2, y2, x3, y3] (8ê°œ ì¢Œí‘œ)")
    
    # ğŸ“„ ì†ê¸€ì”¨ëŠ” orjsonìœ¼ë¡œ ë¹ ë¥´ê²Œ ë¡œë“œ
    print(f"ğŸ“„ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {json_path}")
    with open(json_path, 'rb') as f:
        data = orjson.loads(f.read())
    print("âœ… orjson ë¡œë“œ ì„±ê³µ")
    
    try:
        # ğŸš€ ìµœì í™” 1: orjsonìœ¼ë¡œ ë¡œë“œëœ Python ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì‚¬ìš©
        images = data.get('images', [])
        print(f"ğŸ“Š JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(images)}ê°œ ì´ë¯¸ì§€")
        
        # ğŸš€ ìµœì í™” 2: scandirë¡œ ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” (í•œ ë²ˆë§Œ)
        print("  ğŸ”„ scandirë¡œ ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        filename_to_path = {}
        
        # ğŸš€ ìµœì í™”ëœ lookup í•¨ìˆ˜ í™œìš©
        dataset_lookup_name = "handwriting_train" if 'train' in dataset_name.lower() else "handwriting_valid"
        lookup_func = load_optimized_lookup(dataset_lookup_name)
        
        # Fallbackìš© ìŠ¤ìº” (ìµœì í™”ëœ lookupì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        fallback_cache = {}
        if not lookup_func:
            print("  ğŸ”„ Fallback ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            # Training/Validation êµ¬ë¶„í•´ì„œ ìŠ¤ìº” (os.scandir ì‚¬ìš©)
            if 'train' in dataset_name.lower():
                scan_dirs = [f"{base_path}/1.Training/ì›ì²œë°ì´í„°"]
            else:
                scan_dirs = [f"{base_path}/2.Validation/ì›ì²œë°ì´í„°"]
            
            for scan_dir in scan_dirs:
                if os.path.exists(scan_dir):
                    scanned_files = scan_images_recursive_with_scandir(scan_dir, extensions=('.png',))
                    fallback_cache.update(scanned_files)
        
        print(f"  âœ… ì´ë¯¸ì§€ ê²½ë¡œ ì¤€ë¹„ ì™„ë£Œ: {'ìµœì í™”ëœ lookup ì‚¬ìš©' if lookup_func else f'{len(fallback_cache)}ê°œ fallback ìºì‹œ'}")
        
        # ğŸš€ ìµœì í™” 3: ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
        print("  ğŸ”„ ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì¤‘...")
        image_info = {}  # file_name â†’ image_info
        
        target_count = max_samples if max_samples else None
        if target_count:
            print(f"  ğŸ“Š ëª©í‘œ ì´ë¯¸ì§€ ìˆ˜: {target_count}ê°œ (ì œí•œ)")
        else:
            print(f"  ğŸ“Š ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)")
        
        # orjson ë¡œë“œëœ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ len() ì‚¬ìš© ê°€ëŠ¥
        if target_count and len(images) > target_count:
            print(f"ğŸ“Š {target_count}ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì´ {len(images)}ê°œ ì¤‘)")
            random.seed(42)
            random.shuffle(images)
            images = images[:target_count]
        
        matched_count = 0
        for img in images:
            img_file_name = img.get('file_name', '')
            
            # í™•ì¥ì ì¶”ê°€
            if img_file_name and not img_file_name.endswith('.png'):
                filename = f"{img_file_name}.png"
            else:
                filename = img_file_name
            
            # ğŸš€ ìµœì í™”ëœ ê²½ë¡œ ì°¾ê¸°
            img_path = optimized_find_image_path(filename, base_path, dataset_lookup_name, fallback_cache)
            if img_path:
                image_info[img_file_name] = {
                    'file_path': img_path,
                    'width': img.get('width', 1000),
                    'height': img.get('height', 1000),
                    'filename': filename,
                    'original_json_path': img.get('original_json_path', '')
                }
                matched_count += 1
        
        print(f"  âœ… ì´ë¯¸ì§€ ì •ë³´ ë§¤í•‘ ì™„ë£Œ: {len(image_info)}ê°œ")

        # ğŸš€ ì–´ë…¸í…Œì´ì…˜ì„ image_id ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
        annotations = data.get('annotations', [])
        print("  ğŸ”„ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì¤‘...")
        image_id_to_filename = {}
        for img in images:
            fid = img.get('id')
            fname = img.get('file_name', '')
            image_id_to_filename[fid] = fname

        image_annotations = {}
        for ann in annotations:
            img_id = ann.get('image_id')
            key_fname = image_id_to_filename.get(img_id)
            if not key_fname:
                continue
            if key_fname not in image_annotations:
                image_annotations[key_fname] = []
            image_annotations[key_fname].append(ann)
        print(f"  âœ… ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™” ì™„ë£Œ: {len(image_annotations)}ê°œ ì´ë¯¸ì§€")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ JSON ë°ì´í„° í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        del data
        print(f"  ğŸ—‘ï¸ ì›ë³¸ JSON ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ì§€ë³„ ì–´ë…¸í…Œì´ì…˜ ì „ë‹¬)
        process_args = []
        for img_file_name, info in image_info.items():
            anns = image_annotations.get(img_file_name, [])
            process_args.append((img_file_name, info, anns))
        print(f"  ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(process_args)}ê°œ")
        
        # ğŸš€ ì¦‰ì‹œ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ í•´ì œ)
        del images
        del image_info
        del fallback_cache
        print(f"  ğŸ—‘ï¸ ì›ë³¸ ë”•ì…”ë„ˆë¦¬ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ğŸš€ ë³‘ë ¬ LMDB ìƒì„±
        gpu_bs = int(os.environ.get('FAST_LAYOUT_BATCH', '8'))
        create_parallel_lmdb_from_args(
            process_args, output_path, dataset_name, process_single_handwriting_image,
            path_extractor=_extract_path_handwriting, gpu_prefetch_batch_size=gpu_bs
        )
        
    except Exception as e:
        print(f"âŒ ì†ê¸€ì”¨ LMDB ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ============================================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def group_images_by_original_json(data):
    """ì´ë¯¸ì§€ë“¤ì„ original_json_pathë³„ë¡œ ê·¸ë£¹í™”"""
    groups = {}
    
    for img in data.get('images', []):
        original_path = img.get('original_json_path', '')
        if original_path not in groups:
            groups[original_path] = []
        groups[original_path].append(img)
    
    return groups

def process_json_by_groups(json_path, process_func, max_samples=None):
    """JSON íŒŒì¼ì„ ì›ë³¸ íŒŒì¼ë³„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬"""
    print(f"ğŸ“„ JSON íŒŒì¼ì„ ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬ ì¤‘: {json_path}")
    
    # JSON íŒŒì¼ ë¡œë“œ (fallback ë°©ì‹)
    data, file_handle = load_json_with_orjson(json_path)
    
    try:
        # ì›ë³¸ JSON íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
        groups = group_images_by_original_json(data)
        print(f"ğŸ“Š ì´ {len(groups)}ê°œì˜ ì›ë³¸ JSON íŒŒì¼ ê·¸ë£¹ ë°œê²¬")
        
        # ê° ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬
        total_processed = 0
        for original_path, images in groups.items():
            if max_samples and total_processed >= max_samples:
                break
                
            print(f"ğŸ” ê·¸ë£¹ ì²˜ë¦¬ ì¤‘: {os.path.basename(original_path)} ({len(images)}ê°œ ì´ë¯¸ì§€)")
            
            # ê·¸ë£¹ë³„ ë°ì´í„° êµ¬ì„±
            group_data = {
                'images': images,
                'annotations': [ann for ann in data.get('annotations', []) 
                              if any(img.get('original_json_path') == original_path 
                                    for img in images if img.get('id') == ann.get('image_id'))],
                'info': data.get('info', {}),
                'categories': data.get('categories', [])
            }
            
            # ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
            processed_count = process_func(group_data, original_path)
            total_processed += processed_count
            
            print(f"âœ… ê·¸ë£¹ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬ë¨ (ì´ {total_processed}ê°œ)")
        
        return total_processed
        
    finally:
        # íŒŒì¼ í•¸ë“¤ ì •ë¦¬
        safe_close_file(file_handle)

def test_fast_model_input(lmdb_path):
    """ìƒì„±ëœ LMDBê°€ FAST ëª¨ë¸ì˜ ì…ë ¥ í˜•ì‹ì— ë§ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ” FAST ëª¨ë¸ ì…ë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸: {lmdb_path}")
    
    try:
        dataset = FAST_LMDB(
            lmdb_path=lmdb_path,
            split='train',
            is_transform=False,
            img_size=(640, 640),
            short_size=640
        )
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
        
        # ëª‡ ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        for i in range(min(5, len(dataset))):
            print(f"\nğŸ§ª ìƒ˜í”Œ {i+1} í…ŒìŠ¤íŠ¸:")
            
            img, gt_info = dataset.get_image_and_gt(i)
            print(f"   - ì›ë³¸ ì´ë¯¸ì§€ í˜•íƒœ: {img.shape}")
            print(f"   - ë°”ìš´ë”© ë°•ìŠ¤ ìˆ˜: {len(gt_info['bboxes'])}")
            print(f"   - í…ìŠ¤íŠ¸ ìˆ˜: {len(gt_info['words'])}")
            print(f"   - íŒŒì¼ëª…: {gt_info['filename']}")
            
            if gt_info['bboxes']:
                print(f"   - ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸: {gt_info['words'][0]}")
                if len(gt_info['words']) > 1:
                    print(f"   - ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸: {gt_info['words'][1]}")
                if len(gt_info['words']) > 2:
                    print(f"   - ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸: {gt_info['words'][2]}")
        
        print(f"âœ… FAST ëª¨ë¸ ì…ë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ FAST ëª¨ë¸ ì…ë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def _predict_table_cells_batch(img_cv_full, tables, id_tag):
    """ì—¬ëŸ¬ í…Œì´ë¸” ì˜ì—­ì— ëŒ€í•´ ì…€ ë°•ìŠ¤ë¥¼ ë°°ì¹˜ ì˜ˆì¸¡.
    ë°˜í™˜: [(tx1,ty1,tx2,ty2, [(cx1,cy1,cx2,cy2), ...]), ...]
    """
    if img_cv_full is None or img_cv_full.size == 0 or not tables:
        return []
    table_model = get_table_model()
    if table_model is None:
        return []
    H, W = img_cv_full.shape[:2]
    # ìœ íš¨ í…Œì´ë¸”ë§Œ ì •ë¦¬
    table_regions = []
    for tb in tables:
        try:
            tx1, ty1, tx2, ty2 = map(int, tb['coordinate'])
            tx1 = max(0, min(W, tx1)); tx2 = max(0, min(W, tx2))
            ty1 = max(0, min(H, ty1)); ty2 = max(0, min(H, ty2))
            if tx2 <= tx1 or ty2 <= ty1:
                continue
            table_regions.append((tx1, ty1, tx2, ty2))
        except Exception:
            continue
    if not table_regions:
        return []
    # ì „ì—­ ë°°ì²˜(ì—¬ëŸ¬ ì´ë¯¸ì§€ ê°„ ì§‘ê³„ ë°°ì¹˜) ì‚¬ìš© ì—¬ë¶€ - ì½”ë“œ ê³ ì •(ë©”ëª¨ë¦¬ ê°€ë“œ ì ìš©)
    use_agg = True
    if use_agg:
        # ìš”ì²­ë“¤ì„ ì „ë¶€ ì œì¶œ í›„, ê²°ê³¼ ì¼ê´„ ëŒ€ê¸°
        batcher = _get_global_table_batcher()
        reqs = []
        for (tx1, ty1, tx2, ty2) in table_regions:
            try:
                crop = img_cv_full[ty1:ty2, tx1:tx2]
                if crop is None or crop.size == 0:
                    continue
                # ë©”ëª¨ë¦¬/VRAM í”¼í¬ ì™„í™”: ìµœì¥ë³€ ìƒí•œìœ¼ë¡œ ë‹¤ìš´ìŠ¤ì¼€ì¼
                ch, cw = crop.shape[:2]
                m = max(ch, cw)
                if m > CELL_CROP_MAX_SIDE:
                    scale = CELL_CROP_MAX_SIDE / float(m)
                    new_w = max(1, int(cw * scale))
                    new_h = max(1, int(ch * scale))
                    crop = cv2.resize(crop, (new_w, new_h), interpolation=cv2.INTER_AREA)
                fut = batcher.submit(crop, (tx1, ty1, tx2, ty2))
                reqs.append(((tx1, ty1, tx2, ty2), fut))
            except Exception:
                continue
        results = []
        for (tx1, ty1, tx2, ty2), fut in reqs:
            # ê²°ê³¼ ëŒ€ê¸° ì‹œê°„ ê³ ì •(ë©”ëª¨ë¦¬ ë°©ì–´ ëª©ì )
            cells = fut.result(timeout=60.0)
            if not isinstance(cells, list):
                cells = []
            results.append((tx1, ty1, tx2, ty2, cells))
        print(f"[cell/agg] collected={len(reqs)} returned={len(results)}")
        return results
    # ë©”ëª¨ë¦¬ ë°°ì—´ ê¸°ë°˜ ë°°ì¹˜ ì˜ˆì¸¡ (ì„ì‹œ íŒŒì¼ ì‚¬ìš© ì•ˆ í•¨, ë‹¨ì¼ ì´ë¯¸ì§€ ë‚´ ë°°ì¹˜)
    print(f"[cell/batch] start total_crops={len(table_regions)} in_memory=1")
    if not table_regions:
        return []
    table_bs = 8  # ë³´ìˆ˜ì  ê³ ì •
    results = []
    total_cells = 0
    t_batch0 = time.time()
    for s in range(0, len(table_regions), table_bs):
        chunk = table_regions[s:s+table_bs]
        batch = []
        for (tx1, ty1, tx2, ty2) in chunk:
            try:
                crop = img_cv_full[ty1:ty2, tx1:tx2]
                if crop is None or crop.size == 0:
                    continue
                ch, cw = crop.shape[:2]
                m = max(ch, cw)
                if m > CELL_CROP_MAX_SIDE:
                    scale = CELL_CROP_MAX_SIDE / float(m)
                    new_w = max(1, int(cw * scale))
                    new_h = max(1, int(ch * scale))
                    crop = cv2.resize(crop, (new_w, new_h), interpolation=cv2.INTER_AREA)
                batch.append((crop, (tx1, ty1, tx2, ty2)))
            except Exception:
                continue
        if not batch:
            continue
        batch_arrays = [arr for arr, _ in batch]
        bs = min(len(batch_arrays), table_bs)
        t0 = time.time()
        with TABLE_MODEL_LOCK:
            cell_out_list = table_model.predict(batch_arrays, threshold=TABLE_THRESHOLD, batch_size=bs)
        t1 = time.time()
        out_cells = 0
        for (_, region), cell_out in zip(batch, cell_out_list or []):
            tx1, ty1, tx2, ty2 = region
            cells = []
            try:
                first = cell_out[0] if isinstance(cell_out, (list, tuple)) and cell_out else cell_out
                try:
                    dbg_type = type(first).__name__
                    if isinstance(first, dict):
                        dbg_keys = list(first.keys())[:16]
                    else:
                        dbg_keys = [k for k in ('boxes','result','preds','predictions') if getattr(first,k,None) is not None]
                    print(f"[debug] cells/raw: type={dbg_type} keys={dbg_keys}")
                except Exception:
                    pass
                parsed = _extract_cell_boxes(first)
                for b in parsed or []:
                    cx1, cy1, cx2, cy2 = b['coordinate']
                    cells.append((int(tx1 + cx1), int(ty1 + cy1), int(tx1 + cx2), int(ty1 + cy2)))
            except Exception:
                cells = []
            results.append((tx1, ty1, tx2, ty2, cells))
            out_cells += len(cells)
        print(f"[cell/chunk] {s//table_bs+1}/{(len(table_regions)+table_bs-1)//table_bs} n={len(batch_arrays)} bs={bs} ms={(t1-t0)*1000:.1f} cells={out_cells}")
        total_cells += out_cells
        # ë©”ëª¨ë¦¬ í•´ì œ íŒíŠ¸
        del batch_arrays
    t_batch1 = time.time()
    print(f"[cell/batch] done total_crops={len(table_regions)} total_cells={total_cells} total_ms={(t_batch1-t_batch0)*1000:.1f}")
    return results

def _gpu_prefetch_layout_for_paths(img_paths, batch_size=8):
    """ì£¼ì–´ì§„ ê²½ë¡œë“¤ì— ëŒ€í•´ LayoutDetectionì„ ë°°ì¹˜ë¡œ ìˆ˜í–‰í•˜ê³  ìºì‹œì— ì €ì¥."""
    if not img_paths:
        return
    try:
        # ì „ì—­ í…Œì´ë¸” ë°°ì²˜ê°€ í•„ìš”í•˜ë©´ ì´ˆê¸°í™”ë§Œ ì„ í–‰(ëª¨ë¸ lazy init)
        try:
            if str(os.environ.get('FAST_TABLE_AGG', '1')).lower() in ('1','true','yes','y'):
                _get_global_table_batcher()
        except Exception:
            pass
        model = get_layout_model()
        paths = [p for p in img_paths if p and os.path.exists(p)]
        if not paths:
            return
        for i in range(0, len(paths), batch_size):
            batch = paths[i:i+batch_size]
            try:
                t0 = time.time()
                with LAYOUT_MODEL_LOCK:
                    out_list = model.predict(batch, batch_size=len(batch), layout_nms=True, threshold=LAYOUT_THRESHOLD)
                t1 = time.time()
                _log_verbose(f"[layout/prefetch] batch={len(batch)} ms={(t1-t0)*1000:.1f}")
            except Exception:
                out_list = []
                for p in batch:
                    try:
                        with LAYOUT_MODEL_LOCK:
                            single = model.predict(p, batch_size=1, layout_nms=True, threshold=LAYOUT_THRESHOLD)
                        out_list.append(single[0] if single else None)
                    except Exception:
                        out_list.append(None)
            for p, res in zip(batch, out_list):
                boxes = []
                try:
                    for b in getattr(res, 'boxes', []):
                        label = b.get('label')
                        coord = b.get('coordinate')
                        if label in LAYOUT_LABELS_TO_USE and isinstance(coord, (list, tuple)) and len(coord) == 4:
                            boxes.append({'label': label, 'coordinate': [float(coord[0]), float(coord[1]), float(coord[2]), float(coord[3])], 'score': float(b.get('score', 1.0))})
                except Exception:
                    boxes = []
                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œë¡œ í…Œì´ë¸” ë¼ë²¨ ì¶”ì¶œ
                tables = [b for b in boxes if isinstance(b.get('label'), str) and b.get('label').lower() == 'table']
                _cache_update(p, layout=boxes, tables=tables)
                # ì„ íƒ: í…Œì´ë¸” ì…€ê¹Œì§€ ì‚¬ì „ ì˜ˆì¸¡í•˜ì—¬ ìºì‹œì— ì €ì¥
                if PREFETCH_TABLES and tables:
                    try:
                        with open(p, 'rb') as _f:
                            _arr = np.frombuffer(_f.read(), dtype=np.uint8)
                        img_cv_full = cv2.imdecode(_arr, cv2.IMREAD_COLOR)
                    except Exception:
                        img_cv_full = None
                    if img_cv_full is not None:
                        try:
                            cells = _predict_table_cells_batch(img_cv_full, tables, f"pf")
                        except Exception:
                            cells = None
                        if cells:
                            _cache_update(p, table_cells=cells)  # [(tx1,ty1,tx2,ty2,[(cx1,cy1,cx2,cy2)..]), ...]
    except Exception:
        pass


def _prefetch_predictions_for_args(args_list, path_extractor, batch_size=8):
    """args ë¦¬ìŠ¤íŠ¸ì—ì„œ ê²½ë¡œë¥¼ ì¶”ì¶œí•´ GPU ë°°ì¹˜ ì„ ê³„ì‚°."""
    try:
        paths = []
        for arg in args_list:
            try:
                p = path_extractor(arg)
            except Exception:
                p = None
            if p and os.path.exists(p):
                paths.append(p)
        if not paths:
            return
        # ì¤‘ë³µ ì œê±° (ì…ë ¥ ìˆœì„œ ìœ ì§€)
        paths = list(dict.fromkeys(paths))
        _gpu_prefetch_layout_for_paths(paths, batch_size=batch_size)
    except Exception:
        pass


def _extract_path_text_in_wild(arg):
    """(img_id, img_info, annotations, base_path, lookup_dict) -> img_path"""
    try:
        _, img_info, _, base_path, lookup_dict = arg
        img_file_name = img_info.get('file_name', '')
        if img_file_name and not img_file_name.endswith('.jpg'):
            img_file_name = f"{img_file_name}.jpg"
        if lookup_dict and isinstance(lookup_dict, dict):
            if img_file_name in lookup_dict:
                return lookup_dict[img_file_name]
            for ext in ['.png', '.jpeg']:
                alt = img_file_name.replace('.jpg', ext)
                if alt in lookup_dict:
                    return lookup_dict[alt]
        img_type = img_info.get('type', 'book')
        if img_type == "book":
            image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
        elif img_type == "sign":
            image_dir = f"{base_path}/01_textinthewild_signboard_images_new/01_textinthewild_signboard_images_new/Signboard"
        elif img_type == "traffic sign":
            image_dir = f"{base_path}/01_textinthewild_traffic_sign_images_new/01_textinthewild_traffic_sign_images_new/Traffic_Sign"
        elif img_type == "product":
            image_dir = f"{base_path}/01_textinthewild_goods_images_new/01_textinthewild_goods_images_new/Goods"
        else:
            image_dir = f"{base_path}/01_textinthewild_book_images_new/01_textinthewild_book_images_new/book"
        return os.path.join(image_dir, img_file_name) if img_file_name else None
    except Exception:
        return None


def _extract_path_public_admin(arg):
    """(img_info, annotations, base_path, lookup_dict, dataset_lookup_name, image_path_cache) -> img_path"""
    try:
        img_info, _, base_path, _, dataset_lookup_name, image_path_cache = arg
        img_file_name = img_info.get('image.file.name', '')
        if not img_file_name:
            return None
        return optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
    except Exception:
        return None


def _extract_path_ocr_public(arg):
    """(img_info, annotations, base_path, dataset_lookup_name, image_path_cache) -> img_path"""
    try:
        img_info, _, base_path, dataset_lookup_name, image_path_cache = arg
        img_file_name = img_info.get('file_name', '')
        if img_file_name and not img_file_name.endswith(('.jpg', '.png', '.jpeg')):
            img_file_name = f"{img_file_name}.jpg"
        return optimized_find_image_path(img_file_name, base_path, dataset_lookup_name, image_path_cache)
    except Exception:
        return None


def _extract_path_finance_logistics(arg):
    """(sub_dataset, img_info_data, annotations_for_dataset) -> file_path"""
    try:
        _, info, _ = arg
        return info.get('file_path')
    except Exception:
        return None


def _extract_path_handwriting(arg):
    """(img_file_name, img_info_data, anns) -> file_path"""
    try:
        _, info, _ = arg if len(arg) == 3 else (arg[0], arg[1], [])
        return info.get('file_path')
    except Exception:
        return None

# ============================================================================
# ì „ì—­ í…Œì´ë¸” ë°°ì²˜ (ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ í…Œì´ë¸” í¬ë¡­ì„ ëª¨ì•„ ëŒ€ë°°ì¹˜ ì¶”ë¡ )
# ============================================================================
class _TableCellsFuture:
    def __init__(self):
        import threading
        self._event = threading.Event()
        self._result = None
    def set_result(self, value):
        self._result = value
        try:
            self._event.set()
        except Exception:
            pass
    def result(self, timeout=None):
        try:
            ok = self._event.wait(timeout)
            if not ok:
                return None
        except Exception:
            return None
        return self._result

class _TableBatcher:
    def __init__(self, table_bs=24, timeout_ms=50):
        import threading
        self._q = []
        self._lock = threading.Lock()
        self._cv = threading.Condition(self._lock)
        self._table_bs = int(table_bs)
        self._timeout_ms = int(timeout_ms)
        self._max_pending = int(TABLE_AGG_MAX_PENDING)
        self._thr = threading.Thread(target=self._worker, daemon=True)
        self._thr.start()
    def submit(self, crop_array, region_tuple):
        fut = _TableCellsFuture()
        with self._cv:
            # ëŒ€ê¸°ì—´ì´ ê°€ë“ ì°¨ë©´ ê³µê°„ ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°(ë°±í”„ë ˆì…”)
            while len(self._q) >= self._max_pending:
                self._cv.wait(timeout=0.05)
            self._q.append((crop_array, region_tuple, fut))
            self._cv.notify()
        return fut
    def _worker(self):
        import time
        model = get_table_model()
        if model is None:
            return
        while True:
            try:
                with self._cv:
                    t_start = None
                    batch = []
                    while True:
                        if self._q:
                            if t_start is None:
                                t_start = time.time()
                            batch.append(self._q.pop(0))
                            if len(batch) >= self._table_bs:
                                break
                        else:
                            # ëŒ€ê¸°ì—´ì´ ë¹„ì—ˆìœ¼ë©´ ëŒ€ê¸°
                            self._cv.wait(timeout=self._timeout_ms / 1000.0)
                        if t_start is not None and (time.time() - t_start) * 1000.0 >= self._timeout_ms:
                            break
                    if not batch:
                        continue
                    # í ì†Œë¹„ ì•Œë¦¼(í”„ë¡œë“€ì„œ ê¹¨ìš°ê¸°)
                    self._cv.notify_all()
                # ë½ ë°–ì—ì„œ ì¶”ë¡ 
                arrays = [arr for (arr, _, _) in batch]
                regions = [r for (_, r, _) in batch]
                futs = [f for (_, _, f) in batch]
                bs = len(arrays)
                t0 = time.time()
                with TABLE_MODEL_LOCK:
                    out_list = model.predict(arrays, threshold=TABLE_THRESHOLD, batch_size=bs)
                t1 = time.time()
                total_cells = 0
                results_cells = []
                for region, out in zip(regions, out_list or []):
                    tx1, ty1, tx2, ty2 = region
                    cells = []
                    try:
                        first = out[0] if isinstance(out, (list, tuple)) and out else out
                        parsed = _extract_cell_boxes(first)
                        for b in parsed or []:
                            cx1, cy1, cx2, cy2 = b['coordinate']
                            cells.append((int(tx1 + cx1), int(ty1 + cy1), int(tx1 + cx2), int(ty1 + cy2)))
                    except Exception:
                        cells = []
                    results_cells.append(cells)
                    total_cells += len(cells)
                # futures í•´ì œ
                for fut, cells in zip(futs, results_cells):
                    fut.set_result(cells)
                print(f"[cell/agg] batch n={bs} ms={(t1-t0)*1000:.1f} cells={total_cells}")
                # ë©”ëª¨ë¦¬ í•´ì œ íŒíŠ¸
                del arrays
            except Exception:
                # ì—ëŸ¬ ì‹œ ì ê¹ ì‰¬ê³  ë£¨í”„ ì§€ì†
                time.sleep(0.01)

_TABLE_BATCHER = None
_TABLE_BATCHER_LOCK = threading.Lock()

def _get_global_table_batcher():
    global _TABLE_BATCHER
    if _TABLE_BATCHER is not None:
        return _TABLE_BATCHER
    with _TABLE_BATCHER_LOCK:
        if _TABLE_BATCHER is not None:
            return _TABLE_BATCHER
        # ì½”ë“œ ê³ ì •ê°’ìœ¼ë¡œ ì´ˆê¸°í™”(í™˜ê²½ë³€ìˆ˜ ë¯¸ì‚¬ìš©)
        bs = 8
        timeout_ms = 50
        _TABLE_BATCHER = _TableBatcher(table_bs=bs, timeout_ms=timeout_ms)
        print(f"[cell/agg] initialized table_bs={bs} timeout_ms={timeout_ms}")
        return _TABLE_BATCHER


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ëª¨ë“  í•œêµ­ì–´ OCR ë°ì´í„°ì…‹ train/valid LMDB ìƒì„± (ì „ì²´ ë°ì´í„°, ì œí•œ ì—†ìŒ)")
    print("=" * 60)
    
    # gvfs FTP ê²½ë¡œ í™•ì¸
    if not is_ftp_mounted():
        print("âŒ gvfs FTP ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
        print("ğŸ’¡ íŒŒì¼ ê´€ë¦¬ìì—ì„œ FTP ì„œë²„ì— ì ‘ì†í•˜ì—¬ gvfs ë§ˆìš´íŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”")
        return
    
    if not os.path.exists(FTP_BASE_PATH):
        print("âŒ gvfs FTP ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨")
        return
    
    print("âœ… gvfs FTP ê²½ë¡œ í™•ì¸ ì™„ë£Œ")
    
    # ğŸš€ ìµœì í™”ëœ lookup íŒŒì¼ ìƒíƒœ í™•ì¸ (pickle ìš°ì„ )
    print("\nğŸ” ìµœì í™”ëœ lookup íŒŒì¼ ìƒíƒœ í™•ì¸:")
    datasets = [
        "handwriting_train", "handwriting_valid", 
        "finance_logistics_train", "finance_logistics_valid",
        "ocr_public_train", "ocr_public_valid",
        "public_admin_train", "public_admin_train_partly", "public_admin_valid"
    ]
    
    available_count = 0
    pickle_count = 0
    py_count = 0
    
    for dataset in datasets:
        pkl_gz_file = f"FAST/lookup_{dataset}.pkl.gz"
        pkl_file = f"FAST/lookup_{dataset}.pkl"
        py_file = f"FAST/optimized_lookup_{dataset}.py"
        
        if os.path.exists(pkl_gz_file):
            print(f"  ğŸš€ {dataset} (ì••ì¶•ëœ pickle - ìµœê³ ì†)")
            available_count += 1
            pickle_count += 1
        elif os.path.exists(pkl_file):
            print(f"  âš¡ {dataset} (pickle - ê³ ì†)")
            available_count += 1
            pickle_count += 1
        elif os.path.exists(py_file):
            print(f"  ğŸŒ {dataset} (Python ëª¨ë“ˆ - ì €ì†)")
            available_count += 1
            py_count += 1
        else:
            print(f"  âš ï¸ {dataset} (fallback ì‚¬ìš©)")
    
    print(f"\nğŸ“Š ìµœì í™”ëœ lookup: {available_count}/{len(datasets)}ê°œ ì‚¬ìš© ê°€ëŠ¥")
    print(f"   ğŸš€ Pickle: {pickle_count}ê°œ (ê³ ì†)")
    print(f"   ğŸŒ Python: {py_count}ê°œ (ì €ì†)")
    
    if available_count == 0:
        print("ğŸ’¡ ftp_tree_viewer.pyë¥¼ ì‹¤í–‰í•´ì„œ ìµœì í™”ëœ lookup í•¨ìˆ˜ë“¤ì„ ìƒì„±í•˜ë©´ ì†ë„ê°€ ëŒ€í­ ê°œì„ ë©ë‹ˆë‹¤!")
        print("ğŸ’¡ ê·¸ ë‹¤ìŒ convert_lookup_to_pickle.pyë¥¼ ì‹¤í–‰í•´ì„œ pickleë¡œ ë³€í™˜í•˜ë©´ ë”ìš± ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
    elif pickle_count == 0 and py_count > 0:
        print("ğŸ’¡ convert_lookup_to_pickle.pyë¥¼ ì‹¤í–‰í•´ì„œ Python ëª¨ë“ˆì„ pickleë¡œ ë³€í™˜í•˜ë©´ 5-10ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
    elif pickle_count < len(datasets):
        print("ğŸ’¡ ì¼ë¶€ lookupë§Œ pickleë¡œ ìµœì í™”ë¨. ëˆ„ë½ëœ ê²ƒë“¤ì€ convert_lookup_to_pickle.pyë¡œ ë³€í™˜í•˜ì„¸ìš”!")
    else:
        print("ğŸš€ ëª¨ë“  lookupì´ pickleë¡œ ìµœì í™”ë¨! ìµœê³  ì„±ëŠ¥ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤!")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(LOCAL_OUTPUT_PATH, exist_ok=True)
    
    # LMDB ìœ íš¨ì„±(ìƒ˜í”Œ ì¡´ì¬) ê²€ì‚¬ í•¨ìˆ˜
    def _lmdb_has_samples(lmdb_path: str) -> bool:
        try:
            if not os.path.exists(lmdb_path):
                return False
            env = lmdb.open(lmdb_path, readonly=True, lock=False, readahead=False)
            with env.begin() as txn:
                num = txn.get('num-samples'.encode())
                if num is not None:
                    try:
                        return int(num) > 0
                    except Exception:
                        return False
                # num-samples í‚¤ê°€ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ í‚¤ ì¡´ì¬ ì—¬ë¶€ë¡œ ëŒ€ì²´ í™•ì¸
                cur = txn.cursor()
                try:
                    # ìš°ì„  image- ì ‘ë‘ì‚¬ íƒìƒ‰
                    if cur.set_range(b'image-'):
                        return True
                    # ì•„ë¬´ í‚¤ë‚˜ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì™„ë£Œë¡œ ê°„ì£¼
                    if cur.first():
                        return True
                    return False
                finally:
                    cur.close()
        except Exception:
            return False
    
    # ì´ë¯¸ ì™„ë£Œëœ LMDB í™•ì¸
    completed_lmdbs = []
    lmdb_paths = [
        f"{LOCAL_OUTPUT_PATH}/text_in_wild_train_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/text_in_wild_valid_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/public_admin_train_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/public_admin_train_partly_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/public_admin_valid_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/ocr_public_train_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/ocr_public_valid_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/finance_logistics_train_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/finance_logistics_valid_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/handwriting_train_layout.lmdb",
        f"{LOCAL_OUTPUT_PATH}/handwriting_valid_layout.lmdb"
    ]
    
    for lmdb_path in lmdb_paths:
        if os.path.exists(lmdb_path):
            completed_lmdbs.append(lmdb_path)
            print(f"âœ… ì´ë¯¸ ì™„ë£Œë¨: {os.path.basename(lmdb_path)}")
    
    # ê° ë°ì´í„°ì…‹ë³„ë¡œ train/valid LMDB ìƒì„± (ì™„ë£Œëœ ê²ƒ ì œì™¸) - ì „ì²´ ë°ì´í„° ì²˜ë¦¬
    _tiw_train = f"{LOCAL_OUTPUT_PATH}/text_in_wild_train_layout.lmdb"
    _tiw_valid = f"{LOCAL_OUTPUT_PATH}/text_in_wild_valid_layout.lmdb"
    if (not _lmdb_has_samples(_tiw_train)) or (not _lmdb_has_samples(_tiw_valid)):
        create_text_in_wild_train_valid(max_samples=(DEBUG_SAMPLE_LIMIT if DEBUG_MODE else None))
    else:
        print("â­ï¸ Text in the wild train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    _pa_train = f"{LOCAL_OUTPUT_PATH}/public_admin_train_layout.lmdb"
    _pa_valid = f"{LOCAL_OUTPUT_PATH}/public_admin_valid_layout.lmdb"
    if (not _lmdb_has_samples(_pa_train)) or (not _lmdb_has_samples(_pa_valid)):
        create_public_admin_train_valid(max_samples=(DEBUG_SAMPLE_LIMIT if DEBUG_MODE else None))
    else:
        print("â­ï¸ ê³µê³µí–‰ì •ë¬¸ì„œ OCR train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    _pa_part = f"{LOCAL_OUTPUT_PATH}/public_admin_train_partly_layout.lmdb"
    _pa_part_alt = f"{LOCAL_OUTPUT_PATH}/public_admin_train_partly.lmdb"
    if not (_lmdb_has_samples(_pa_part) or _lmdb_has_samples(_pa_part_alt)):
        create_public_admin_train_partly(max_samples=(DEBUG_SAMPLE_LIMIT if DEBUG_MODE else None))
    else:
        print("â­ï¸ ê³µê³µí–‰ì •ë¬¸ì„œ OCR train_partly LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    _ocr_train = f"{LOCAL_OUTPUT_PATH}/ocr_public_train_layout.lmdb"
    _ocr_valid = f"{LOCAL_OUTPUT_PATH}/ocr_public_valid_layout.lmdb"
    if (not _lmdb_has_samples(_ocr_train)) or (not _lmdb_has_samples(_ocr_valid)):
        create_ocr_public_train_valid(max_samples=(DEBUG_SAMPLE_LIMIT if DEBUG_MODE else None))
    else:
        print("â­ï¸ 023.OCR ë°ì´í„°(ê³µê³µ) train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    _fl_train = f"{LOCAL_OUTPUT_PATH}/finance_logistics_train_layout.lmdb"
    _fl_valid = f"{LOCAL_OUTPUT_PATH}/finance_logistics_valid_layout.lmdb"
    if (not _lmdb_has_samples(_fl_train)) or (not _lmdb_has_samples(_fl_valid)):
        create_finance_logistics_train_valid(max_samples=(DEBUG_SAMPLE_LIMIT if DEBUG_MODE else None))
    else:
        print("â­ï¸ 025.OCR ë°ì´í„°(ê¸ˆìœµ ë° ë¬¼ë¥˜) train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    _hw_train = f"{LOCAL_OUTPUT_PATH}/handwriting_train_layout.lmdb"
    _hw_valid = f"{LOCAL_OUTPUT_PATH}/handwriting_valid_layout.lmdb"
    if (not _lmdb_has_samples(_hw_train)) or (not _lmdb_has_samples(_hw_valid)):
        create_handwriting_train_valid(max_samples=(DEBUG_SAMPLE_LIMIT if DEBUG_MODE else None))
    else:
        print("â­ï¸ 053.ëŒ€ìš©ëŸ‰ ì†ê¸€ì”¨ OCR train/valid LMDB ì´ë¯¸ ì™„ë£Œë¨")
    
    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ë°ì´í„°ì…‹ train/valid LMDB ìƒì„± ì™„ë£Œ! (ì „ì²´ ë°ì´í„° ë³€í™˜)")
    print("\nğŸ“ ìƒì„±ëœ LMDB íŒŒì¼ë“¤:")
    for lmdb_path in lmdb_paths:
        print(f"   - {lmdb_path}")

if __name__ == '__main__':
    main() 