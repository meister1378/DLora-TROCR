#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë‹¨ì¼ ì´ë¯¸ì§€ FAST Detection ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (train_fast_from_lmdb.pyë¡œ í•™ìŠµí•œ ì²´í¬í¬ì¸íŠ¸ìš©)

ì‚¬ìš© ì˜ˆ:
python FAST/infer_single_image.py \
  --checkpoint /home/mango/ocr_test/outputs/fast_lmdb_train/checkpoint_epoch_1.pth \
  --image /home/mango/ocr_test/FAST/5350034-2011-0001-0019.jpg \
  --output /home/mango/ocr_test/FAST/5350034-2011-0001-0019_det.png \
  --config FAST/config/fast/korean_ocr/multi_lmdb_config.py \
  --device cuda
"""

import os
import sys
import matplotlib
matplotlib.use("Agg")  # GUI ë°±ì—”ë“œ ë¹„í™œì„±í™”(ë¸”ë¡œí‚¹ ë°©ì§€)
import argparse
import json
import cv2
import numpy as np
import torch
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# í”„ë¡œì íŠ¸ ë£¨íŠ¸/FAST ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
ROOT = '/home/mango/ocr_test'
FAST_DIR = os.path.join(ROOT, 'FAST')
sys.path.insert(0, ROOT)
sys.path.insert(0, FAST_DIR)

from mmcv import Config, ConfigDict
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
try:
    from peft import PeftModel
except Exception:
    PeftModel = None
from models import build_model
from dataset.utils import scale_aligned_short
import torchvision.transforms as transforms


def _try_load_cfg_from_checkpoint(ckpt):
    # mmdet ê³„ì—´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìžì£¼ ì“°ì´ëŠ” í‚¤ë“¤ì„ íƒìƒ‰í•´ cfgë¥¼ ì¶”ì¶œ
    for key in ['cfg', 'config']:
        if key in ckpt and ckpt[key] is not None:
            data = ckpt[key]
            try:
                if isinstance(data, str):
                    # ë¬¸ìžì—´ì´ë©´ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” Python dict ë¬¸ìžì—´ì¼ ìˆ˜ ìžˆìŒ
                    if os.path.exists(data):
                        return Config.fromfile(data)
                    try:
                        parsed = json.loads(data)
                        return Config(parsed)
                    except Exception:
                        pass
                if isinstance(data, dict):
                    return Config(data)
            except Exception:
                pass
    if 'meta' in ckpt and ckpt['meta'] is not None:
        meta = ckpt['meta']
        for key in ['cfg', 'config']:
            if key in meta and meta[key] is not None:
                try:
                    if isinstance(meta[key], dict):
                        return Config(meta[key])
                except Exception:
                    pass
    return None


def load_model(cfg_path, checkpoint_path, device_str='cuda'):
    device = torch.device(device_str if torch.cuda.is_available() and device_str == 'cuda' else 'cpu')

    ckpt = torch.load(checkpoint_path, map_location=device)

    # êµ¬ì„± ë¡œë“œ: ìš°ì„  ì¸ìž cfg, ì—†ìœ¼ë©´ ckptì—ì„œ ì‹œë„
    if cfg_path and os.path.exists(cfg_path):
        cfg = Config.fromfile(cfg_path)
    else:
        cfg = _try_load_cfg_from_checkpoint(ckpt)
        if cfg is None:
            raise RuntimeError('êµ¬ì„±(cfg)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --config ê²½ë¡œë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.')

    # ë³´í˜¸: cfg.test_cfg ê¸°ë³¸ê°’ ì£¼ìž…
    try:
        if not hasattr(cfg, 'test_cfg') or cfg.test_cfg is None:
            cfg.test_cfg = ConfigDict(dict(min_area=5, min_score=0.3, bbox_type='rect'))
    except Exception:
        pass

    model = build_model(cfg.model)
    model = model.to(device)

    # state_dict ê°€ì ¸ì˜¤ê¸° (ema/state_dict/ì§ì ‘)
    state_dict = ckpt.get('state_dict', ckpt.get('ema', ckpt))
    # DataParallel í˜¸í™˜
    def _strip_prefix(k: str) -> str:
        # torch.compile ëž˜í¼: '_orig_mod.' ì œê±°, DDP: 'module.' ì œê±°
        return k.replace('_orig_mod.', '').replace('module.', '')
    new_state = {_strip_prefix(k): v for k, v in state_dict.items()}
    missing, unexpected = model.load_state_dict(new_state, strict=False)
    if missing:
        print(f'âš ï¸ missing keys: {len(missing)}')
    if unexpected:
        print(f'âš ï¸ unexpected keys: {len(unexpected)}')

    model.eval()
    return model, cfg, device


def load_trocr(trocr_path: str, device: torch.device):
    """
    TrOCR ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    - trocr_pathëŠ” `checkpoint-60000` ê°™ì€ ë””ë ‰í„°ë¦¬ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    - PEFT ì–´ëŒ‘í„°(LoRA/DoRA)ê°€ í¬í•¨ëœ ìƒíƒœë¡œ ì €ìž¥ë˜ì–´ ìžˆë‹¤ë©´ ìžë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
    """
    # ê°€ëŠ¥í•œ ê²½ìš°: ë””ë ‰í„°ë¦¬ì— ì „ì²´ ëª¨ë¸ì´ ì €ìž¥ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ë¡œë“œ
    try:
        processor = TrOCRProcessor.from_pretrained(trocr_path)
    except Exception:
        # processorê°€ ë””ë ‰í„°ë¦¬ì— ì—†ìœ¼ë©´ ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
        base_name = os.environ.get("TROCR_BASE", "ddobokki/ko-trocr")
        processor = TrOCRProcessor.from_pretrained(base_name)

    try:
        model = VisionEncoderDecoderModel.from_pretrained(trocr_path)
        model.to(device)
        model.eval()
        return processor, model
    except Exception as e:
        # ì–´ëŒ‘í„°ë§Œ ì €ìž¥ëœ ê²½ìš°: ë² ì´ìŠ¤ ëª¨ë¸ + PEFT ì–´ëŒ‘í„° ì£¼ìž…
        if PeftModel is None:
            raise e
        base_name = os.environ.get("TROCR_BASE", "ddobokki/ko-trocr")
        base_model = VisionEncoderDecoderModel.from_pretrained(base_name)
        # ì–´ëŒ‘í„° ë¡œë“œ ì „, í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§žì¶° ë””ì½”ë” ìž„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ
        try:
            target_vocab_size = len(getattr(processor, "tokenizer", {}))
            if isinstance(target_vocab_size, int) and target_vocab_size > 0:
                if hasattr(base_model, "decoder") and hasattr(base_model.decoder, "resize_token_embeddings"):
                    base_model.decoder.resize_token_embeddings(target_vocab_size)
                base_model.config.vocab_size = target_vocab_size
        except Exception:
            pass

        # 1) ë£¨íŠ¸ì— ë‹¨ì¼ ì–´ëŒ‘í„° í˜•ì‹(adapter_config.json ë“±) ì¡´ìž¬ ì‹œ
        adapter_cfg = os.path.join(trocr_path, "adapter_config.json")
        adapter_bin = os.path.join(trocr_path, "adapter_model.bin")
        adapter_safetensors = os.path.join(trocr_path, "adapter_model.safetensors")
        if os.path.exists(adapter_cfg) and (os.path.exists(adapter_bin) or os.path.exists(adapter_safetensors)):
            model = PeftModel.from_pretrained(base_model, trocr_path)
            model.to(device)
            model.eval()
            return processor, model

        # 2) ì„œë¸Œë””ë ‰í„°ë¦¬(encoder_dora/decoder_lora)ì— ê° ì–´ëŒ‘í„°ê°€ ì €ìž¥ëœ í˜•ì‹
        enc_dir = os.path.join(trocr_path, "encoder_dora")
        dec_dir = os.path.join(trocr_path, "decoder_lora")
        def _has_adapter(dir_path: str) -> bool:
            return os.path.isdir(dir_path) and os.path.exists(os.path.join(dir_path, "adapter_config.json")) and \
                   (os.path.exists(os.path.join(dir_path, "adapter_model.bin")) or os.path.exists(os.path.join(dir_path, "adapter_model.safetensors")))

        if _has_adapter(dec_dir) or _has_adapter(enc_dir):
            model = base_model
            # ë””ì½”ë” ì–´ëŒ‘í„° ìš°ì„  ë¡œë“œ
            if _has_adapter(dec_dir):
                model = PeftModel.from_pretrained(model, dec_dir, adapter_name="decoder_lora")
            else:
                model = PeftModel.from_pretrained(model, enc_dir, adapter_name="encoder_dora")
            # ë‚˜ë¨¸ì§€ ì–´ëŒ‘í„° ì¶”ê°€ ë¡œë“œ
            if _has_adapter(enc_dir) and hasattr(model, "load_adapter"):
                model.load_adapter(enc_dir, adapter_name="encoder_dora")
            if _has_adapter(dec_dir) and hasattr(model, "load_adapter"):
                try:
                    model.load_adapter(dec_dir, adapter_name="decoder_lora")
                except Exception:
                    pass
            model.to(device)
            model.eval()
            return processor, model

        # í•´ë‹¹ í˜•ì‹ ëª¨ë‘ ì•„ë‹ˆë©´ ì›ëž˜ ì˜¤ë¥˜ ì „ë‹¬
        raise e


def preprocess(image_path, short_size):
    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        raise FileNotFoundError(image_path)
    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    org_h, org_w = img.shape[:2]
    img_resized = scale_aligned_short(img, short_size)
    proc_h, proc_w = img_resized.shape[:2]
    pil_img = Image.fromarray(img_resized).convert('RGB')
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    tensor = transform(pil_img).unsqueeze(0)
    # ëª¨ë¸ headê°€ img_meta['org_img_size'][0] í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¼ë‹¤
    meta = {
        'org_img_size': [np.array([org_h, org_w])],
        'img_size': [np.array([proc_h, proc_w])],
        'filename': [os.path.basename(image_path)]
    }
    return tensor, meta, (org_h, org_w), (proc_h, proc_w), img


def visualize(image_rgb, dets, org_size, proc_size, output_path=None):
    # fast_head.get_resultsëŠ” ë‚´ë¶€ì—ì„œ ì›ë³¸ ì¢Œí‘œê³„ë¡œ ë³µì›í•˜ë¯€ë¡œ ì¶”ê°€ ìŠ¤ì¼€ì¼ë§ì€ í•˜ì§€ ì•ŠëŠ”ë‹¤
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.imshow(image_rgb)
    ax.axis('off')
    # detsëŠ” head.get_resultsì—ì„œ ë°˜í™˜ëœ outputs['results']
    # í˜•íƒœ: [ { 'bboxes': np.ndarray(M, K), 'scores': np.ndarray(M) } ]
    boxes_drawn = 0
    if dets:
        res = dets[0] if isinstance(dets, list) else dets
        bboxes = res.get('bboxes') if isinstance(res, dict) else None
        if bboxes is not None:
            try:
                import numpy as np
                for bb in bboxes:
                    bb = np.array(bb).reshape(-1)
                    if bb.size >= 8:
                        xs = bb[0::2]; ys = bb[1::2]
                        x1, y1, x2, y2 = xs.min(), ys.min(), xs.max(), ys.max()
                    elif bb.size >= 4:
                        x1, y1, x2, y2 = bb[:4]
                    else:
                        continue
                    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                             linewidth=2, edgecolor='r', facecolor='none')
                    ax.add_patch(rect)
                    boxes_drawn += 1
            except Exception:
                pass
    plt.tight_layout()
    if output_path:
        plt.savefig(output_path, dpi=200, bbox_inches='tight')
        print(f'ðŸ’¾ ì €ìž¥: {output_path}')
        plt.close(fig)
    else:
        plt.show()


def main():
    parser = argparse.ArgumentParser(description='FAST ë‹¨ì¼ ì´ë¯¸ì§€ Detection ì¶”ë¡ ')
    parser.add_argument('--checkpoint', required=True, type=str)
    parser.add_argument('--image', required=True, type=str)
    parser.add_argument('--output', default=None, type=str)
    parser.add_argument('--config', default=None, type=str, help='ì„ íƒ: cfg ë¯¸í¬í•¨ ckptì¸ ê²½ìš° í•„ìš”')
    parser.add_argument('--device', default='cuda', choices=['cuda', 'cpu'])
    parser.add_argument('--trocr_checkpoint', type=str, default=None, help='TrOCR í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í„°ë¦¬ (ì˜ˆ: /home/.../checkpoint-60000)')
    args = parser.parse_args()

    model, cfg, device = load_model(args.config, args.checkpoint, args.device)
    trocr_processor = None
    trocr_model = None
    if args.trocr_checkpoint:
        trocr_processor, trocr_model = load_trocr(args.trocr_checkpoint, device)
    short_size = getattr(getattr(cfg, 'data', None), 'test', None)
    if short_size is not None:
        short_size = getattr(cfg.data.test, 'short_size', 736)
    else:
        short_size = 736

    img_tensor, meta, org_size, proc_size, img_rgb = preprocess(args.image, short_size)
    img_tensor = img_tensor.to(device)

    with torch.no_grad():
        # get_resultsëŠ” generate_bbox ë‚´ë¶€ì—ì„œ cfg.test_cfgë¥¼ ì°¸ì¡°í•˜ë¯€ë¡œ ì „ì²´ cfgë¥¼ ì „ë‹¬í•´ì•¼ í•¨
        outputs = model(img_tensor, img_metas=meta, cfg=cfg)
    # forwardëŠ” head.get_resultsì˜ ë°˜í™˜ì„ outputs.update(...)ë¡œ í•©ì¹˜ë¯€ë¡œ 'results' í‚¤ë¥¼ ì½ëŠ”ë‹¤
    dets = outputs.get('results', outputs)
    if isinstance(dets, dict) and 'results' in dets:
        dets = dets['results']

    # ê²°ê³¼ í†µê³„
    n = 0
    if dets:
        res0 = dets[0] if isinstance(dets, list) else dets
        bboxes = res0.get('bboxes') if isinstance(res0, dict) else None
        n = 0 if bboxes is None else len(bboxes)
    print(f'âœ… ê²€ì¶œ ìˆ˜: {n}')

    visualize(img_rgb, dets, org_size, proc_size, args.output)

    # ì„ íƒ: ê²€ì¶œëœ ì˜ì—­ì— ëŒ€í•´ TrOCRë¡œ ê°„ë‹¨ížˆ ì¸ì‹ ì‹œì—°(ì˜ì—­ ìˆ˜ê°€ ë§Žì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ìƒìœ„ ëª‡ ê°œë§Œ)
    if trocr_model is not None and dets and isinstance(dets, list) and len(dets) > 0:
        res = dets[0]
        bboxes = res.get('bboxes')
        scores = res.get('scores')
        if bboxes is not None and scores is not None:
            top_k = min(5, len(bboxes))
            print(f"\n[TrOCR demo] Top-{top_k} boxes recognition from {args.trocr_checkpoint}")
            for i in range(top_k):
                bb = np.array(bboxes[i]).reshape(-1)
                if bb.size >= 8:
                    xs = bb[0::2]; ys = bb[1::2]
                    x1, y1, x2, y2 = int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())
                elif bb.size >= 4:
                    x1, y1, x2, y2 = map(int, bb[:4])
                else:
                    continue
                crop = img_rgb[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                pil = Image.fromarray(crop)
                pv = trocr_processor(pil, return_tensors="pt").pixel_values.to(device)
                gen = trocr_model.generate(pv, max_length=64)
                text = trocr_processor.batch_decode(gen, skip_special_tokens=True)[0]
                print(f"  [{i}] {text}")


if __name__ == '__main__':
    main()


